{
  "hash": "9630d52815ff739b8609a52d373588af",
  "result": {
    "engine": "knitr",
    "markdown": "# Regression in the data science workflow {#sec-ch-intro}\n\n\n\n## Learning goals {.unnumbered}\n\n-   Define regression analysis and how it is used to answer questions about real-world phenomena\n-   Describe the data science workflow\n-   Define reproducibility and how it is implemented in the data science workflow\n\n\n::: {.cell}\n\n:::\n\n\n## Regression analysis {#sec-regression-analysis}\n\nWe use data to gain insights and knowledge about real-world phenomena. Often, there is a **population** we are interested in studying (e.g., all adults in the United States), but we do not have access to data on the full population. Or we want to make predictions for what is expected to happen in the population in the future. In either case, we analyze data about a **sample** (subset) drawn from the population to gain insights or make predictions about the population. The sample is made of the observations in the data obtained in the **collect** stage of @fig-data-science-workflow.\n\nOne way this is done, in general, is through statistical models. **Statistical models** are a broad array of mathematical models that are used to describe how data are generated in the population (also called *data generating* models). This text will primarily focus on a class of statistical models called regression analysis.\n\n::: {.terms latex=\"\"}\n-   **Population**: Entire group being studied\n\n-   **Sample**: Subset of the population that is used in the analysis\n\n-   **Statistical model**: Mathematical models used to describe how data are generated in a population\n:::\n\n### What is regression analysis? {#sec-what-is-regression}\n\n**Regression analysis** (also called regression models) is a set of statistical models that describe multivariable relationships, the relationships between two or more variables. In particular, they describe the relationship between a response variable and one or more predictor variables. The **response variable** (also called *outcome* or *dependent variable*) is the outcome of interest. In other words, it is the variable about which we wish to understand variability or predict. The **predictor variables** (also called *explanatory variables* or *independent variables*) are the variables used to explain variability or predict the response variable. We denote the response variables $Y$ and the predictor variables as $X_1, X_2, \\ldots, X_p$, where $p$ is the number of predictors.\n\n::: {.terms latex=\"\"}\n-   **Regression analysis**: Set of statistical models to describe multivariable relationships between a response and one or more predictor variables\n\n-   **Response variable**: Variable whose variability we wish to understand or we wish to predict\n\n-   **Predictor variable(s)**: Variable(s) used to explain variability in the response or to predict new values of the response variable\n:::\n\nIn regression analysis, we assume the values of the response variable be generated by the following process\n\n$$\nY = \\text{Model} + \\text{Error}\n$$ {#eq-model-error-eq}\n\nMore specifically, the model is a function of the predictor variables, $\\text{Model} = f(X_1, X_2, \\ldots, X_p)$ . We input some values of the predictor variables and the output is $f(X_1, X_2, \\ldots, X_p)$. The Error, then, is the difference between the observed value $Y$ and the output from $f(X_1, X_2, \\ldots, X_p)$. Much of regression is choosing the $\\text{Model}$ and using the $\\text{Error}$ to evaluate the model's performance and how well it explains the relationships in the data.\n\nThere are two types of regression models that differ on how they define $f(X_1, X_2, \\ldots, X_p)$. @breiman2001statistical referred to these as the \"two cultures\" of statistical modeling.\n\nIn **parametric regression models**, we assume the model function takes a specific form and then use techniques to estimate the parameters of the model. For example, we may assume the model takes the form\n\n$$\nf(X_1, X_2, \\ldots, X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n$$ {#eq-parametric}\n\nwhere $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p$ are the **model coefficients**. Thus, the full form of the model from @eq-model-error-eq is\n\n$$\n\\begin{aligned}\n&Y = f(X_1, X_2, \\ldots, X_p) + \\epsilon \\\\[10pt]\n\\Rightarrow &Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\n\\end{aligned}\n$$ {#eq-parametric-model}\n\nwhere $\\epsilon$ is the error.\n\nIn parametric regression, we use regression methods to **fit the model**, find estimated values for these model coefficients. We then input these estimated coefficients into @eq-parametric to produce predicted values of the response variable, denoted $\\hat{Y}$.\n\n$$\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + \\dots + \\hat{\\beta}_pX_p\n$$\n\nWhen we fit models using parametric methods, we make inherent assumptions about the underlying form of the relationships in the data. In @sec-ch-slr-conditions, we show ways to check a set of model conditions to evaluate whether these assumptions are actually true in the data. One advantage to parametric models is that we can use them to interpret the relationship between the response and predictor variables. In @sec-ch-slr and @sec-ch-mlr, we will talk extensively about how we might interpret the estimated coefficients, and how for example, we can use this equation to predict values of life expectancy. For now, the goal is to see that when using parametric methods, we derive a mathematical equation of the relationship between the response variable and predictor variables then estimate the coefficients in that equation.\n\nThe vast majority of this book focuses on parametric models, specifically linear regression (models for quantitative response variables) and logistic regression (models for categorical response variables).\n\nThe second type of regression model is **non-parametric regression models**. These models differ from parametric models in that there is no assumed structure of the function between the response and predictor variables. The function $f(X_1, X_2, \\ldots, X_p)$ is defined by finding a way to closely model the trend in the sample data without fitting it so closely that it couldn't apply to new data. The advantage to this approach is that we do not impose an assumed structure on the relationship between the response and predictor variables. The primary disadvantage, however, is that these models are often uninterpretable, making it difficult to describe the relationships in the data.\n\n::: {.terms latex=\"\"}\n-   **Parametric regression:** Methods in which the model structure is predefined and the data are used to estimate coefficients of the model\n\n-   **Non-parametric regression**: Methods in which there is no predefined structure for the model, and the model is defined by getting close (to a point) to the observations\n\n-   **Fit a model**: Use data to estimate a model\n:::\n\n### Why fit a regression model? {#sec-why-regression}\n\nIn general, there are two primary purposes for regression analysis: inference and prediction. **Inference** is the process of drawing conclusion and insights from the model. **Prediction** is the process of computing the expected value of the response variable based on values of the predictor variable(s).\n\nBoth tasks are often performed in a single analysis. It is generally beneficial to be able to use a model to generate both predictions and insights from the data. The analysis questions from **plan** stage of the data science workflow (@fig-data-science-workflow) are used to determine the primary purpose of an analysis. As we will see throughout the book, many analysis decisions are made based on whether the analysis is focused on inference or prediction (or both).\n\n::: {.terms latex=\"\"}\n-   **Inference:** Process of drawing conclusions and insights from a model\n\n-   **Prediction**: Process of computing the expected value of the response variable based on value(s) of the predictor variable(s)\n:::\n\nHere, we are interested in models that are intperpretable and are useful for gaining insights about the data. Even as evaluate models with prediction as the primary objective, we still take the interpretability into account. Therefore, we will often balance predictive accuracy with ease of interpretation. This is often important for models that are used in research, as the goal is to use data to understand a particular phenomena. In other contexts, such as industry for example, the importance of predictive accuracy may far exceed the that of interpretability. In this case, non-parametric models, such as those commonly used in machine learning, may be preferred over regression methods. We will discuss a set of non-parametric models in @sec-cecision-trees and refer readers to *Introduction to Statistical Learning* [@james2021introduction] for a more in-depth introduction to non-parametric models.\n\n### Example: Country's life expectancy {#sec-intro-life-exp}\n\nWe apply what we've discussed about regression models thus far to an example motivated by @zarulli2021health, who studied the relationship between life expectancy and a country's healthcare system. Life expectancy is how long an individual is expected to live on the day they are born based on factors about their surrounding environment. <!--# make sure this is correct--> There are large differences in life expectancy around the world, so it is important to understand the factors that impact life expectancy in order to help improve outcomes for individuals in countries with lower life expectancy.\n\nIn this section, we will use data from @zarulli2021health to examine the relationship between life expectancy, education, and income inequality. The data were originally obtained by researchers from the Human Development Database (<https://hdr.undp.org/data-center>) and the World Health Organization ([https://apps.who.int/nha/database](https://apps.who.int/nha/database/)).\n\nThe data set is available in `life-expectancy.csv`. It contains information about life expectancy, health-care related factors, and other societal factors for 140 countries. We will use the following variables in this section. The definitions are from the respective original data sources.\n\n-   `life_exp`: The average number of years that a newborn could expect to live, if he or she were to pass through life exposed to the sex- and age-specific death rates prevailing at the time of his or her birth, for a specific year, in a given country, territory, or geographic area. (from the World Health Organization)\n\n-   `income_inequality`: Measure of the deviation of the distribution of income among individuals or households within a country from a perfectly equal distribution. A value of 0 represents absolute equality, a value of 100 absolute inequality (Gini coefficient). (from the Human Development Database)\n\n-   `education`: Indicator of whether a country’s education index is above (`High`) or below (`Low`) the median index for the 140 countries in the data set.\n\n    -   Education index: Average of mean years of schooling (of adults) and expected years of school (of children), both expressed as an index obtained by scaling wit the corresponding maxima. (from the Human Development Database)\n\nSee @zarulli2021health for a full codebook.\n\n@fig-life-exp-eda shows the distributions and @tbl-life-exp-eda shows the summary statistics for `life_expectancy` and `income_inequality`. These are used to better understand the data, a process called exploratory data analysis (EDA). EDA is introduced more fully in @sec-ch-eda.\n\n\n::: {#fig-life-exp-eda .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Life expectancy](01-intro_files/figure-html/fig-life-exp-eda-1.png){#fig-life-exp-eda-1 width=100%}\n:::\n\n::: {.cell-output-display}\n![Income inequality](01-intro_files/figure-html/fig-life-exp-eda-2.png){#fig-life-exp-eda-2 width=100%}\n:::\n\nDistributions of variables life expectancy analysis\n:::\n\n\n\n::: {#tbl-life-exp-eda .cell tbl-cap='Summary statistics of life expectancy and income inequality\"'}\n::: {.cell-output-display}\n\n\n|Variable          | Mean|  SD|  Min|   Q1| Median (Q2)|   Q3|  Max| Missing|\n|:-----------------|----:|---:|----:|----:|-----------:|----:|----:|-------:|\n|life_exp          | 71.6| 8.1| 51.6| 65.4|        72.8| 78.0| 84.1|       0|\n|income_inequality | 19.8| 9.7|  5.4| 11.5|        18.1| 28.3| 44.2|       0|\n\n\n:::\n:::\n\n\n::: {.yourturn latex=\"\"}\nWhat do you observe about the distributions of `life_exp` and `income_inequality based on` @fig-life-exp-eda and @tbl-life-exp-eda?[^01-intro-1]\n:::\n\n[^01-intro-1]: Example answers: Have the countries have life expectancy of 72.8 years or higher. There are a few countries with low life expectancy less than 55 years. There appear to be two subgroups of countries in regard to income inequality; those with income inequality between 5 and 15 and those between 20 and 30. There are two countries with high income inequality over 40.\n\nWe know the distribution of `education` based on its definition. Fifty percent of the observations are categorized as `Low` and 50% are categorized as `High`.\n\nSuppose we wish to fit a regression model to predict a country's life expectancy using income inequality and education. In parametric regression, we can specify the form of the model as\n\n$$\n\\begin{aligned}\n\\text{life\\_exp} &= f(\\text{income\\_inequality},\\text{education}) + \\epsilon \\\\[10pt]\n& =  \\beta_0 + \\beta_1~\\text{income\\_inequality} + \\beta_2~\\text{education} + \\epsilon\n\\end{aligned}\n$$ {#eq-life-exp-model}\n\n::: {.yourturn latex=\"\"}\nIn @sec-why-regression, we introduced the two primary purposes of regression analysis, prediction and inference.\n\n-   What is an example a prediction question that can be answered from @eq-life-exp-model?\n-   What is an example of an inference question that can be answered from @eq-life-exp-model?[^01-intro-2]\n:::\n\n[^01-intro-2]: **Example prediction question**: What is the life expectancy for a country with high education and no income inequality?\\\n    **Example inference question**: How does a country’s life expectancy change as income inequality increases?\n\nWe use the data to fit the model and find estimates for $\\beta_0, \\beta_1, \\text{ and }\\beta_2$. @eq-life-exp-estimated is the equation of the estimated model, and @fig-life-exp-estimated is a visual representation of this model. We discuss the process of choosing the model form, estimating the coefficients, and interpreting the model beginning in @sec-ch-slr.\n\n\n::: {.cell}\n\n:::\n\n\n$$\n\\widehat{\\text{life\\_exp}} = 81.096 -0.562 \\times \\text{income\\_inequality} + 2.386 \\times \\text{education}\n$$ {#eq-life-exp-estimated}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visual representation of model estimated regression model](01-intro_files/figure-html/fig-life-exp-estimated-1.png){#fig-life-exp-estimated width=100%}\n:::\n:::\n\n\n## Regression in a data science workflow {#sec-regression-in-ds-workflow}\n\nRegression is typically one step in a larger data science workflow. In this section, we will zoom out so we can see where regression fits in the context of this workflow.\n\n### Data science approach {#sec-define-data-science}\n\nData science is a relatively new and rapidly evolving discipline, and there is no single definition of what data science is and how it compares to related disciplines. This book approaches regression through the lens of data science, so let's take a moment to explore how others have defined data science and use it to describe what it means to use take a \"data science approach\" to regression.\n\nIn the 1962 paper *The Future of Data Analysis*, statistician John Tukey describes the field of data analysis, which many argue is actually a description of what we call data science today. <!--# is there a citation to the donoho definition--> He says data analysis includes\n\n> \"procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\" [@tukey1962future, pp. 2]\n\nHe later on goes to argue that data analysis is, in fact, a science, because it contains the three characteristics of a science\n\n-   *intellectual content*\n-   *organization into an understandable form,*\n-   *reliance upon the test of experience as the ultimate standard of validity.* [@tukey1962future, pp. 5]\n\nThe way we think about and work with data has changed dramatically since Tukey's 1962 definition of \"data analysis\". Today, data are ubiquitous and have become integrated into every day life, and very large data sets (called \"big data\") have become increasingly common. Additionally, the \"machinery\" that Tukey refers to has become much more sophisticated and widely accessible, vastly increasing the data analysis capabilities available to researchers, industry professionals, and learners alike. <!--# make sure this isn't from the donoho paper-->\n\nGiven these differences in today's data landscape, let's look at some more recent definitions of data science.\n\nWikipedia defines data science as\n\n> \"an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.\" [@wikipediaDataScience]\n\nIn *Modern Data Science with R*, @baumer2017modern define data science as \"science of extracting meaningful information from data\". In *R for Data Science,* @wickham2023r say it is \"an exciting discipline that allows you to transform raw data into understanding, insight, and knowledge.\" In *Data Science: A First Introduction*, @timbers2022data define data science as the \"process of generating insight from data through reproducible and auditable processes\". In *Telling Stories with Data*, @alexander2023telling concludes that data science is\n\n> *\"*the process of developing and applying a principled, tested, reproducible, end-to-end workflow that focuses on quantitative measures in and of themselves, and as a foundation to explore questions\"\n\nLastly, @alby2023data emphasizes the domain knowledge in their definition of data science in the book *Data Science in Practice,*\n\n> \"Data science is currently still a consolidating application area of artificial intelligence in which interdisciplinary knowledge from statistics, computer science, and expert knowledge from other disciplines are converged to further develop methods for unlocking data patterns in these fields.\" (pg. 17)\n\n<!--# when do I make things block quotes versus not?-->\n\nThese definitions tend to emphasize different aspects of data science; however, there are common themes across all of them. Taking these definitions into account, as we introduce regression analysis using a \"data science approach\", we are conducting regression such that the following hold:\n\n1.  We apply knowledge, skills, and tools from statistics and computer science.\n2.  We use domain knowledge to interpret results and glean useful insights from data.\n3.  We conduct analysis using a reproducible workflow.\n\nPillars <!--# is there a better word?--> (1) and (2) are illustrated throughout the book as we walk through case study examples in each chapter. Pillar (3) is best demonstrated as these methods are put into practice, so we introduce the concept o f a reproducible workflow in @sec-reproducibility, discuss tools for conducting reproducible analyses in @sec-quarto, and practice implementing it as part of the assignments in the supplemental materials. <!--# Insert URL-->\n\n::: {.terms latex=\"\"}\n**Data science approach to regression**\n\nWe conduct regression analysis such that the following hold:\n\n1.  We apply knowledge, skills, and tools from statistics and computer science.\n2.  We use domain knowledge to interpret results and glean useful insights from data.\n3.  We conduct analysis using a reproducible workflow.\n:::\n\n### Data science workflow {#sec-data-science-workflow}\n\nThe focus of this book, regression analysis, is one part of a much larger data science workflow. It is important to understand what happens before and after regression modeling, because each step in the workflow informs the others.\n\n![Data science workflow[^01-intro-3]](images/01-data-science-workflow.png){#fig-data-science-workflow fig-align=\"center\" width=\"90%\"}\n\n[^01-intro-3]: This workflow is inspired by the data science workflows in @wickham2023r and @timbers2022data.\n\n[^01-intro-4]: This workflow is inspired by the data science workflows in @wickham2023r and @timbers2022data.\n\n<!--# is there a way to add reproducibility to the image?-->\n\n@fig-data-science-workflow is the general data science workflow. As illustrated in @fig-data-science-workflow, the process begins before any data analysis. The first step is **plan**, which starts with clearly defining the analysis (or research) question. In practice, this step is often done in collaboration with subject matter experts (e.g., business partners or research collaborators). The analysis question informs decisions made at each step of the workflow, so it is important to take the time to clearly define it at this step. <!--# should I provide resources or guidance on this?--> Additionally, given the vast amounts of data available, having a clear analysis question helps us focus and navigate through what could otherwise be an overwhelming amount of data. <!--# is there more to planning? If not, maybe this step should be renamed?-->\n\nOnce the analysis question has been defined, the next step is **collect.** This is the point in which we collect the data that will be analyzed to answer the analysis question. Data scientists are often **data consumers**, individuals who work with data that has already been collected [@gebru2021datasheets], so in practice, this stage is focused on obtaining the correct sample data from an existing data set or data base. It is important for the data scientist understand the **data provenance**, a record of the origins of a data set along with any processing or changes it has undergone since its original creation. Additionally, it is important to review the **codebook**, documentation of the variables in the data set and their definitions. As we learn more about the data and what the variables measure, we may need to refine the analysis question posed in the previous step to one that can be feasibly answered from the available data. Having an understanding about the contents and history of the data also provides important context as we interpret analysis results.[^01-intro-5]\n\n[^01-intro-5]: Sometimes the data scientist is part of the data collection process, thus making them one of the **data creators** [@gebru2021datasheets], in addition to being a data consumer. As a data creator, it is imperative to provide detailed documentation about the data, so that data consumers have the necessary information to make informed decisions about the data. See @gebru2021datasheets for more on data documentation.\n\nNow it's time to start working with the data. We import it into the statistical software, then begin to **clean** (or preprocess) the data. This is the point we ensure the data has imported correctly and begin to check for potential errors or missingness in the data. We talk more about this step in @sec-initial-check.\n\nNext, we **wrangle** and **explore** the data, often called **exploratory data analysis*.*** This is the point in which we really begin to understand the observations in the data, distributions of the variables in the data, and relationships between variables. We begin to think about the regression modeling, so we may create new variables from existing ones, start to consider how to handle outliers or deal with missing observations. We talk about this step extensively in @sec-ch-eda.\n\n::: {.terms latex=\"\"}\n-   **Data consumers:** individuals who work with data that have already been collected [@gebru2021datasheets]\n-   **Data provenance:** Record of a dataset's origin and history of changes\n-   **Codebook**: Documentation of variables in a data set\n:::\n\nAfter we have explored the data it is time to **model** the data, the primary topic of this text. This is where we apply statistical methods to more rigorously describe relationships between variables and glean insights from the data to answer the analysis questions posed in the first step. Note from @fig-data-science-workflow that we often iterate through modeling, exploring, and wrangling, as the modeling may uncover areas in which we need to consider variable transformations (@sec-ch-transformations) or consider new relationships in the data.\n\nOnce we have finalized the model and drawn conclusions, it's time to **communicate** the results. This is a very important step in the workflow, because as @wickham2014datascience put it, \"the end product of an analysis is not a model: it is rhetoric\". We often conduct data science to inform decisions, such as informing business decisions or to share new knowledge through innovative research results. Therefore, it is important that we accurately and effectively communicate the results of the work to collaborators, stakeholders, and sometimes a general audience. Throughout the book, we will discuss how to communicate interpretations and conclusions from the regression model, along with various nuances that can make our communication more or less effective.\n\nBecause the goal of this book is to introduce regression analysis methods, the \"model\" part of the data science workflow will be given out-sized attention, compared to the time it often takes in practice. We will illustrate the other parts of the workflow throughout, because they are important for fully understanding and using the regression analysis results, and to remind us that regression analysis is one part of a larger data science workflow.\n\n### Reproducibility {#sec-reproducibility}\n\nThe third pillar of the data science approach introduced in [@sec-define-data-science] is using a reproducible workflow. @alexander2023telling defined a reproducible workflow as one \"*which can be exactly redone, given* all *the materials used...The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables.\"* <!--# do i need a page number?--> Using this definition as the baseline, we will discuss the following components of a reproducible workflow:\n\n1.  Data\n2.  Code and analysis reports\n3.  Version control\n4.  Organization\n5.  Environment\n\nReproducibility is infused throughout the data science workflow in @fig-data-science-workflow, from the initial plan through communicating results using reproducible tools. The components of reproducibility are introduced separately to help organize our thinking, but they are greatly intertwined in practice. For example, it is difficult to talk about best practices for coding without taking into account the data.\n\n#### Data\n\nThe first component of a reproducible workflow is clear documentation for the data. We introduced the notion of data sheets from @gebru2021datasheets to document data provenance and a codebook to document the variables in the data set. <!--# make sure i'm representing data sheets correctly--> The data sheet provides background for the data that is useful context as we interpret results. The codebook is documentation of the individual variables, so we are clear about what is measured as we conduct the analysis. In general, a codebook should include the following details for each column in the data set:\n\n-   Column name\n-   Data type (e.g., quantitative, categorical, date, identifier)\n-   Precise definition of the variable\n-   Possible range of values or categories\n-   Units (if applicable)\n\n::: {.analysis_in_practice latex=\"\"}\nSometimes codebooks have incomplete information. If information about the data provenance or about individual variables are missing, work with collaborators who are knowledgeable about the data collection process if available, or use exploratory data analysis (@sec-ch-eda) to fill in as much missing information as possible.\n:::\n\n#### Code and analysis reports\n\nWe will use the statistical software R for the analyses in the book. The code and analyses will be written in one of two file types when using R: scripts (`.R`) and Quarto documents (`.qmd`). In general, we use scripts for processes that produce output but not analyses (e.g., creating data sets and writing functions). In contrast, we use Quarto documents for more complete analyses in which there is code, output, and narrative for the process and interpretation of results. This book focuses on complete analyses, so Quarto documents should be used. Quarto documents are more formally introduced in @sec-quarto.\n\nPart of making code reproducible is making it readable to other users (including future you). In this text, we will do this, in part, by utilizing the tidyverse, a specific R syntax that utilizes human-readable names. The tidyverse is introduced in @sec-tidyverse. Additionally, we use narrative in Quarto documents or comments in scripts to describe what a set of code is doing. The goal is not to explain every line of code, but to give general ideas about what analysis task is being done in a set of code and the motivation for doing it. Comments and narrative can also be helpful to explain parts of code that may not be immediately obvious to another reader (or future you) [@alexander2023telling]. <!--# page number?-->\n\nWe can further improve the readability of code through the choice of naming convention for variables and functions and how the code is formatted. See Chapter 4 of *R for Data Science* [@wickham2023r] for further reading on code style to improve readability.\n\n#### Version control\n\n**Version control** is a \"system that records changes to a file or set of files over time so that you can recall specific versions later.\" [@chacon2014pro] The ability to systematically track changes to a file helps make work more reproducible and aids in collaboration. Instead of saving new versions of a file each time we update it (i.e., `my-analysis.qmd`, `my-analysis-v2.qmd`, `my-analysis-final.qmd`, `my-analysis-final-updated.qmd`), we practice version control by documenting the changes made to a single file and making it easier to see the history of changes. It is also useful for debugging code, as version control allows makes it easier to go back to a version of the file before the coding issues occurred.\n\n*Git* is a software used for version control. It is installed on a computer or virtual machine and configured to use with the analysis software. Throughout an analysis, we write **commit messages*,*** short and informative messages that describe the changes that have been made to a document (e.g., `Added model interpretation`). These changes are **commit**ted (saved)and tracked in git. It is similar to the track changes feature in a word processor with the addition of messages that briefly describe the changes at each iteration.\n\nGitHub (<https://github.com>) and GitLab (<https://gitlab.com>) are two online platforms for storing git-based projects. They serve as a nice space to backup git-based projects along as a centralized platform for collaboration on git-based projects. They are similar to cloud storage platforms <!--# is that what thes are called?--> such as Dropbox or Google Drive, with additional functionality specific to git.\n\nThe assignments in the supplemental materials <!--# post URL--> are designed to for regular version control throughout. The book *Happy Git and GitHub for the useR* [@bryan2025happygit] is a great reference for the technical details about installing and using git and GitHub with R.\n\n::: {.terms latex=\"\"}\n-   **Version control**: Process of systematically tracking changes to a file <!--# citation for this definition? -->\n\n-   **git**: Software used for version control\n\n-   **GitHub/ GitLab**: online platforms for storing git-based projects\n\n-   **Commit messages**: Short and informative messages to describing a change\n\n-   **Commit**: Save and store changes in git\n:::\n\n#### Organization\n\nIt is good practice to keep all files associated with a given analysis within a single folder to make it easy for you and others to navigate. These folders are called *projects* in RStudio and *workspaces* in Positron, the data analysis software platforms made by Posit. <!--# should I explain what RStudio and Positron are here? I do this in the next chapter-->\n\nAn advantage of housing all files for an analysis within a single folder is that files can referenced using relative paths (e.g., `data/mydata.csv` instead of `User/id1234/my-project/data/my-data.csv`). Relative paths are important for reproducibility and collaboration, because the path names still work when the project is opened on different computers.\n\nOther considerations for organization are the file structure and naming conventions. There are many ways to organize and name files within a single analysis folder, so it is important to choose a file structure and naming conventions that are most easily understood by you and your collaborators. At a minimum, files should be named in such a way that can be easily read by both humans and the computer. Chapter 7 of *R with Data Science* [@wickham2014datascience] and Chapter 3 of *Telling Stories with Data* [@alexander2023telling] provide examples and tips on file structure and naming.\n\n#### Environment\n\nThe last, and perhaps most overlooked, component of reproducibility is the computing environment in which the analysis was done. In the context of analyses using R, the environment means the version of the R packages (collections of functions) used for the analysis. Software changes over time, so documenting the environment is key for long-term reproducibility.\n\nThere are a few ways to do this in R. A rigorous approach is using the **renv** R package [@renv] which stores the R packages in the project directory [@wickham2023r]. Another option is to run the function `sessionInfo()` and save the results in the project directory. This function will print give information about the R version, details about the computer, packages, and their versions.\n\n<!--# do I add a your turn here?-->\n\n## As much art as it is science\n\n> *\"All sciences have much of art in their makeup.\"* @tukey1962future\n\nDoing regression analysis (and data science in general) requires a blend of art and science. Good regression analysis needs both rigorous theory-based methods and the judgment (the \"artistry\") of the data scientist as they make decisions throughout the analysis process. In the same way a chef relies on their knowledge to make decisions about how much seasoning to add to dish, a data scientist relies on their understanding of statistical methods to make decisions as they prepare, explore, model, and interpret data.\n\n@tukey1962future argues that data analysis (or regression analysis in our case) is not just an application of statistical theory with the sole objective to \"optimize\" but rather requires a \"heavy emphasis on judgement.\" He goes on further to say that to analyze data well, \"it must be a matter of judgment, and 'theory' whether statistical or non-statistical, will have to guide, not command\" \\[pp. 10\\]. This judgment is not arbitrary, but come from three sources (summarized from @tukey1962future):\n\n-   judgment based on domain knowledge\n-   judgment based on knowledge statistical methods\n-   judgment based on experience from analyzing a variety of data\n\nThroughout the book, we will demonstrate how judgment is used to make decisions in the analysis, particularly as the regression models become more complex in later chapters. As much as we will highlight parts of the decision-making process, using judgment and doing the \"art\" of regression analysis can't be learned by merely reading a textbook. It is a skill that is developed after consistent <!--# or a lot? or frequent?-->practice working with data. As the world becomes increasingly driven by artificial intelligence, it is this ability to infuse the \"art\" along with the science that will continue to make human data scientists indispensable. <!--# or irreplacable or invalulable?-->\n\n## Summary\n\nIn this chapter, we introduced the data science workflow and the how regression analysis fits within that workflow. We defined reproducibility and the components to take into account when developing a reproducible workflow. We introduced regression analysis, why we fit regression models (inference and prediction), and the role of judgment (\"art\") in regression analysis (and data science in general).\n\nThe next chapter [@sec-ch-computing] is a review (or short introduction) to using the tidyverse for data wrangling, visualization, and writing reproducible reports in Quarto.\n",
    "supporting": [
      "01-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}