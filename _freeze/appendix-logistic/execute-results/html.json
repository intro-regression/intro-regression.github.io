{
  "hash": "d3289a76ec1a9d33f50256cfd5ff4220",
  "result": {
    "engine": "knitr",
    "markdown": "# Mathematics of logistic regression {#sec-appendix-logistic}\n\n\n\nIn @sec-ch-logistic, we introduced logistic regression models for binary response variables. Here we will show some of the mathematics underlying these models, making using of the matrix notation for regression introduced in @sec-appendix-linear.\n\n## Matrix representation of logistic regression\n\nGiven a binary response variable $Y$ and predictors $X_1, X_2, \\ldots, X_p$, the logistic regression model is\n\n$$\n\\text{logit}(\\pi) = \\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n$$ {#eq-logistic-model}\n\nwhere $\\pi = Pr(Y = 1)$.\n\nSimilar to linear regression, we can write a matrix representation of @eq-logistic-model.\n\n$$\n\\text{logit}(\\boldsymbol{\\pi}) = \\log\\Big(\\frac{\\boldsymbol{\\pi}}{1 - \\boldsymbol{\\pi}}\\Big) = \\mathbf{X}\\boldsymbol{\\beta}\n$$ {#eq-logistic-matrix}\n\nWe have the following components in @eq-logistic-matrix:\n\n-   $\\boldsymbol{\\pi}$ is the $n \\times 1$ vector of probabilities, such that $\\boldsymbol{\\pi}_i = Pr(y_i = 1)$\n-   $\\mathbf{X}$ is the $n \\times (p + 1)$ design matrix. Similar to linear regression, the first column is $\\mathbf{1}$, a column of 1's corresponding to the intercept.\n-   $\\boldsymbol{\\beta}$ is a $(p+1) \\times 1$ vector of model coefficients.\n\nThough not directly in @eq-logistic-model or @eq-logistic-matrix, the underlying data also includes $\\mathbf{y}$, an $n\\times 1$ vector of the binary response variables.\n\nWe are often interested in the probabilities computed from the logistic regression model. The probabilities computed from @eq-logistic-matrix are\n\n$$\\boldsymbol{\\pi} = \\frac{e^{\\mathbf{X}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{X}\\boldsymbol{\\beta}}}$$ {#eq-logistic-probability-matrix} See @sec-prob-odds for more detail about the relationship between the logit, odds, and probability.\n\n## Estimation {#sec-logistic-estimation-matrix}\n\nWe want to find estimates $\\hat{\\boldsymbol{\\beta}}$ that are the best fit for the data based on the model in @eq-logistic-matrix. In @sec-logistic-interpret-coef, we outlined how we use maximum likelihood estimation to find $\\hat{\\boldsymbol{\\beta}}$. Here we will show more of the mathematical details behind the model estimation.\n\n::: {.math_rules latex=\"\"}\nLet $Z$ be a random variable that takes values 0 or 1. Then $Z$ follows a **Bernoulli distribution** such that\n\n$$\nPr(Z = z) = p^{z}(1 - p)^{1-z}\n$$\n\nwhere $p = Pr(z = 1)$.\n\n<br>\n\n$E(Z) = p$ and $Var(Z) = p(1-p)$.\n:::\n\nThe response variable follows a Bernoulli distribution, such that $P(Y = y_i) = \\pi_{i}^{y_i}(1 - \\pi_i)^{1-\\pi_i}$. Let $\\mathbf{x}_i^\\mathsf{T}$ be the $i^{th}$ row of the design matrix $\\mathbf{X}$. Then, using @eq-logistic-probability-matrix, we have\n\n$$\nP(Y=y_i) = \\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{y_i}\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{1 - y_i}\n$$ {#eq-logistic-proby-matrix} that the likelihood function is a measure of how likely we observe the data given particular values of the model parameters $\\hat{\\boldsymbol{\\beta}}$.\n\n::: {.math_rules latex=\"\"}\nLet $Z_1, Z_2, \\ldots, Z_n$ be independent Bernoulli random variables. The **joint distribution** of $Z_1, Z_2, \\ldots, Z_n$ (the probability of observing these values) is\n\n$$\nf(Z_1, Z_2, \\ldots, Z_n) = \\prod_{i=1}^n p_i^{z_i}(1-p_i)^{1-z_i}\n$$\n\nwhere $p_i = Pr(Z_i = 1)$ .\n:::\n\nUsing @eq-logistic-proby-matrix, the likelihood function for logistic regression is\n\n$$\nL(\\boldsymbol{\\beta}|\\mathbf{X},\\mathbf{y}) = \\prod_{i=1}^n\\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{y_i}\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{1 - y_i}\n$$ {#eq-logistic-likelihood}\n\nTo make the math more manageable, we will maximize the log likelihood shown in @eq-logistic-log-likelihood. Maximizing @eq-logistic-log-likelihood is equivalent to maximizing @eq-logistic-likelihood.\n\n$$\n\\begin{aligned}\n\\log L(\\boldsymbol{\\beta}|\\mathbf{X},\\mathbf{y}) &= \\sum_{i=1}^n y_i \\log\\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big) + \\sum_{i=1}^n(1-y_i)\\log\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)\\\\[10pt]\n& \\Bigg[\\text{Given } 1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}} = \\frac{1}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Bigg] \\\\[10pt]\n& =  \\sum_{i=1}^n y_i \\log(e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) - \\sum_{i=1}^ny_i \\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) + \\sum_{i=1}^ny_i \\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}})  \\\\[10pt]\n& = \\sum_{i=1}^ny_i\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta} - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}})\n\\end{aligned}\n$$ {#eq-logistic-log-likelihood}\n\nWe take the first derivative of @eq-logistic-log-likelihood with respect to $\\boldsymbol{\\beta}$. An outline of the steps is shown below. The maximum likelihood estimator is the vector of coefficients such that is the solution to $\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}} = 0$ .\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}} &= \\sum_{i=1}^ny_ix_i^\\mathsf{T}\\boldsymbol{\\beta} - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) \\\\[10pt]\n&= \\sum_{i=1}^ny_i\\mathbf{x}_i^\\mathsf{T} - \\frac{e^{\\mathbf{x}_i^\\mathbf{T}\\boldsymbol{\\beta}} \\mathbf{x}_i^\\mathsf{T}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\n\\end{aligned}\n$$ {#eq-logistic-par-deriv}\n\nThere is no closed-from solution for this, i.e., there is no neat formula for $\\hat{\\boldsymbol{\\beta}}$ as we found in @sec-least-sq-matrix for linear regression. Therefore, numerical approximation methods such are used to find the maximum likelihood estimators $\\hat{\\boldsymbol{\\beta}}$. One popular method is Newton-Raphson, a \"root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function\" [@wikipedia_newtons_method]. Numerical approximation methods such as Newton Raphson systematically search the space of of possible values of $\\hat{\\boldsymbol{\\beta}}$ until it converges on the solution (the \"root\") to @eq-logistic-par-deriv.\n\n## Inference for logistic regression {#sec-logistic-inference-matrix}\n\nIn @sec-logistic-inf, we introduced inference for a single coefficient $\\beta_j$ in the logistic regression model. Because there is no closed-form solution for the maximum likelihood estimator $\\hat{\\boldsymbol{\\beta}}$ found in @sec-logistic-estimation-matrix, there is no closed form solution for the mean and variance for the distribution of $\\hat{\\boldsymbol{\\beta}}$. We rely on theoretical results to know the distribution of $\\hat{\\boldsymbol{\\beta}}$ as $n$ gets large (called *asymptotic results*)**.**\n\nGiven $n$ is large,\n\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, (\\mathbf{X}^\\mathsf{T}\\mathbf{V}\\mathbf{X})^{-1})\n$$ {#eq-dist-beta-logistic}\n\nwhere $\\mathbf{V}$ is an $n \\times n$ diagonal matrix, such that $V_{ii}$ is the estimated variance for the $i^{th}$ observation.[^appendix-logistic-1]\n\n[^appendix-logistic-1]: Recall that the variance of the Bernoulli distribution depends on $\\pi$, so each observation has a different variance. This is in contrast to linear regression, where all observations have the same variance $\\sigma^2_{\\epsilon}$.\n\nThe standard error used for hypothesis testing and confidence intervals for a single coefficient $\\beta_j$ , is computed as the $j^{th}$ diagonal element of $Var(\\hat{\\boldsymbol{\\beta}})^{1/2} = (\\mathbf{X}^\\mathsf{T}\\mathbf{V}\\mathbf{X})^{-1/2}$. This is why the hypothesis tests and confidence intervals in @sec-logistic-inf-theory are only reliable for large $n$, because they depend on asymptotic approximations in @eq-dist-beta-logistic. We can use simulation-based methods if the data has a small sample size.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}