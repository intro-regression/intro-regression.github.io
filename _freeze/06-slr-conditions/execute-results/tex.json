{
  "hash": "4f4159d8a2908c5d20e4b46e48f06664",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute: \n  echo: false\n  warning: false\n  message: false\n---\n\n# Model conditions and diagnostics {#sec-ch-slr-conditions}\n\n\n\n## Learning goals {.unnumbered}\n\n-   Describe how model conditions are used to check the assumptions for linear regression\n-   Use the data to check model conditions and diagnostics\n-   Identify strategies to handle violations in the conditions\n-   Identifying influential observations using Cook's distance\n-   Use leverage and standardized residuals to understand potential outliers\n-   Identify strategies for dealing with outliers and influential points\n-   Check model conditions and diagnostics using R\n\n```{=html}\n<!--#\n## Software and packages {.unnumbered}\n\n-   `library(tidyverse)` [@tidyverse]\n\n-   `library(broom)` [@broom-2]\n\n-   `library(knitr)` [@knitr]\n-->\n```\n\n\n\n## Introduction: Coffee grades {#sec-coffee-intro}\n\nWhat makes a delicious cup of coffee? Is there a relationship between a coffee's aroma and its flavor? We will consider these questions by analyzing data from the Coffee Quality Database (<https://database.coffeeinstitute.org/>). The data set used in this analysis was curated by James LeDoux and was featured as part of the TidyTuesday weekly data visualization challenge in July 2020 [@tidytuesday]. It contains a variety of features and quality ratings for over 1,000 coffees scraped from the Coffee Quality Database in 2018.<!--# add something about how the grades are determined-->The data are in `coffee-grades.csv`. We will use the following variables in this chapter:\n\n-   `aroma` : Aroma grade (0: worst aroma - 10: best aroma)\n\n-   `flavor`: Flavor grade (0: worst flavor - 10: best flavor)\n\n::: {.objective latex=\"\"}\nThe goal of the analysis is to use the aroma grade to understand variability in the flavor grade, and assess model conditions and diagnostics to evaluate whether regression analysis is suitable for drawing reliable conclusions from the data.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {#fig-coffee-eda .cell layout=\"[[1,1],[1]]\"}\n::: {.cell-output-display}\n![Distribution of `flavor`](06-slr-conditions_files/figure-pdf/fig-coffee-eda-1.png){#fig-coffee-eda-1 width=85%}\n:::\n\n::: {.cell-output-display}\n![Distribution of `aroma`](06-slr-conditions_files/figure-pdf/fig-coffee-eda-2.png){#fig-coffee-eda-2 width=85%}\n:::\n\n::: {.cell-output-display}\n![`flavor` versus `aroma`](06-slr-conditions_files/figure-pdf/fig-coffee-eda-3.png){#fig-coffee-eda-3 width=85%}\n:::\n\nUnivariate and bivariate exploratory data analysis for coffee data\n:::\n\n\n\n::: {#tbl-coffee-eda .cell tbl-cap='Summary statistics for `aroma` and `flavor`'}\n::: {.cell-output-display}\n\n\n|Variable | Mean|  SD| Min|  Q1| Median (Q2)|  Q3| Max| Missing|\n|:--------|----:|---:|---:|---:|-----------:|---:|---:|-------:|\n|aroma    |  7.6| 0.3| 5.1| 7.4|         7.6| 7.8| 8.8|       0|\n|flavor   |  7.5| 0.3| 6.1| 7.3|         7.6| 7.8| 8.8|       0|\n\n\n:::\n:::\n\n\nFrom the exploratory data analysis in @fig-coffee-eda, we see preliminary indication of a relationship between the aroma and flavor grades. There is an outlier in @fig-coffee-eda-3 that has a very low aroma grade but a flavor grade close to the average.\n\n\n::: {#tbl-coffee-model .cell tbl-cap='Linear regression model of the `aroma` versus `flavor` with 95% confidence intervals'}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:-----------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept) |     1.47|     0.151|      9.75|       0|    1.173|     1.764|\n|aroma       |     0.80|     0.020|     40.25|       0|    0.761|     0.839|\n\n\n:::\n:::\n\n\nWe use the model in @tbl-coffee-model to describe the relationship between the aroma and flavor of coffee. For each additional point in the aroma grade, the flavor grade is expected to increase by 0.8 points, on average.\n\n::: {.yourturn latex=\"\"}\n-   Use @tbl-coffee-model to conduct a hypothesis test to determine if the data provide evidence of a linear relationship between `aroma` and `flavor`.\n-   Is the confidence interval consistent with the conclusion from the test?[^06-slr-conditions-1]\n:::\n\n[^06-slr-conditions-1]: The null hypothesis is there is no linear relationship between `aroma` and `flavor` $(H_0: \\beta_1 = 0)$ and the alternative is that there is a linear relationship $(H_a:\\beta_1 \\neq 0)$. The p-value is $\\approx 0$, so we reject the null hypothesis. The data provide evidence of a linear relationship between the two variables. The confidence interval is consistent, because 0 is not in the interval.\n\nFrom the interval in @tbl-coffee-model, we are 95% confident that for each additional point in the aroma grade, the flavor grade increases by 0.761 to 0.839 points, on average. In summary, coffees that smell better are expected to taste better!\n\nThe inferential methods introduced in @sec-ch-slr-inf used to make conclusions about the relationship between the response and predictor variables rely on a set of underlying assumptions about the data. Though we are unable to check if these underlying conditions hold for the entire population, we will see in @sec-model-conditions how we are able to check a set of model conditions to evaluate whether these assumptions reasonably hold in the sample data.\n\nAs seen in @fig-coffee-eda-3, there is an outlying observation that is outside the general trend of the data, given its aroma grade is much lower than the others but its flavor grade is around the average. This is a valid data point and not the result of data entry error; therefore, we'd like to evaluate whether this outlier has a noticeable impact on the estimated regression model and inferential conclusions. In @sec-model-diagnostics, we will introduce a set of model diagnostics to assess the potential impact of this observation on the results, and the impact of outliers more generally.\n\n## Model conditions {#sec-model-conditions}\n\n### Assumptions and the LINE conditions\n\n@sec-slr-foundation introduced the assumptions underlying the population when doing simple linear regression. These assumptions are important, as the reliability of the interpretations and inferential conclusions from the regression analysis rely on being consistent with the assumed framework. There are various analysis tasks we can do with a simple linear regression model: describe the relationship between a response and predictor variable, predict the response given values of the predictor, and draw conclusions about a population slope using inference. As we'll see in this chapter, we need some or all of these assumptions to hold depending on the analysis task.\n\nRecall the assumptions from @sec-slr-foundation:\n\n1.  The distribution of the response $Y$ is normal for a given value of the predictor $X$.\n\n2.  The expected value (mean) of $Y|X$ is $\\beta_0 + \\beta_1 X$. There is a linear relationship between the response and predictor variable.\n\n3.  The variance $Y|X$ is $\\sigma^2_{\\epsilon}$. This variance is equal for all values of $X$ and thus does not depend on $X$.\n\n4.  The error terms for each observation, $\\epsilon$ in @eq-slr-model-ch5, are independent. This also means the values of the response variable, and observations more generally, are independent.\n\nThe assumptions are based on population-level data, yet we work with sample data in practice. Therefore, we have a set of model conditions we can check using the data at hand to assess these assumptions.\n\nAfter fitting a simple linear regression model, we check four conditions to evaluate whether the data are consistent with the underlying assumptions. They are commonly referred to using the mnemonic LINE.<!--# is there a citation for the original use?--> These four conditions are the following:\n\n-   **L**inearity: There is a linear relationship between the response and predictor variables.\n-   **I**ndependence: The residuals are independent of one another. <!--# Is there a more useful way to describe independence? One residual does not inform another residual? -->\n-   **N**ormality: The distribution of the residuals is approximately normal.\n-   **E**qual variance: The spread of the residuals is approximately equal for all predicted values.\n\nAfter fitting the regression model, we calculate the residuals for each observation, $e_i = y_i - \\hat{y}_i$, and use the residuals to check the conditions. We introduce the four model conditions in order based on the LINE acronym; however, the conditions may be checked in any order.\n\n\n::: {.cell}\n\n:::\n\n\n### Linearity\n\nThe **linearity** condition states that there is a linear relationship between the response and predictor variable. Though we don't expect the relationship between the two variables to be perfectly linear, it should be linear enough that it would be reasonable to summarize the relationship between the two variables using the linear regression model.\n\nRecall from @sec-coffee-intro that we used a scatterplot and the correlation coefficient to describe the shape and other features of the relationship between the `aroma` (predictor) and `flavor` (response). This exploratory data analysis provides an initial assessment of whether the linear regression model seems like a reasonable fit for the data. For example, if we saw a clear curve in the scatterplot or calculated a correlation coefficient close to 0, we might conclude that the proposed linear regression model would not sufficiently capture the relationship between the two variables and thus rethink the modeling strategy. While the exploratory data analysis can help provide initial assessment and help us thoughtfully consider our modeling approach, it should [**not**]{.underline} be solely relied upon as the final evaluation of whether the proposed model sufficiently captures the relationship between the two variables. We analyze the residuals for that assessment.\n\n<!--# Why not rely on EDA to conclude linearity?-->\n\nTo check linearity, we make a scatterplot of the residuals versus the predicted values as shown in @fig-coffee-linearity. We have included a horizontal line at $\\text{residuals} = 0$ to more easily see when the model over or under predicts.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Residuals versus predicted values from the model of `flavor` and `aroma`](06-slr-conditions_files/figure-pdf/fig-coffee-linearity-1.png){#fig-coffee-linearity width=85%}\n:::\n:::\n\n\nThis scatterplot looks very different than any of the scatterplots we've seen thus far in the exploratory data analysis sections in the text. Let's break down what this plot is showing, then use it to make an assessment about the linearity condition.\n\nWhen we fit a linear regression model, we are using the predictor to explain some of the variability in the response variable. Thus the residuals represent the remaining variability in the response variable that is not explained by the regression model (see @sec-slr-sum-sq for more detail). If the relationship between the response and predictor is linear, then we would expect the linear regression model has sufficiently captured the systematic variability that can be explained by the predictor, leaving the residuals to capture any random variability we generally expect when analyzing sample data. **Keeping this in mind, we conclude that the linearity condition is satisfied if the residuals are randomly scattered around 0.** We can check this by asking the following:\n\n> *Based on the plot, can you generally guess with some certainty whether the residual would be positive or negative for a given predicted value or range of predicted values?*\n>\n> -   If the answer is no, then the linearity condition is satisfied.\n>\n> -   If the answer is yes, then the current linear model does not adequately capture the relationship between the response and predictor variables. A model that incorporates information from more predictors or a different modeling approach is needed (we'll discuss some of these in @sec-ch-mlr and \\@\\@sec-ch-transformations).\n\n@fig-linearity-not-met shows an example of a residual plot for simulated data in which the linearity condition is not met. There is a parabolic pattern in the plot, so the residuals are not randomly scattered around 0.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Example of violation in linearity condition. The residuals are not randomly scattered around 0. Based on the plot, we can generally guess with some certainty whether the residual will be positive or negative for a given predicted value or range of predicted values.](06-slr-conditions_files/figure-pdf/fig-linearity-not-met-1.png){#fig-linearity-not-met width=85%}\n:::\n:::\n\n\n<!--# is this framing too confusing bc \"no\" means satisfied?-->\n\nNow let's use the plot of residuals versus predicted values in @fig-coffee-linearity to assess the linearity condition for the model in @tbl-coffee-model. **Based on this plot, we conclude** **the linearity condition is satisfied.** We cannot determine with great certainty whether the residual will be positive or negative for any predicted value or range of predicted values, as the points are randomly scattered around the horizontal line at $y = 0$. Thus the linear model adequately captures the relationship between aroma and flavor grades, and the residuals capture the random variability in flavor grade due to sources other than the aroma.\n\n::: {.analysis_in_practice latex=\"\"}\nWe made the assessment about linearity considering the range of fitted values that represents the bulk of the data. Because there is only one outlying observation with a fitted value less than 6, we are unable to determine what the random variability of the residuals would look like for fitted values in this range. Thus the presence of a few extreme outliers. Outliers are discussed in more detail in @sec-model-diagnostics.\n:::\n\n### Independence\n\nThe next condition in LINE is **independence**, that the residuals are independent of each other and that the sign of one residual does not inform the sign of other residuals. <!--# less confusing way to explain this?-->This condition is primarily checked based on the given information about the subject matter and the data collection process. This condition can be more challenging to evaluate, especially if we do not have much information about the data. If the condition is violated, there are two general scenarios in which the violation occurred. If neither of these apply to the data, then it is usually reasonable to conclude that the independence condition is satisfied.\n\nThe first scenario in which independence is often violated is due to a **serial effect.** <!--# does this term need to be cited?--> This occurs when data have a chronological order or are collected over time, and there is some pattern in the residuals when examining them in chronological order. If the data have a natural chronological order or were collected over time, make a scatterplot of residuals versus time order. Similar to checking the linearity condition, seeing no discernible pattern in the plot indicates the independence condition is satisfied. In this case, it means that knowing something about the order in which an observation was collected does not give information about whether the model tends to over or under predict. Otherwise, the independence condition is violated.\n\n<!--# do i need a graph illustrating this?-->\n\nThe second common violation of independence is due to a **cluster effect**. <!--# does this term need to be cited?--> This is when there are subgroups present in the data that are not accounted for by the model. To check this condition, modify the plot of the residuals versus predicted by using color, shape, faceting, or visual features to differentiate the observations by the subgroup. Similar to checking the linearity condition, we expect the residuals to be randomly scattered above and below $y = 0$ for each subgroup. If this is not the case, there is indication that subgroup should be accounted for in the model. We will talk more about strategies for dealing with violations in the model conditions in @sec-conditions-not-satisfied and how to include categorical predictors in the model to account for subgroups in @sec-ch-mlr.\n\nBased on the description of the coffee data, we conclude that the independence condition is satisfied. We do not have evidence of a serial or cluster effect in the data description.\n\n<!--# add callout about spatial correlation. It's a type of sbugroup but a map is even more useful bc you can check correlation between neighboring areas-->\n\n### Normality\n\nThe next condition is **normality**, that the residuals are normally distributed. Though not explicitly part of the condition, we also know that the mean of the residuals, and thus the center of the distribution, is approximately 0. This condition arises from the assumption from @sec-slr-foundation that the distribution of the response variable, and thus the residuals, at each value of the predictor variable is normal. In practice, it would be nearly impossible to check the distribution of the response or residuals for each possible value of the predictor, so we will look at the overall distribution of the residuals to check the normality condition.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The distribution of the residuals for model of `flavor` and `aroma` with normal density curve](06-slr-conditions_files/figure-pdf/fig-coffee-normality-1.png){#fig-coffee-normality width=85%}\n:::\n:::\n\n\n@fig-coffee-normality shows the distribution of the residuals along with the theoretical curve for a normal distribution that is centered at 0 the mean of the residuals, and a standard deviation of 0.229, the standard deviation of the residuals. Similar to the other conditions, we are looking for obvious departures from normality when assessing this condition. This might include strong skewness, multiple modes, large outliers, or other major departures from normality.\n\nFrom @fig-coffee-normality, we see the distribution of the residuals is approximately normal, as the shape of the histogram is unimodal and symmetric, the general shape of the normal curve. Therefore, we conclude that **the normality condition is satisfied**.\n\n<!--# Should I introduce a goodness-of-fit test here, or is this fine?-->\n\n<!--# Maybe move the point about sample size that comes later here-->\n\n### Equal variance\n\nThe last condition is **equal variance** (also called *constant variance*), that the variability of the residuals is approximately equal for each predicted value. This condition stems from the assumption in @sec-slr-foundation that $\\sigma_{\\epsilon}^2$ is equal for all values of the predictor, and thus for all predicted values.\n\nTo check this condition, we will go back to the plot of the residuals versus predicted values from @fig-coffee-linearity. We look to see if the vertical spread of the residuals is approximately equal as we move along the $x$-axis. Since we are working with sample data, we don't expect the vertical spread of the residuals to be exactly equal as we move along the $x$-axis (for example, there may be outliers). Therefore, similar to previous conditions, we are looking for obvious departures from equal variance to conclude that the condition is not met. @fig-equal-var-not-met shows an example using simulated data of plot of residuals versus fitted when the equal variance condition is not satisfied. There is a distinguishable \"fan-shape\" showing that the spread of the residuals increases as the predicted value increases. Additionally, keep in mind that we are examining the vertical spread for the majority of the data, not including outliers in the assessments as shown by the dotted lines in @fig-coffee-equal-var to serve as a guide as we assess equal variance.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Example of residuals from simulated data that violate the equal variance condition. The vertical spread of the residuals increases as the predicted value increases.](06-slr-conditions_files/figure-pdf/fig-equal-var-not-met-1.png){#fig-equal-var-not-met width=85%}\n:::\n:::\n\n\n<!--# Is there a way to draw the lines for checking equal variance?-->\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plot of the residuals versus predicted values. The vertical spread is approximately for each predicted value, as shown by the residuals generally falling between the two dotted lines when moving along the $x$-axis.](06-slr-conditions_files/figure-pdf/fig-coffee-equal-var-1.png){#fig-coffee-equal-var width=85%}\n:::\n:::\n\n\nFrom @fig-coffee-equal-var, the vertical spread of the residuals is approximately equal, as the vast majority of the residuals are between the two dotted lines as we move along the $x$-axis. Therefore, **the equal variance condition is satisfied.**\n\n<!--# Consider including a test or more robust way to assess equal variance.-->\n\n::: {.analysis_in_practice latex=\"\"}\n**Checking model conditions**\n\n-   **Linearity**: Plot residuals versus predicted values. Look for points to be randomly scattered around 0 on the $y$-axis $(\\text{residuals} = 0)$ .\n\n-   **Independence**: Use the description of the data and data collection process to assess if the observations can reasonably be treated as independent. If data are collected over time, examine plot of residuals versus time to assess potential serial effect. If unaccounted for subgroups are represented in the data, examine plot of residuals versus predicted for each subgroup.\n\n-   **Normality**: Plot the distribution of the residuals. Look for a distribution that is approximately unimodal and symmetric.\n\n-   **Equal variance**: Plot the residuals versus predicted values. Look for approximately equal vertical spread as we move along the $x$-axis.\n:::\n\n## Strategies when conditions aren't satisfied {#sec-conditions-not-satisfied}\n\nIf all the model conditions are satisfied, we can move on to the next step in the analysis; however, that is sometimes not the case in practice. The good news is we there are a variety of methods to address violations in the model conditions. Additionally, many of the analysis tasks we do are robust to violations in some of the model conditions, meaning we can still obtain reliable analysis results, even if some conditions are not satisfied.\n\nFor each model condition we will discuss the analysis tasks for which satisfying the model condition is very important and some approaches we can take to address the violations in the condition.\n\n### Linearity\n\nThe linearity condition is the most important condition, because the regression analysis relies on the assumption that the linear regression model adequately summarizes the relationship between the response and predictor variable (see @sec-slr-appropriate). Therefore, the linearity condition must be satisfied in order for any interpretations and conclusions from the regression analysis to be useful and reliable. This means it is necessary for fitting the linear model, interpreting the model coefficients, drawing conclusions inference conducted using simulation-based or theory-based methods.\n\nIf the linearity condition is not satisfied, we can ask ourselves a few questions:\n\n1.  Is the violation in the linearity condition due to the fact that there is, in fact, no evidence of a meaningful relationship between the response and predictor variable? In this case, reconsider if a linear regression model is the most useful way to understand these data.\n2.  Is the violation in the linearity condition because there is evidence of a meaningful non-linear relationship between the response and predictor variable? If this is the case, there are a few options to work with this non-linear relationship.\n    -   Add a higher-order term, for example $x^2$, in the model to capture the non-linearity of the relationship. We discuss higher-order model terms in more detail in @sec-ch-mlr. <!--# add reference at some point-->\n    -   Apply a transformation on the response and/or predictor variable, so there is a linear relationship between the transformed variables. Then proceed with simple linear regression using the transformed variable(s). We discuss variable transformations in @sec-ch-transformations.\n\n### Independence\n\nThe independence condition states that the residuals are independent of one another. This assurance is most important when estimating variability about the line or variability in the estimated regression coefficients when doing statistical inference. If the independence condition is not satisfied and some residuals are correlated with one another, then our procedures may underestimate the true variability about the regression line and ultimately the variability in the estimated regression coefficients. <!--# add link to inference chapter?--> This is due to the fact that each observation is not fully contributing new information, because there is some correlation between two or more observations. Thus the **effective sample size**, or how many observations independently provide information, is smaller than the true sample size $n$. This could result in misleading inferential conclusions, such as concluding there is a linear relationship between the response and predictor variable, when in fact there is no such a relationship.\n\nIf the independence condition is not met, more advanced statistical methods would be required to properly deal with the correlated residuals. These methods are beyond the scope of this book, but you can learn more about these methods called **multilevel models** or **hierarchical models** in books such as @roback2021beyond.\n\nIf we observe some pattern in the residuals based on subgroup, then there are two approaches to address it. The first is creating a separate model for each subgroup. The primary disadvantage to this approach is that we may have a different estimate for the slope and intercept for each subgroup, and thus it may be more difficult to draw overall conclusions about the relationship between the response and predictor variables. The second option is to fit a model that includes the subgroup as a predictor. This will allow us to account for differences by subgroup while maintaining a single model that makes it easier to draw overall conclusions. This type of model is called a **multiple linear regression** model, because it includes more than one predictor variable. We will introduce these models more in detail in @sec-ch-mlr.\n\nIf the violations in the independence condition are not addressed, we must use caution when drawing conclusions from the model or calculating predictions given the systematic under or over prediction by subgroup. In this case, we should consider if the model can be used to produce the type of valuable inferential insights we intend.\n\n<!--# needs some discussion about what to do if time is the issue-->\n\n<!--# Need to work in that the condition doesn't really matter if you just want estimates of the slope and intercept with no plan to do inference-->\n\n### Normality\n\nBy the Central Limit Theorem, we know the distribution of the estimate slope (the statistic of interest) is normal when the sample size is sufficiently large (see @sec-slr-inf-clt for more details), regardless of the distribution of the original data. Therefore, when the sample size is \"large enough\", we can be confident that the distribution of the estimated slope $\\hat{\\beta}_1$ is normal, even if the residuals (and thus the response variable) do not follow a normal distribution.\n\nA common threshold for a \"large enough\" sample size is 30. This means if the sample has at least 30 observations (which is often the case for modern data sets!), we can use all the methods we've discussed to fit a simple linear regression model and draw conclusions from inference, even if the residuals are not normally distributed. Thus the inferential methods based on the Central Limit Theorem are considered robust to violations in the normality assumption, because we can feel confident about the conclusions from the inferential procedures even if the normality condition is not satisfied (given $n > 30$). One note of caution is that if the sample size is very close to 30, then the inferential methods based on the Central Limit Theorem are not robust to major departures form normality such as extreme skewness or outliers.\n\nRecall from @sec-ch-slr-inf the objective of simulation-based inference is to use the data to generate the sample distribution. Therefore, we do not make any assumptions about the sampling distribution of the estimated slopes. Because there are no assumptions about the sampling distribution, the normality condition is not required for simulation-based inferential procedures.\n\n::: {.analysis_in_practice latex=\"\"}\nThirty should not be treated as an absolute threshold. In other words, we should not treat data with a sample size of 29 materially different from data with a sample size of 31. If the sample size is small:\n\n-   The distribution of the residuals should not have large departures from normality, in order to have reliable conclusions from the CLT-based inferential results.\n-   You can conduct simulation-based inference, because these methods do not rely any assumptions about the sampling distribution of $\\hat{\\beta}_1$, and thus do not rely on assumptions about the distribution of the residuals.\n:::\n\n### Equal variance\n\nThe equal variance condition is as assessment of whether the distribution of the residuals is approximately equal for each value of the predictor variable. This is equivalent to the distribution of the response about the line for each predictor variable. The **regression standard error** $\\hat{\\sigma}_{\\epsilon}$ is the estimated value of this variability about the line, and it is the assumed to be the same for all values of the predictor (see @sec-slr-foundation for more detail). When conducting inference based on the Central Limit Theorem, this regression standard error is used to calculate $SE_{\\hat{\\beta}_1}$, the estimated standard error for the estimated slopes. Thus similar to violations in the independence condition, if the variability about the regression line is not equal for all values of the predictor, the regression standard error will not be accurate and thus the standard error of the slope, our estimate of the sampling variability in this statistic will be inaccurate as well. The calculation of the confidence interval and test statistic for hypothesis testing both use $SE_{\\hat{\\beta}_1}$, so these values will likely be inaccurate as well. Therefore, we should approach conclusions from the theory-based inferential procedures with caution if the equal variance condition is not met.\n\nSimilar to the normality condition, the equal variance condition is not required for simulation-based inference, because $SE_{\\hat{\\beta}_1}$ is estimated from the sampling distribution simulated from the data.\n\nThere are approaches for handling violations in the equal variance condition. These approaches typically involve performing a transformation on the response variable. We will discuss these further in \\@\\@sec-ch-transformations.\n\n### Recap\n\n@tbl-slr-conditions provides a summary of the model conditions and whether they must be satisfied to reliably conduct each modeling task.\n\n+-----------------+-----------------------+----------------------------+------------------------+\n| Model condition | Interpret and predict | Simulation-based inference | Theory-based inference |\n+=================+=======================+============================+========================+\n| Linearity       | Important             | Important                  | Important              |\n+-----------------+-----------------------+----------------------------+------------------------+\n| Independence    | Important             | Important                  | Important              |\n+-----------------+-----------------------+----------------------------+------------------------+\n| Normality       | Not needed            | Not needed                 | Not needed if $n > 30$ |\n|                 |                       |                            |                        |\n|                 |                       |                            | Important if $n < 30$  |\n+-----------------+-----------------------+----------------------------+------------------------+\n| Equal variance  | Not needed            | Not needed                 | Important              |\n+-----------------+-----------------------+----------------------------+------------------------+\n\n: Summary of necessary conditions for various analysis tasks on the slope {#tbl-slr-conditions}\n\nSo far we have discussed the model conditions to assess whether the assumptions for simple linear regression reasonably hold true for our data. We have not, however, dealt with the presence of outliers in the data and the potential impact (or \"influence\") they may have on the regression model. In the next section, we discuss measures we can use to identify and assess the potential impact of such outlying observations.\n\n## Model diagnostics {#sec-model-diagnostics}\n\nOne challenge that may arise when doing regression analysis (or really any type of data analysis) is how to handle outliers in the data. Because we are focused on the linear relationship between two (and eventually three or more) variables, there are multiple ways in which the outliers may show up in our analysis:\n\n1.  Outlier in the value of the predictor variable\n2.  Outlier in the value of the response variable\n3.  Outlier outside the general pattern of the relationship between the response and predictor variables\n\nA single observation could fall into multiple categories. For example look at the outlier identified in @fig-coffee-eda. This observation is around the center of the distribution for the response variable `flavor` but has a low outlying value in terms of the predictor `aroma`. Therefore, it is an outlier as described by categories (1) and (3) above. There is a set of model diagnostics to assess if there are observations in the data that fall into one or more of these outlier scenarios, and more importantly, if any such outlying observations have an out-sized impact on the regression model.\n\n### Influential points\n\nAn observation is considered an **influential point** if the estimated regression coefficients noticeably differ when the point is included in the data used to fit the model versus when it is not. @fig-influential-point illustrates the potential impact an influential point can have on the estimated regression line. Additionally, influential points can affect $SE_{\\hat{\\beta}_1}$, estimated standard error of the slope, which can result in unreliable inferential conclusions as described in the previous section.\n\n\n::: {#fig-influential-point .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![With influential point](06-slr-conditions_files/figure-pdf/fig-influential-point-1.png){#fig-influential-point-1 width=85%}\n:::\n\n::: {.cell-output-display}\n![Without influential point](06-slr-conditions_files/figure-pdf/fig-influential-point-2.png){#fig-influential-point-2 width=85%}\n:::\n\nSimulated data showing the effect of an infulential point representd by the red triangle. The slope of the line changes when the influential point is removed from the data set.\n:::\n\n\nSometimes potential influential points can be identified in a scatterplot of the response versus predictor variable in the exploratory data analysis. In the EDA, we have called out these points as being outliers, as they are typically outside of the general trend of the relationship between the two variables. Identifying these points from the EDA can become more difficult, however, when there are multiple predictor variables (see @sec-ch-mlr). Therefore, we will use a set of model diagnostics to identify observations that are outliers and perhaps more importantly, those that are influential.\n\nThere are three diagnostic measures to identify outliers and influential points:\n\n-   **Leverage:** Identify observations that are outliers in the values of the predictor\n-   **Standardized residuals:** Identify observations that are outliers in the value of the response\n-   **Cook's distance:** Identify observations that are influential (a combination of leverage and standardized residuals)\n\nUltimately, we are most interested in examining Cook's distance, because it provides the indication about each observation's impact on the regression analysis. Cook's distance takes into account information gleaned from leverage and standardized residuals, so we will introduce these diagnostics first.\n\n### Leverage {#sec-slr-leverage}\n\nThe **leverage** of the $i^{th}$ observation is a measure of the distance between its value of the predictor, $x_i$, and the average value of the predictor across all $n$ observations, $\\bar{x} = \\frac{1}{n}\\sum_{j=1}^n x_j$.\n\nWhen doing simple linear regression, an observation's leverage, denoted $h_i$, is calculated using @eq-slr-leverage.\n\n$$\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n(x_j - \\bar{x})^2}\n$$ {#eq-slr-leverage}\n\n<!--# Maybe provide more explanation about where this formula comes from?-->\n\nThe values of the leverage are between $\\frac{1}{n}$ and 1 for each observation in the data set, and the average value of leverage across all $n$ observations is $\\frac{(p+1)}{n}$, where $p$ is the number of predictors in the model. In simple linear regression $p = 1$, thus the average leverage is $\\frac{2}{n}$.\n\nAn observation is considered to have *large leverage* if its leverage is greater than $\\frac{2(p+1)}{n}$ , i.e., greater than $\\frac{2 \\times 2}{n}$ for simple linear regression. This means that an observation is considered to have large leverage if its value of leverage is greater than twice the average value of leverage for the observations in the data set.\n\n<!--# briefly explain why we're using h and point reader to appendix for details in matrix notation-->\n\n::: {.analysis_in_practice latex=\"\"}\nLeverage only depends on the values of the predictor variable(s). It does [**not**]{.underline} depend on values of the response.\n:::\n\nLet's take a look at the leverage for the observations in the coffee ratings data set. There are 1338 observations in the data and one predictor variable, so the threshold for identifying observations with large leverage is $\\frac{2 \\times 2}{1338} =$ 0.003.\n\n\n::: {.cell}\n\n:::\n\n\n@fig-coffee-leverage is a visualization of the leverage for each observation in the data set. The red, horizontal dotted line marks the threshold for identifying points as having large leverage. There are 114 observations that have large leverage.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Leverage for observations in the coffee ratings data](06-slr-conditions_files/figure-pdf/fig-coffee-leverage-1.png){#fig-coffee-leverage width=85%}\n:::\n:::\n\n\n::: {.yourturn latex=\"\"}\nThere are 114 observations that have large values for the leverage. What does it mean for an observation to have \"large leverage\" in the context of the coffee data?\n:::\n\nLet's take a look at the observations with the largest values of leverage to get a better understanding about which points may be large leverage.\n\n\n::: {#tbl-coffee-large-leverage .cell tbl-cap='Observations with the top five highest values for leverage'}\n::: {.cell-output-display}\n\n\n| Leverage| aroma| flavor|\n|--------:|-----:|------:|\n|    0.047|  5.08|   7.75|\n|    0.015|  6.17|   6.50|\n|    0.012|  6.33|   6.50|\n|    0.011|  8.75|   8.67|\n|    0.011|  6.42|   6.50|\n\n\n:::\n:::\n\n\nRecall that the leverage only depends on the values of the predictor variable, so we only need to consider `aroma` to better understand why these points are large leverage. From the EDA in @sec-coffee-intro, the average aroma grade in the data set is 7.572 points and the standard deviation is 0.316 . The observations with the largest values of leverage have aroma grades that are either much higher (e.g., 8.75) or much lower (e.g., 5.08) than the average. Thus the large leverage points includes coffees that both smell much better and perhaps much worse than the majority of the coffees in the data.\n\nEven when we identify points with large leverage, we still want to understand how these points are (or are not) influencing the model results. Thus, knowing an observation has large leverage is not enough to warrant any specific action. We'll examine Cook's distance to determine if the observation is influential in the model in @sec-cooks-distance and discuss approaches to deal with such influential points in @sec-outliers-impact .\n\n### Standardized residuals {#sec-slr-std-resid}\n\nThe residual is an indication of how well the regression model predicts the value of the response for a given observation. One reason the model may predict poorly for an observation is because it has an observed value of the response that is much different than the rest of the data. Therefore residuals can be used to identify observations that are outliers based on Scenario 2 in @sec-model-diagnostics. We will examine the magnitude of the residuals $|e_i|$ to identify such points, because residuals can be positive (model underpredicted) or negative (model overpredicted).\n\nThe residuals have the same units as the response variable, so what is considered a residual with large magnitude depends on the scale and range of the response variable. This means what is considered a \"large\" residuals can be different for every analysis. We can address this by instead examining **standardized residuals**, defined as the residual divided by its standard error. @eq-std-resid shows the formula to calculate the standardized residual for the $i^{th}$ observation.\n\n$$\nstd.resid_i = \\frac{e_i}{SE_{e_i}} = \\frac{e_i}{\\hat{\\sigma}_{\\epsilon} \\sqrt{1 - h_i}}\n$$ {#eq-std-resid}\n\nThe residuals for every analysis are now on the same scale. Thus, we can use a common threshold to determine residuals that are considered to have large magnitude. An observation is an outlier in the response if it its standardized residual has a magnitude greater than two, $|std.resid_i| > 2$ ($std.resid_i < - 2 \\hspace{1mm} \\text{ or }\\hspace{1mm} std.resid_i > 2$). For large data sets, we recommend a more restrictive threshold of 3, so there is not an overwhelming number of observations considered outliers that may require further evaluation.\n\n::: {.analysis_in_practice latex=\"\"}\nOne assumption for linear regression is that the residuals are normally distributed and centered at 0 (see @sec-slr-foundation). This means the standardized residuals are normally distributed with mean of 0 and standard deviation of 1, $N(0,1)$. In a standard normal distribution, we expect about 95% of the observations to have values between -2 and 2. By setting 2 as the threshold, we are essentially identifying the 5% of observations with the most extreme (high and low) residuals.\n\nIf there are a lot of observations, it may not be practical to closely examine the approximately 5% of observations that were identified as potential outliers. Thus we may use a more restrictive threshold, e.g., $|std.resid_i| > 3$ (or higher) to identify points worth more close examination or only focus on those that are identified as influential points by Cook's distance.\n:::\n\n<!--# add callout connecting large standardized residuals to large leverage?-->\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Standardized residuals versus observation number for model of `flavor` and `aroma`](06-slr-conditions_files/figure-pdf/fig-coffee-stdresid-1.png){#fig-coffee-stdresid width=85%}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n<!--# maybe use visualization to show why these have big std residuals-->\n\n@fig-coffee-stdresid shows 12 observations with large magnitude residuals based on the threshold $\\pm 3$ . This includes observations that have positive residuals with large magnitude (indicating the model greatly underpredicted) and observations with negative residuals with large magnitude (indicating the model greatly overpredicted).\n\nThese five observations with the largest magnitude residuals are shown in @tbl-coffee-large-resid.\n\n<!--# think of a way to visualize observations with large magnitude residuals-->\n\n\n::: {#tbl-coffee-large-resid .cell tbl-cap='Observations with large magnitude residuals'}\n::: {.cell-output-display}\n\n\n| aroma| flavor| .fitted| .std.resid|\n|-----:|------:|-------:|----------:|\n|  5.08|   7.75|    5.53|       9.90|\n|  7.17|   6.08|    7.20|      -4.90|\n|  7.17|   6.17|    7.20|      -4.51|\n|  7.92|   6.83|    7.80|      -4.25|\n|  7.00|   6.17|    7.07|      -3.92|\n\n\n:::\n:::\n\n\nWe see from @tbl-coffee-large-resid and @fig-coffee-large-resid that these observations have flavor ratings that are lower than would be expected from the model based on their aroma ratings. The exception is the observation with the largest magnitude standardized residual that has a higher actual flavor rating than the model predicts based on its very low aroma rating.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Flavor grade versus aroma grade. Points marked with a red triangle are the observations with large standardized residuals with magnitude > 3.](06-slr-conditions_files/figure-pdf/fig-coffee-large-resid-1.png){#fig-coffee-large-resid width=85%}\n:::\n:::\n\n\n::: {.analysis_in_practice latex=\"\"}\nWe can use a plot of standardized residuals versus predicted values such as the one in @fig-coffee-stdresid to check the linearity and equal variance conditions from @sec-model-conditions.\n:::\n\nAs with observations that have large leverage, we want to assess whether these points have out-sized influence on the model coefficients to help determine how to deal with these outliers (or if we need to do anything at all). To do so, we will look at the last diagnostic, Cook's distance.\n\n### Cook's distance {#sec-cooks-distance}\n\nAt this point we have introduced a diagnostic to identify outliers in the predictor variable (leverage) and one to identify outliers in the response variable (standardized residuals). Now we will use a single measure that combines information from the leverage and standardized residuals to identify potential influential points. This measure is called Cook's distance.\n\n**Cook's distance** is a measure of an observation's overall impact on the estimated model coefficients. In @eq-cooks-d, we see that Cook's distance takes into account an observation's leverage, $h_i$, the standardized residuals, $std.resid_i$, and the number of predictors in the model, $p$ ( $p=1$ for simple linear regression).\n\n$$\nD_i = \\frac{1}{p}(std.resid_i)^2\\Big(\\frac{h_i}{1-h_i}\\Big)\n$$ {#eq-cooks-d}\n\n<!--# add something referring reader to the appendix-->\n\n<!--# do i need to explain cook's distance? I think that's out of scope - maybe put something in the appendix?-->\n\nAs with leverage and standardized residuals, there are thresholds we use to determine if a point is potentially influential. A commonly used threshold is 1. If an observation has a Cook's distance greater than 1, $D_i > 1$, it is considered an influential point and is worth closer examination given its potential influence on the model.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Cook's distance versus observation number for coffee ratings data](06-slr-conditions_files/figure-pdf/fig-coffee-cooksd-1.png){#fig-coffee-cooksd width=85%}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n@fig-coffee-cooksd shows the values of Cook's distance for the observations in the data set. It includes a horizontal line at 1 marking the threshold for determining if an observation is influential. There is one observation in the data that has a value of Cook's distance greater than 1; its value is 2.432. This is the observation with an aroma grade of `large_cooks$aroma` and flavor grade of `large_cooks$flavor`.\n\n### Handling influential points {#sec-outliers-impact}\n\nTo better understand the impact of influential points, let's take a look at our model fit with and without the influential observation.\n\n\n::: {#tbl-influential-impact .cell tbl-cap='Estimated model coefficients for model with and without influential observation'}\n::: {.cell-output-display}\n\n\n|Term        | With Influential Point| Without Influential Point|\n|:-----------|----------------------:|-------------------------:|\n|(Intercept) |                   1.47|                     1.137|\n|aroma       |                   0.80|                     0.843|\n\n\n:::\n:::\n\n\nIn @tbl-influential-impact, the estimates for the intercept and coefficient of `aroma` change based on whether the influential point is included or not included in the data. Therefore, it is now our task as the data scientist to determine how to best proceed given the influential point.\n\n#### Outlier based on predictor {.unnumbered}\n\nIf an observation is an outlier based on the predictor variable, first consider if it is the result of a data entry error that can be fixed. If not, one option is to remove this observation from the analysis, particularly if the outlier is an influential point or a result of a data entry error. Doing so limits the scope of the conclusions that can be drawn from the analysis and the range of values for which the model can be used for prediction, because removing the outlying observation narrows the range of possible values for the predictor. When taking this approach, it is important to mention it when communicating results. Additionally, we need to be careful not to extrapolate if using the model for prediction. <!--# maybe clarify this a bit?-->\n\nThe influential point identified in @sec-cooks-distance is an outlier because it has a value of the predictor much lower than the rest of the data. Therefore, we could remove this observation and thus limit the scope of the analysis to coffee that has an aroma rating of 6 or higher. We can also keep this observation in the analysis and thus have some information about coffees with aroma ratings less than 6. Given there is a single observation with a rating less than 6, however, conclusions about coffees with low coffee ratings should be made with caution.\n\n#### Outlier based on response {.unnumbered}\n\nIf the observation is an outlier based on the value of the response variable, first consider if it is the result of a data entry error. If it is not and the value of the predictor is within the range of the other observations in the data, the observation should not be removed from the analysis. Removing the observation could result in taking out useful information for understanding the relationship between the response and predictor variable.\n\nIf the outlier is influential, one approach is to build the model with and without the influential point, then compare the estimated values and inference for the model coefficients. Another approach is to transform the response variable to reduce the effect of outliers (more on transformations in \\@\\@sec-ch-transformations). A last approach is to collect more data, so the model information can be informed by more observations; however, collecting more data is often not feasible in practice.\n\n## Modeling in practice {#sec-model-in-practice}\n\nIn this chapter, we have introduced a set of model conditions and diagnostics to evaluate how well the model fits the data and satisfies the assumptions for linear regression. Now we'll conclude the chapter by discussing how we might incorporate these tools in practice.\n\n1.  **Exploratory data analysis (EDA)**: Start every regression analysis with some exploratory data analysis, as in @sec-coffee-intro . The EDA will help you better understand the data, particularly the variables that will be used in the regression model. Through the EDA we may also notice points that stand out as potential outliers and potential influential points that may be of interest for further exploration. The EDA can provide valuable insights about the data; however, it cannot be used alone to confirm whether the model is a good fit or confirm if a point is influential. We need the model conditions and diagnostics to make such a robust assessment. The EDA is important, however, to help determine the best analysis approach and to understand the data enough to more fully understand the model interpretations and influential conclusions.\n\n2.  **Model fitting:** Fit the regression model.\n\n3.  **Model conditions and diagnostics**: Check the model conditions (LINE) to assess whether the model is a good fit for the data.\n\n    Then, check Cook's distance to determine if there are potential influential points in the data.\n\n    -   If there are influential points, check the leverage and standardized residuals to try to understand why these points are influential. Use this information, along with the analysis objective and subject matter expertise, to determine how to best proceed with the dealing with these points.\n\n    -   If there are no influential points, we can examine the leverage and standardized residuals for a deeper understanding of the data; however, no further action is needed in terms of refitting the model.\n\n    Make any adjustments as needed based on the model conditions and diagnostics.\n\n    ::: {.analysis_in_practice latex=\"\"}\n    Modeling in practice is an iterative process. We repeat Steps 2 and 3 until we find a model that is the best fit for the data.\n    :::\n\n4.  **Prediction and inference**: Once the model is finalized, and we are satisfied with the results of the model conditions and diagnostics, then we can use the model for prediction and inference to answer the analysis questions.\n\n5.  **Communication**: Once we have the final inferential results and predictions, communicate the overall conclusions in a way that can be easily understood by the intended audience.\n\n<!--# add something here dealing with the outlier we found-->\n\n## Model conditions and diagnostics in R {#sec-conditions-in-r}\n\nThe residuals, along with other observation-level statistics, can be obtained using `augment()` in the **broom** R package [@broom]. This package is in the suite of packages that are included as part of **tidymodels**[@tidymodels]. The output produced by this function is shown in @tbl-coffee-aug.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoffee_model <- lm(flavor ~ aroma, data = coffee_ratings)\n\ncoffee_model_aug <- augment(coffee_model)\n```\n:::\n\n\n\n::: {#tbl-coffee-aug .cell tbl-cap='First ten rows of output produced by the `augment` function'}\n::: {.cell-output-display}\n\n\n| flavor| aroma| .fitted| .resid|    .hat| .sigma| .cooksd| .std.resid|\n|------:|-----:|-------:|------:|-------:|------:|-------:|----------:|\n|   8.83|  8.67|    8.40| 0.4259| 0.00978|  0.229| 0.01715|      1.864|\n|   8.67|  8.75|    8.47| 0.2019| 0.01114|  0.230| 0.00440|      0.884|\n|   8.50|  8.42|    8.20| 0.2959| 0.00613|  0.230| 0.00515|      1.293|\n|   8.58|  8.17|    8.00| 0.5758| 0.00342|  0.229| 0.01084|      2.513|\n|   8.50|  8.25|    8.07| 0.4319| 0.00419|  0.229| 0.00747|      1.885|\n|   8.42|  8.58|    8.33| 0.0878| 0.00836|  0.230| 0.00062|      0.384|\n|   8.50|  8.42|    8.20| 0.2959| 0.00613|  0.230| 0.00515|      1.293|\n|   8.33|  8.25|    8.07| 0.2619| 0.00419|  0.230| 0.00275|      1.143|\n|   8.67|  8.67|    8.40| 0.2658| 0.00978|  0.230| 0.00668|      1.164|\n|   8.58|  8.08|    7.93| 0.6479| 0.00268|  0.229| 0.01072|      2.826|\n\n\n:::\n:::\n\n\nWe use the following columns produced by `augment()` to check model conditions and diagnostics:\n\n-   `.fitted`: Predicted values\n-   `.resid`: Residuals\n-   `.hat`: Leverage\n-   `.cooksd`: Cook's distance\n-   `.std.resid`: Standardized residuals\n\nThe plots used to check the model conditions and diagnostics are produced using `ggplot()` functions. We can also produce similar plots in base R format using the `plot` function. The QQ-plot produced from `plot()` is used to check the normality condition. The distribution of the residuals is approximately normal if the points lie along the diagonal line.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06-slr-conditions_files/figure-pdf/unnamed-chunk-4-1.png){width=85%}\n:::\n\n::: {.cell-output-display}\n![](06-slr-conditions_files/figure-pdf/unnamed-chunk-4-2.png){width=85%}\n:::\n\n::: {.cell-output-display}\n![](06-slr-conditions_files/figure-pdf/unnamed-chunk-4-3.png){width=85%}\n:::\n\n::: {.cell-output-display}\n![](06-slr-conditions_files/figure-pdf/unnamed-chunk-4-4.png){width=85%}\n:::\n:::\n\n\n## Summary\n\nIn this chapter, we introduced model conditions and diagnostics to evaluate the fit of a simple linear regression model. We discussed the LINE model conditions: linearity, independence, normality, and equal variance, and how they are used to check the assumptions for linear regression. We also discussed scenarios in which assumptions are important or may be relaxed. We used model diagnostics, specifically Cook's distance, leverage, and standardized residuals, to identify outliers. We discussed strategies for handling influential points that may impact model results. We concluded by describing the full process of modeling in practice and how to implement the concepts from this chapter in R.\n\nThus far, we have discussed simple linear regression, using one predictor variable to understand variability in a response variable. In the next chapter, we will extend what we know about simple linear regression to models with two or more predictor variables, called *multiple linear regression*.\n\n<!--# Exercises -->\n\n<!--# Exercise idea: Can you make the plot using X instead of the fitted values. Why or why not?-->\n\n<!--# does this chapter need more \"your turn\" opportunities?-->\n",
    "supporting": [
      "06-slr-conditions_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}