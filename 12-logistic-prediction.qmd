# Logistic regression: Prediction and evaluation {#sec-ch-logistic-prediction}

```{r}
#| include: false
source("_common.R")
```

## Learning goals {.unnumbered}

-   Compute predictions from the logistic regression model
-   Use model predictions to classify observations
-   Construct and interpret a confusion matrix
-   Use the ROC curve to evaluate model performance and select classification threshold
-   Evaluate model performance using AUC, AIC, and BIC
-   Implement a model building workflow for logistic regression using R

```{=html}
<!--#
## Software and packages {.unnumbered}

-   `tidyverse` [@tidyverse]

-   `tidymodels` [@tidymodels]

-   `knitr` [@knitr]

-   `pROC` [@pROC]
-->
```

## Introduction: Predicting comfort with driverless cars

In @sec-ch-logistic, we introduced data from the 2024 General Social Survey in which adults in the United States their opinions were asked on a variety of issues, including their comfort with driverless cars. In the previous chapter, we fit a logistic regression model used the model to describe the characteristics associated with the odds an adult is comfortable with driverless cars. We continue with the analysis in this chapter, with a focus on using the model for prediction. We will also evaluate the model performance and show an example workflow for comparing two logistic regression models.

We use the variables below in this chapter. The variable definitions are based on survey prompts and variable definitions in the *General Social Survey Documentation and Public Use File Codebook* [@gss2024].

-   `aidrive_comfort`: Indicator variable for respondent's comfort with driverless (self-driving) cars. `0`: Not comfortable at all; `1`: At least some comfort.

    -   This variable was derived from responses to the original survey prompt: "Comfort with driverless cars". Scores ranged from 0 to 10 with 0 representing "totally uncomfortable with this situation" and 10 representing "totally comfortable with this situation". Responses of 0 on the original survey were coded as `aidrive_comfort = 0`. All other responses coded as 1.

-   `tech_easy` : Response to the question, "Does technology make our lives easier?" Categories are `Neutral`, `Can't choose` (Respondent doesn't know / is unable to provide an answer), `Agree`, `Disagree`.

-   `age`: Respondent's age in years

-   `income`: Response to the question "In which of these groups did your total family income, from all sources, fall last year? That is, before taxes." Categories are `Less than $20k`, `$20-50k`, `$50-110k`, `$110k or more`, `Not reported` .

    -   Note: These categories were defined based on the 27 categories in `income16` from the original survey.

-   `tech_harm`: Response to the question, "Does technology do more harm than good?". Categories are `Neutral`, `Can't choose` (Respondent doesn't know / is unable to provide an answer), `Agree`, `Disagree`.

-   `polviews`: Response to the question, "I'm going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal--point 1--to extremely conservative--point 7. Where would you place yourself?" Categories are `Moderate`, `Liberal`, `Conservative`, `No reported`.

    -   Note: These categories were defined based on the original 7 point scale.

<!--# do i want to put Sex back in this chapter?-->

```{r}
#| echo: false

library(tidyverse)
library(tidymodels)
library(ggridges)
library(knitr)
library(Stat2Data)
library(pROC)

# load data and level the factors
gss24_ai <- read_csv("data/gss24-ai.csv") |>
   mutate(income_fct = factor(income_fct, levels = 
                                c("Less than $20k", "$20-50k", "$50-110k", "$110k or more", "Not reported")), 
         polviews_fct = factor(polviews_fct, levels = 
                                 c("Moderate", "Liberal", "Conservative", "Not reported")),
         tech_harm = factor(tech_harm, levels = c("Neutral", "Can't choose", "Agree", "Disagree")),
         aidrive_comfort = factor(aidrive_comfort), 
         tech_easy = factor(tech_easy, levels = c("Neutral", "Can't choose", "Agree", "Disagree"))
   )


# main effects model
m <- glm(aidrive_comfort ~ sex + age + income_fct + tech_harm + tech_easy, data = gss24_ai, family = binomial)
```

## Exploratory data analysis

### Univariate EDA

We conducted exploratory data analysis for the response variable `aidrive_comfort` and the predictors `age` and `tech_easy` in @sec-logistic-intro-gss. Here we focus on EDA for the new predictors and their relationship with the response variable.

```{r}
#| label: fig-aidrive-eda-2
#| fig-cap: Univariate exploratory data analysis 
#| fig-subcap: 
#|   - "`income`"
#|   - "`polviews`"
#|   - "`tech_harm`"
#| layout-ncol: 2

ggplot(data = gss24_ai, aes(x = income_fct)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(x = "",
       y = "Count") +
  theme_bw() 

ggplot(data = gss24_ai, aes(x = polviews_fct)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(x = "",
       y = "Count") +
  theme_bw() 

ggplot(data = gss24_ai, aes(x = tech_harm)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(x = "",
       y = "Count") +
  theme_bw() 
  
```

@fig-aidrive-eda-2 shows the distributions of the predictors that are new in this chapter. From the distribution of `income` in @fig-aidrive-eda-2-1 , we see that the most common response for income is between \$50-110K. There is a sizable proportion of the respondents who did not report an income. Studies have shown that failure to report income in surveys is not random [e.g., @jabkowski2024not], so it will be worth noting if the `Not Reported` indicator has a statistically significant relationship with the response variable as we continue the analysis.

The distribution of `polviews_fct` in @fig-aidrive-eda-2-2 shows a relatively even distribution along the range of political views. A few respondents chose not to report their political views. Lastly, the distribution of `tech_harm` in @fig-aidrive-eda-2-3 shows that most people either disagree are have neutral feelings about the statement that technology causes more harm than good.

### Bivariate EDA

```{r}
#| label: fig-aidrive-bivariate-eda-2
#| fig-cap: "Bivariate exploratory data analysis. Blue: `adrive_comfort = 0`, Red: `aidrive_comfort = 1`"
#| fig-subcap: 
#|   - "`aidrive_comfort` vs. `income`"
#|   - "`aidrive_comfort` vs. `polviews`"
#|   - "`aidrive_comfort` vs. `tech_harm`"
#| layout-ncol: 2


ggplot(data = gss24_ai, aes(x = income_fct, fill = aidrive_comfort)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "",
       y = "Proportion") +
  theme_bw()  +
   scale_fill_manual(values = c("#1d457f", "#cc5c76"))

ggplot(data = gss24_ai, aes(x = polviews_fct, fill = aidrive_comfort)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "",
       y = "Proportion") +
  theme_bw() +
   scale_fill_manual(values = c("#1d457f", "#cc5c76"))

ggplot(data = gss24_ai, aes(x = tech_harm, fill = aidrive_comfort)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "",
       y = "Proportion") +
  theme_bw() +
   scale_fill_manual(values = c("#1d457f", "#cc5c76"))
```

The visualizations in @fig-aidrive-bivariate-eda-2 show the relationships between the response variable and each new predictor variable. @fig-aidrive-bivariate-eda-2-1 shows the relationship between `income` and `aidrive_comfort`. The graph shows that a higher proportion of respondents in the higher income categories are comfortable with driverless cars compared to respondents in lower income categories or who did not report income. This suggests an individual's income may be useful in understanding the chance they are comfortable with driverless cars.

The relationship between `polviews` and `aidrive_comfort` is in @fig-aidrive-bivariate-eda-2-2. Those who identify as "liberal" on the political spectrum are the most likely to be comfortable with driverless cars, and those who did not report a political affiliation are the least likely. Those who identify as "moderate" or "conservative" have about the same odds of being comfortable with driverless cars.

Lastly, @fig-aidrive-bivariate-eda-2-3 is the relationship between `tech_harm` and `aidrive_comfort`. Those who disagree that technology causes more harm than good are the most likely to be comfortable with driverless cars. Those who did not provide a response or agree that technology causes more harm than good are the least likely to be comfortable with driverless cars.

### Initial model

We begin by fitting a model using the predictors from @sec-ch-logistic, `age` and `tech_easy`, along with a new predictor `income` to predict whether an individual is comfortable with driverless cars. We'll use this model for the majority of the chapter as we introduce prediction, classification, and model assessment for logistic regression. In @sec-logistic-workflow, we'll use cross validation and the model selection workflow from @sec-ch-model-eval to compare this model to another one that also includes `polviews` and `tech_harm`.

```{r}
#| label: tbl-aidrive-comfort-fit
#| tbl-cap: "Model of `aidrive_comfort` versus `age`, `tech_easy`, and `income` with 95% confidence intervals"


aidrive_comfort_fit <- glm(aidrive_comfort ~ age + tech_easy + income_fct, 
                            data = gss24_ai, family = "binomial")

tidy(aidrive_comfort_fit, conf.int = TRUE) |> 
  kable(digits = 3)


aidrive_comfort_intercept <- tidy(aidrive_comfort_fit) |> filter(term == "(Intercept)") |> pull(estimate)
```

::: {.yourturn latex=""}
Consider the coefficients for the indicators of `income_fct`. Are they consistent with the observations from the EDA? Why or why not. [^12-logistic-prediction-1]
:::

[^12-logistic-prediction-1]: Yes, the coefficients and confidence intervals for the indicators for `income_fct s`upport the observations from the EDA. The indicators for higher income, `$50-110K`, and `$110k or more` are positive. Thus, those with higher income are more likely to be comfortable with driverless cars compared to individuals in the lowest level, after adjusting for `age` and `tech_easy`.

## Prediction and classification

In @sec-ch-logistic, we introduced the logistic regression, where we used the model to describe and draw conclusions about the relationship between the response and predictor variables. In practice, logistic regression models are widely used for classification, particularly in data science and machine learning. They are part of a branch of machine learning models called **supervised learning**, in which the model is built and evaluated using data that contains observed outcomes. Therefore, we will now focus on using the model to predict whether an individual is comfortable with self-driving cars given particular characteristics.

### Prediction {#sec-logistic-prediction}

Recall from @sec-logistic-model that the response variable in the logistic regression model is the logit (log-odds). When we input values of `age`, `tech_easy`, and `income` into the model in @tbl-aidrive-comfort-fit, the model will output the log odds an individual with those characteristics is comfortable with driverless cars. Once we have the predicted log odds, we can use the relationships in @sec-prob-odds to compute the predicted odds and the predicted probability. <!--# do i show odds to prob in previous chapter?--> @tbl-aidrive-comfort-predict shows the predicted log odds, predicted odds, and predicted probability for 10 observations in the data set.

```{r}
#| label: tbl-aidrive-comfort-predict
#| tbl-cap: "Predictions from model in @tbl-aidrive-comfort-fit for 10 respondents"

aidrive_comfort_aug <- augment(aidrive_comfort_fit)


aidrive_comfort_aug <- aidrive_comfort_aug |>
  mutate(pred_odds = exp(.fitted), 
         pred_prob = pred_odds / (1 + pred_odds), 
         obs_num = 1:nrow(aidrive_comfort_aug))

aidrive_comfort_aug |>
  slice(1:10) |>
  select(obs_num, aidrive_comfort, age, tech_easy, income_fct, .fitted, pred_odds, pred_prob) |>
  kable(digits = 3, col.names = c("", "aidrive_comfort", "age", "tech_easy",
                                  "income", "Pred. log odds", 
                                  "Pred. odds", "Pred. probability"))
```

Let's show how the predicted values for the first observation are computed. The predicted log odds the first individual who is 33 years old, agrees that technology makes life better, and has an annual income of \$110k or more are

$$
\begin{aligned}
\log \Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) &=  0.032 - 0.016  \times 33 + 0.075   \times 0 +  0.576 \times 1 \\ 
&- 0.436  \times 0 +  0.264 \times 0 +  0.526  \times 0 \\
& + 1.223  \times 1 + 0.265 \times 0 \\
& = 1.3
\end{aligned}
$$

where $\hat{\pi}$ is the predicted probability of being comfortable with driverless cars.

```{r}
#| label: pred-prob
#| include: false

new_obs <- tibble(age = 33, income_fct = "$110k or more", tech_easy = "Agree")

predict(aidrive_comfort_fit, new_obs)
```

Using the predicted log odds from @tbl-aidrive-comfort-predict, the predicted odds for this individual are

$$\widehat{\text{odds}} = e^{\log \big( \frac{\hat{\pi}}{1-\hat{\pi}}\big)} = e^{1.306} = 3.69 $$

Lastly, we use the odds from @tbl-aidrive-comfort-predict to compute the predicted probability:

$$
\hat{\pi} = \frac{\widehat{\text{odds}}}{1 + \widehat{\text{odds}}} = \frac{3.692}{1 + 3.692} = 0.787
$$

Note: These values may differ slightly from the values in the the table, because we are computing predictions using rounded coefficient.

::: {.yourturn latex=""}
Show how to compute the predicted log odds, odds, and probability for individual #2 in @tbl-aidrive-comfort-predict.[^12-logistic-prediction-2]
:::

[^12-logistic-prediction-2]: **Log odds:** $\log \Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) =  0.032 - 0.016  \times 19 + 0.075   \times 0 +  0.576 \times 0
    - 0.436  \times 0 +  0.264 \times 0 +  0.526  \times 0
     + 1.223  \times 1 + 0.265 \times 0 = 0.951$

    **Odds:** $e^{\log(\frac{\hat{\pi}}{1-\hat{\pi}})} = e^{0.953} = 2.59$

    **Probability:** $\frac{\widehat{\text{odds}}}{1 + \widehat{\text{odds}}} = \frac{2.593}{1 + 2.593} = 0.722$

### Classification {#sec-classification}

Knowing the predicted odds and probabilities can be useful for understanding how likely individuals with various characteristics will be comfortable with driverless cars. In many contexts, however, we would like to group individuals based on whether or not the model predicts they are comfortable with driverless cars. For example, the marketing team for a robotaxi company may want to use targeted marketing strategies and offer discounts to potential new customers. To have a successful marketing campaign, they want to direct the marketing to those who are comfortable with driverless cars.

As we've seen thus far, the logistic regression model does not directly produce predicted values of the binary response variable. Therefore, we can group observations based on the predicted probabilities computed from the model output. This process of grouping observations based on the predictions is called **classification**. The groups the observations are put into are the **predicted classes**. In the case of our analysis, we will use the model to classify observations into the class of those not comfortable with driverless cars (`aidrive_comfort = 0`) or the class of those comfortable with driverless cars (`aidrive_comfort = 1`) .

<!--# Key terms: Predicted class: Group assignment for an observation based on model predictions Classification: Process of using the model predictions to assign observations to classes-->

We showed how to compute the predicted probabilities from the logistic regression output in @sec-logistic-prediction, and we will use those probabilities to classify observations. The question, then, is how large does the probability need to be to classify an observation as having the response $Y = 1$ ? In terms of our analysis, how large does the probability of being comfortable with driverless cars need to be to classify an individual as being comfortable with driverless cars, `aidrive_comfort = 1`?

When using the logistic regression model for classification, we define a threshold, such an observation is classified as $\hat{Y} = 1$ if the predicted probability is greater than the threshold. Otherwise, the observation is classified as $\hat{Y} = 0$. If we're unsure what threshold to set, we can start with a threshold of 0.5, the default threshold typically used in statistical software. This means if the model predicts an observation is more likely than not to have response $Y = 1$, even if just by a small amount, then the observation is classified as having response $\hat{Y} = 1$.

For now, let's use the threshold equal to 0.5 to assign the predicted classes of `aidrive_comfort` for the respondents in the sample data based on the predicted probabilities produced from the model in @tbl-aidrive-comfort-fit.

```{r}
#| label: tbl-aidrive-comfort-pred-class
#| tbl-cap: "Predicted class based on model in @tbl-aidrive-comfort-fit and threshold of 0.5 for 10 respondents"

aidrive_comfort_aug <- aidrive_comfort_aug |>
mutate(pred_class = if_else(pred_prob > 0.5, "1", "0"))

aidrive_comfort_aug |>
  slice(1:10) |>
  select(obs_num, aidrive_comfort, pred_prob, pred_class) |>
  kable(digits = 3, col.names = c("", "aidrive_comfort", "Pred. probability", "Pred. class" ))
```

@tbl-aidrive-comfort-pred-class shows the observed value of the response (`aidrive_comfort`), predicted probability, and predicted class for ten respondents. For many of these respondents, the observed and predicted classes are equal. There are some, such as Observation 6, in which the predicted classes differs from the observed. In this instance, the respondent had a combination of `age`, `income`, and `tech_easy`, that are associated with a higher probability of being comfortable with driverless cars; however, the individual responded they are not comfortable with driverless cars in the General Social Survey.

We need a way to more holistically evaluate how well the predicted classes align with the observed classes. To do so, we use a **confusion matrix**, a $2 \times 2$ table of the observed classes versus the predicted classes.

```{r}
#| label: tbl-confusion-matrix
#| tbl-cap: "Confusion matrix for model in @tbl-aidrive-comfort-fit and threshold of 0.5. Observed class (columns), Predicted class (rows)"


aidrive_comfort_aug |>
  count(aidrive_comfort, pred_class) |>
  pivot_wider(names_from = aidrive_comfort, 
              values_from = n) |>
  kable(col.names = c("Pred. class", "0", "1")) |>
  kableExtra::add_header_above(c(" " = 1, "Observed class" = 2), escape = FALSE) 
```

<!--# Maybe make this table in keynote, so I can label rows to say actual-->

@tbl-confusion-matrix shows the confusion matrix for the model in @tbl-aidrive-comfort-fit and a threshold of 0.5. In this table, the rows define the predicted classes and the columns define the observed classes. Let's break down what each cell is in the table:

-   There are 349 observations with the predicted class of 0 and observed class of 0.
-   There are 228 observations with the predicted class of 0 and observed class of 1.
-   There are 343 observations with the predicted class of 1 and observed class of 0.
-   There are 601 observations with the predicted class of 1 and observed class of 1.

We compute various statistics from the confusion matrix to evaluate how well the observations are classified. The first statistics we can calculate are the accuracy and misclassification rate. The **accuracy** is the proportion <!--# or percentage?--> of observations that are correctly classified (the observed and predicted classes are the same). The accuracy based on @tbl-confusion-matrix is

$$
\text{accuracy} = \frac{349 + 601}{349 + 228 + 343 + 601} = 0.625 
$$ {#eq-logistic-accuracy}

Using the model in @tbl-aidrive-comfort-fit and the threshold of 0.5, 62.5% of the observations are correctly classified.

The **misclassification rate** is the proportion of observations that are not correctly classified (the observed and predicted classes differ). The misclassification rate based on @tbl-confusion-matrix is

$$
\text{misclassification} = \frac{228 + 343}{349 + 228 + 343 + 601} = 0.375
$$ {#eq-logistic-misclassification}

Using the model in @tbl-aidrive-comfort-fit and the threshold of 0.5, 37.5% of the observations are incorrectly classified. Note that the misclassification rate is equal to $1 - \text{accuracy}$ ,and vice versa.

::: {.analysis_in_practice latex=""}
When the distribution of the response variable is largely imbalanced, the accuracy can be a misleading measure of how well the observations are classified. For example, suppose there are 100 observations, such that 5% of the observations have an observed response $Y = 1$, and 95% of the observations have an observed response of $Y = 0$. We may observe an imbalanced distribution like this when building a model to detect the presence of a rare disease, for example.

<br>

Let's suppose based on the model and threshold, all observations have a predicted class of 0, and the confusion matrix looks like the following:

```{r}
tribble(
  ~"Pred.class", ~"0", ~"1",
  0,  95, 5,
  1, 0,0
) |>
   kable(col.names = c("Pred. class", "0", "1")) |>
  kableExtra::add_header_above(c(" " = 1, "Observed class" = 2), escape = FALSE)
```

\
The accuracy for this model will be (95 + 0 ) / (95 + 5) = 0.95. Based on this value, it appears the classification has performed very well, even though we did not accurately classify any of the observations in which the observed response is $Y = 1$.
:::

The accuracy and misclassification rate provide a nice initial indication of the how well the model classifies observations, but they do not give a complete picture. For example, suppose we want to know how many of the people who are actually comfortable with driverless cars are predicted to be comfortable based on the model predictions and threshold. Or suppose we want to know how many people who are actually not comfortable with driverless cars were incorrectly classified as being comfortable. To answer these and similar questions, let's take a more detailed look at the confusion matrix.

|   | **Not comfortable with driverless cars** $(y_i = 0)$ | **Comfortable with driverless cars** $(y_i = 1)$ |
|------------------------|------------------------|------------------------|
| **Classified** **not comfortable**$(\hat{y}_i = 0)$ | True negative (TN) | False negative (FN) |
| **Classified comfortable**$(\hat{y}_i = 1)$ | False positive (FP) | True positive (TP) |

: Detailed confusion matrix {#tbl-confusion-matrix-detailed}

<!--# check the notation used in this table, y_i and p_i-->

@tbl-confusion-matrix-detailed shows in greater detail what is being quantified in each cell of the confusion matrix. We will use these values to compute more granular values about the classification. As in @tbl-confusion-matrix, the rows define the predicted classes and the columns define the observed classes. The values in the cell indicate the following:

-   **True negative (TN):** The number of observations that are predicted to be not comfortable with driverless cars ( $\hat{y}_i = 0$) and have observed response of not comfortable ($y_i = 0$) .

-   **False negative (FN):** The number of observations that are predicted to be not comfortable with driverless cars ( $\hat{y}_i = 0$) and have observed response of comfortable ( $y_i = 1$) .

-   **False positive (FP):** The number of observations that are predicted to be comfortable with driverless cars ( $\hat{y}_i = 1$) and have observed response of not comfortable ( $y_i = 0$) .

-   **True positive (TP):** The number of observations that are predicted to be comfortable with driverless cars ( $\hat{y}_i = 1$) and have observed response of comfortable ( $y_i = 1$) .

Using these definitions, the general form of accuracy computed in @eq-logistic-accuracy is

$$
\text{accuracy} = \frac{\text{True negative} + \text{True positive}}{\text{True negative} + \text{False negative} + \text{False positive} + \text{True positive}}
$$ {#eq-accuracy-general}

::: {.yourturn latex=""}
Write the general equation for the misclassification computed in @eq-logistic-misclassification using the terms in @tbl-confusion-matrix-detailed.[^12-logistic-prediction-3]
:::

[^12-logistic-prediction-3]: $$\text{misclassification} = \frac{\text{False negative} + \text{False positive}}{\text{True negative} + \text{False negative} + \text{False positive} + \text{True positive}}
    $$

Now let's take a look at additional statistics that help us quantify how well the observations are classified. First, we'll focus on the column containing those who have observed values $y_i = 1$.

The **sensitivity** (**true positive rate**) is the proportion of those with observed $y_i = 1$ that were correctly classified as $\hat{y}_i = 1$. In machine learning contexts, this value is also called **recall** or **probability of detection**.

$$
\text{Sensitivity} = \frac{\text{True positive}}{\text{False negative} + \text{True positive}}
$$ {#eq-sensitivity}

The **false negative rate** is the proportion of those with observed $y_i = 1$ that were incorrectly classified as $\hat{y}_i = 0$.

$$
\text{False negative rate} = \frac{\text{False negative}}{\text{False negative} + \text{True positive}}
$$ {#eq-false-negative}

The false negative rate is equal to $1 - \text{Sensitivity}$ and vice versa. The denominators for the false negative rate and the sensitivity are the total number of observations with observed response $y_i = 1$. In terms of our analysis, this is the total number of people who responded they are comfortable with driverless cars in the General Social Survey.

Next, we look at the column of containing those who have observed values of $y_i =0$.

The **specificity** (**true negative rate**) is the proportion of those with observed $y_i = 0$ who were correctly classified as $\hat{y}_i = 0$.

$$
\text{Specificity} = \frac{\text{True negative}}{\text{True negative} + \text{False positive}}
$$ {#eq-specificity}

The **false positive rate** is the proportion of those with observed $y_i = 0$ who were incorrectly classified as $\hat{y}_i = 1$. In machine learning contexts, this value is also called the **probability of false alarm.**

$$
\text{False positive rate} = \frac{\text{False positive}}{\text{True negative} + \text{False positive}}
$$ {#eq-false-positive}

The false positive rate is equal to $1 - \text{specificity}$. The denominators for the specificity and false positive rate are the total number of observations with observed response $y_i = 0$. In terms of our analysis, this is the total number of people who responded they are not comfortable with driverless cars in the General Social Survey.

The values shown thus far quantify how well observations are classified based on the observed response. Another question that is often of interest is among those with predicted class of $\hat{y}_i = 1$, how many actually have observed values of $y_i = 1$? This value is called the **precision.**

$$
\text{Precision} = \frac{\text{True positive}}{\text{False positive} + \text{True positive}}
$$ {#eq-precision}

Now the denominator is the number of observations that have a predicted class $\hat{y}_i = 1$, the second row in the @tbl-confusion-matrix-detailed. In the context of our analysis, the precision is how many of the individuals who are predicted to be comfortable with driverless cars actually responded on the General Social Survey that they are comfortable with driverless cars. If we're using the model to identify individuals for a targeted marketing campaign, the precision can be useful in quantifying whether the marketing will generally capture those who are actually comfortable with driverless cars or if a large proportion would be aimed at those who actually aren't comfortable with driverless cars and likely won't become robotaxi customers.

::: {.yourturn latex=""}
Use @tbl-confusion-matrix to compute the following:

-   Sensitivity

-   Specificity

-   Precision[^12-logistic-prediction-4]
:::

[^12-logistic-prediction-4]: $$\begin{aligned}\text{Sensitivity} &= 601/ (228 + 601) = 0.725 \\ \text{Specificity} &= 349 / (349 + 343) = 0.504 \\\text{Precision} &= 601 / (343 + 601) = 0.637\end{aligned}$$

We have shown how we can derive a lot of useful information from the confusion matrix. As we use the confusion matrix to evaluate how well observations are classified; however, we need to keep in mind that the confusion matrix is determined based on the model predictions [**and**]{.underline} the threshold for classification. For example, in @tbl-confusion-matrix-0.3, we see a confusion matrix for the same model in @aidrive-comfort-fit using the threshold of 0.3.

```{r}
#| label: tbl-confusion-matrix-0.3
#| tbl-cap: "Confusion matrix using model in @tbl-aidrive-comfort-fit and threshold of 0.3"

aidrive_comfort_aug |>
mutate(pred_class2 = if_else(pred_prob > 0.3, "1", "0")) |>
  count(aidrive_comfort, pred_class2) |>
  pivot_wider(names_from = aidrive_comfort, 
              values_from = n) |>
  kable(col.names = c("Pred. class", "0", "1")) |>
  kableExtra::add_header_above(c(" " = 1, "Observed class" = 2), escape = FALSE)
```

Due to the low threshold, there are many more observations classified as $\hat{y}_i = 1$ compared to @tbl-confusion-matrix. The accuracy rate is now 58.1% and the misclassification rate is 41.9% even though the model hasn't changed. From this example, we see that even if the model is unchanged, the metrics computed from the confusion matrix will differ based on the threshold. Therefore, we would like a way to evaluate the model performance independent from the choice of threshold.

## ROC Curve {#sec-roc}

Ideally, we want a way to evaluate the the model performance regardless of threshold and then choose a threshold that results in the "best" classification as determined by the statistics from the previous section. We could make confusion matrices across a range of thresholds, but this process could be cumbersome and time consuming to make so many confusion matrices. Rather than make individual confusion matrices, we will use the **receiving operator characteristic (ROC) curve** shown in @fig-aidrive-comfort-roc-curve. The ROC curve is a single visualization to holistically evaluate the model fit and see how well the model classifies at different thresholds. We can use the data from the ROC curve to choose a classification threshold. <!--# add something about origin of ROC curve or where it gets its name-->

```{r}
#| label: fig-aidrive-comfort-roc-curve
#| fig-cap: "ROC curve for model in @tbl-aidrive-comfort-fit with point marked at threshold = 0.4754"

aidrive_comfort_aug |>
  roc_curve(aidrive_comfort, pred_prob, 
            event_level = "second")  |>
 mutate(highlight = if_else(specificity == 0.4754335, "1", "0")) |>
  autoplot() +
  labs(x = "1 - Specificity (False positive rate)", 
       y = "Sensitivity (True positive rate)") +
  annotate("point", x = 0, y = 1, color = "darkgreen") +
  annotate("point", x = 0, y = 1, color = "darkgreen", size = 3, shape = "circle open") +
  annotate(
    "label", x = 0.02, y = 0.99, label = "Perfect classification", hjust = 0, color = "darkgreen", fontface = "bold", vjust = 1, fill = "white"
  ) +
  annotate("point", x = 1, y = 0, color = "#cc5c76") +
  annotate("point", x = 1, y = 0, color = "#cc5c76", size = 3, shape = "circle open") +
  annotate(
    "label", x = 0.98, y = 0.01, label = "All observations classified incorrectly", hjust = 1, color = "#cc5c76", fontface = "bold", vjust = 0, fill = "white"
  ) +
  annotate(
    "segment", color = "steelblue", x = 0, y = 0, xend = 1, yend = 1, size = 2, alpha = 0.6
  ) +
  annotate(
    "label", x = 0.58, y = 0.5, label = "True positive rate\n= false positive rate", hjust = 0, color = "steelblue", fontface = "bold", fill = "white"
  ) +
   annotate("point", x = 1 - 0.4754335 , y = 0.7503016, color = "red", size = 3, shape = "circle open") + 
  annotate("point", x = 1 - 0.4754335 , y = 0.7503016, color = "red")
```

<!--# need to update this graph so that it is not a direct copy of Mine's, including the colors-->

@fig-aidrive-comfort-roc-curve is the ROC curve for the model in @tbl-aidrive-comfort-fit. The $x$-axis on the ROC curve is $1 - \text{Specificity}$, the false positive rate, and the $y$-axis is $\text{Sensitivity}$, the true positive rate. Thus, the ROC curve is a visualization of the true positive rate versus the false positive rate at classification thresholds ranging from 0 to 1 (equivalent to the log odds ranging from $-\infty$ to $\infty$). The diagonal line represents a model fit in which the true positive rate and false positives are equal regardless of the threshold. This means the model is unable to distinguish the observations that actually have an observed response of $y_i = 1$ versus those that do not, so it is essentially the same as using a coin flip to classify observations (not a good model!). In contrast, ROC curves that hit closer to the top-left corner indicate a model that is good at distinguishing true positives and false positives.

```{r}
#| label: fig-roc-curve-examples
#| fig-cap: Example ROC curves for different model performs
#| fig-subcap: 
#|   - Poor distinction 
#|   - Good distinction 
#|   - Nearly perfect distinction
#| layout-ncol: 3

## Simulated data generated by ChatGPT: https://chatgpt.com/share/e/68dd42b9-a5e0-8006-a66c-fc1b3c40ad3e

set.seed(0)
n_pos <- 120
n_neg <- 120

## example ROC curves

## curve 1 - AUC about 0.57
pos2 <- rbeta(n_pos, 1.0, 1.0)    # positives: Beta(1,1)  (uniform)
neg2 <- rbeta(n_neg, 1.0, 1.5)    # negatives: Beta(1,1.5)
y2 <- c(rep(1, n_pos), rep(0, n_neg))
pred2 <- c(pos2, neg2)
df2 <- data.frame(observed = factor(y2), predicted = pred2)

df2 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()

## curve 2 - AUC about 0.8
pos1 <- rbeta(n_pos, 1.5, 1.0)    # positives: Beta(1.5, 1)
neg1 <- rbeta(n_neg, 2.0, 4.0)    # negatives: Beta(2, 4)
y1 <- c(rep(1, n_pos), rep(0, n_neg))
pred1 <- c(pos1, neg1)
df1 <- data.frame(observed = factor(y1), predicted = pred1)

df1 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()

## curve 3 - AUC about 0.99

pos3 <- rbeta(n_pos, 2.5, 1.0)    # positives: Beta(2.5,1) (skewed high)
neg3 <- rbeta(n_neg, 1.0, 8.0)    # negatives: Beta(1,8)   (skewed low)
y3 <- c(rep(1, n_pos), rep(0, n_neg))
pred3 <- c(pos3, neg3)
df3 <- data.frame(observed = factor(y3), predicted = pred3)

df3 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()
```

<!--# think of better words than distinction?-->

@fig-roc-curve-examples shows example ROC curves for different model fits. @fig-roc-curve-examples-1 is the ROC curve for a model that does a poor job distinguishing between the true positives and false positives (close to the diagonal line) and @fig-roc-curve-examples-3 is the ROC curve for a model that almost perfectly distinguishes between the true and false positives (close to the top-left corner). Generally, we expect to see ROC curves somewhere in the middle, similar to @fig-roc-curve-examples-2. Here, the model generally does a good job distinguishing between the true and false positives, but we will expect to get some false positives if we want a high true positive rate (sensitivity).

Each point in the ROC curve is $(1 - \text{Specificity, } \text{Sensitivity})$ at a given threshold and can be thought of as representing an individual confusion matrix for a given threshold. For example, the point marked in red on the ROC curve in @fig-aidrive-comfort-roc-curve shows corresponds to $1 - \text{Specificity} = 0.525$ and $\text{Sensitivity} = 0.75$. This point corresponds to the classification threshold 0.4888088. The corresponding confusion matrix is shown in @tbl-aidrive-roc-confmat. Observations with predicted probability $\hat{\pi}_i > 0.4888088$ are classified as being comfortable with driverless cars, and those with $\hat{\pi}_i \leq 0.4888088$ are classified as not being comfortable with driverless cars.

```{r}
#| label: tbl-aidrive-roc-confmat
#| tbl-label: "Confusion matrix for model in @tbl-aidrive-comfort at threshold 0.4888088"

aidrive_comfort_aug |>
  mutate(pred_class3 = if_else(pred_prob >0.4888088, "1", "0")) |>
  count(aidrive_comfort, pred_class3) |>
  pivot_wider(names_from = aidrive_comfort, 
              values_from = n) |>
  kable() |>
  kableExtra::add_header_above(c(" " = 1, "Observed class" = 2), escape = FALSE)
```

<!--# would be nice to have the roc curve and confusion mat side-by-side-->

::: {.yourturn latex=""}
Compute the sensitivity and specificity from @tbl-aidrive-roc-confmat and compare these to the values observed on the curve in @fig-aidrive-roc.
:::

```{r}
#| label: roc-optimize 

roc_optimize <- aidrive_comfort_aug |>
  roc_curve(aidrive_comfort, pred_prob, 
            event_level = "second") |>
  mutate(d = sqrt((1 - specificity)^2 + (sensitivity - 1)^2)) |>
  arrange(d) |>
  slice(1)
```

In the next section, we will talk more about using the ROC curve to evaluate the model fit beyond a visual assessment. For now, let's discuss how to use the ROC curve to determine a probability threshold for classification. When we use a model for classification, we want a high true positive rate and a low false positive rate. Therefore, one of the most straightforward ways of using the ROC curve to identify a classification threshold is to choose the threshold that corresponds to the point on the curve that is closest to the top-left corner. <!--# This will be the point that has the best ratio of true positive rate to false positive rate. not sure if that is true--> <!--# does this relate to that F1 metric from machine learning?-->. There are many ways to identify this point mathematically. For example, we can find the combination of $\text{Sensitivity}$ and $1 - \text{Specificity}$ that minimizes the following:

$$
\sqrt{(1 - \text{Specificity})^2 + (\text{Sensitivity} - 1)^2}
$$

<!--# verify this equation is correct.-->

In terms of the ROC curve in @fig-aidrive-roc, this point is at a false positive rate of `r round((1 - roc_optimize$.specificity), 3)` and true positive rate of `r round(roc_optimize$sensitivity, 3)`, corresponding to a threshold of `r round(roc_optimize$.threshold, 3)`.

While this may be a reasonable approach for identifying a threshold in many contexts, we often need to consider the practical implications when making analysis decisions in practice. For example, suppose we build a logistic regression model to diagnosis a medical illness. A classification of "1" means the patient has the illness and undergoes a treatment plan. A classification of "0" means the patient does not have the illness and does not undergo a treatment plan.

::: {.yourturn latex=""}
What is a "true positive" in this scenario? What is a "false positive"?[^12-logistic-prediction-5]
:::

[^12-logistic-prediction-5]: A true positive is correctly classifying a patient with the illness as a "1", having the illness. A "false positive" is incorrectly classifying a patient that doesn't have the illness as a "1", having the illness.

When determining the probability threshold for classification, we need to think carefully about the implications of the analysis decision. More specifically, one thing we need to consider is the severity of the treatment the patient will undergo. If the treatment is minimal, then perhaps we might be willing to set a threshold that results in more false positives. If the treatment is very invasive, however, we want to minimize false positives. Otherwise, there will be many patients who do not need the treatment who will undergo an invasive treatment. Similarly, we want to consider the implications of lower sensitivity and not diagnosing individuals who actually have the illness.

::: yourturn
-   If the main objective is high sensitivity, do we set a probability threshold closer to 0 or to 1?

-   If the main objective is high specificity, do we set a probability threshold closer to 0 or to 1?[^12-logistic-prediction-6]
:::

[^12-logistic-prediction-6]: If the main objective is high sensitivity, we set a low threshold close to 0. If the main objective is high specificity, we set a high threshold close to 1.

## Model evaluation and comparison {#sec-logistic-model-eval}

In @sec-classification, we discussed methods for evaluating how well a model classifies observations at a given threshold. As in linear regression, we want to quantify the overall model fit, independent of the threshold, so we can evaluate how well the model fits the data and compare multiple models.

### Area Under the Curve (AUC)

The ROC curve visualizes how well the model differentiates between true positives and false positives for the full range of classification thresholds 0 to 1. Therefore, in addition to helping us identify a classification threshold, it can be used to evaluate the model performance. The **Area Under the Curve (AUC)** is a measure of the model performance and is computed as the area under the ROC curve. The values of AUC range from 0.5 to 1. An $AUC = 0.5$ corresponds to an ROC curve on the diagonal line, indicating the model is unable to distinguish between true and false positives. An $AUC = 1$ corresponds to a curve that meets the top-left corner, indicating the model is able to perfectly distinguish between true and false positives.

```{r}
#| label: fig-roc-curve-examples-auc
#| fig-cap: Examle ROC curves with corresponding AUC
#| fig-subcap: 
#|   - AUC = 0.57
#|   - AUC = 0.79
#|   - AUC = 0.99
#| layout-ncol: 3

## Simulated data generated by ChatGPT: https://chatgpt.com/share/e/68dd42b9-a5e0-8006-a66c-fc1b3c40ad3e

set.seed(0)
n_pos <- 120
n_neg <- 120

## example ROC curves

## curve 1 - AUC about 0.57
pos2 <- rbeta(n_pos, 1.0, 1.0)    # positives: Beta(1,1)  (uniform)
neg2 <- rbeta(n_neg, 1.0, 1.5)    # negatives: Beta(1,1.5)
y2 <- c(rep(1, n_pos), rep(0, n_neg))
pred2 <- c(pos2, neg2)
df2 <- data.frame(observed = factor(y2), predicted = pred2)

df2 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()

## curve 2 - AUC about 0.8
pos1 <- rbeta(n_pos, 1.5, 1.0)    # positives: Beta(1.5, 1)
neg1 <- rbeta(n_neg, 2.0, 4.0)    # negatives: Beta(2, 4)
y1 <- c(rep(1, n_pos), rep(0, n_neg))
pred1 <- c(pos1, neg1)
df1 <- data.frame(observed = factor(y1), predicted = pred1)

df1 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()

## curve 3 - AUC about 0.99

pos3 <- rbeta(n_pos, 2.5, 1.0)    # positives: Beta(2.5,1) (skewed high)
neg3 <- rbeta(n_neg, 1.0, 8.0)    # negatives: Beta(1,8)   (skewed low)
y3 <- c(rep(1, n_pos), rep(0, n_neg))
pred3 <- c(pos3, neg3)
df3 <- data.frame(observed = factor(y3), predicted = pred3)

df3 |> 
  roc_curve(observed, predicted, 
            event_level = "second") |>
  autoplot()
```

```{r}
#| label: aidrive-auc

aidrive_comfort_auc <- aidrive_comfort_aug |>
  roc_auc(aidrive_comfort, pred_prob,
    event_level = "second"
  ) |>
  pull(.estimate)
```

@fig-roc-curve-examples-auc shows the ROC curves from @fig-roc-curve-examples along with the AUC for each curve. Similar to $R^2$ for linear regression ([@sec-slr-sum-sq]), we prefer models with AUC close to 1. However, there is no single threshold that defines "good" AUC. What is considered a "good" AUC depends on the subject matter context and complexity of the modeling task.

The AUC for the model in @tbl-aidrive-comfort-fit is `r round(aidrive_comfort_auc, 3)`. Predicting individuals' opinions is a complex modeling task, so the model is a reasonably good fit for the data. We are only using three predictors, however, so we will consider other predictors in @sec-logistic-workflow to potentially improve the model performance.

::: {.analysis_in_practice latex=""}
Though we prefer models with high values of AUC (close to 1), we do not want a model in which $AUC = 1$ exactly. When the $AUC  =1$, it means the model has perfect separation, the ability to perfectly distinguish between the true positives and false positives. Though this may seem like ideal model performance, it is often a sign that the model is overfit <!--# do a i make a call back to the earlier section--> and will not effectively classify observations in new data.
:::

### Comparing models using AIC and BIC {#sec-logistic-aic-bic}

Similar to linear regression, we can use the Akaike's Information Criterion (AIC) [@akaike1974] and the Bayesian Schwarz's Criterion (BIC) [@schwarz1978estimating] as relative measures for comparing logistic regression models. The equations for AIC and BIC in logistic regression are the same as these in linear regression @sec-aic-bic.

$$
\begin{aligned}
&\text{AIC} = -2 \log L + 2(p+1) \\[5pt]
&\text{BIC} = -2 \log L + \log(n)(p+1)
\end{aligned}
$$

where $\log L$ is the log-likelihood of the model, $n$ is the number of observations, and $p + 1$ is the number of terms in the model. The penalty applied by BIC for the number of predictors in the model is greater than the penalty applied in AIC, as $\log(n) > 2$ (when $n > 8$). Therefore, BIC will tend to preference more parsimonious models, those with a fewer number of predictors. The values of AIC and BIC are not meaningful to interpret for individual models. These values are most useful for comparing models in the model selection process. When using AIC and BIC to compare models, we can use the guidelines in @tbl-aic-bic-guidelines, the same as with linear regression.

Let's use AIC and BIC to compare the model from @tbl-aidrive-comfort-fit to a model that includes the same predictors along with `polviews` (how individuals rate their political views) and `tech_harm` (whether an individual thinks technology generally does more harm than good).

```{r}
#| label: tbl-aic-bic-compare 
#| tbl-cap: AIC and BIC for two candidate models 

aidrive_fit_2 <- glm(aidrive_comfort ~ age + tech_easy + income +
                       polviews + tech_harm, 
                     data = gss24_ai, family = "binomial")

aic_bic_model1 <- glance(aidrive_comfort_fit) |> select(AIC, BIC)
aic_bic_model2 <- glance(aidrive_fit_2) |> select(AIC, BIC)

stats <- aic_bic_model1 |> bind_rows(aic_bic_model2)

models <- c("age, income, tech_easy", "age, income, tech_easy, polviews, tech_harm")

bind_cols(models, stats) |>
  kable(digits = 3, col.names =c("Model", "AIC", "BIC"))
```

@tbl-aic-bic-compare shows AIC and BIC for both models. Based on these measures, there is very strong evidence in favor of the model that includes the additional predictors `polviews` and `tech_harm`.

<!--# include log-loss, cross entropy? Is this too much machine learning?  -->

<!--# Drop-in-deviance test - this should go in inference-->

## Prediction and evaluation in R {#sec-logistic-prediction-R}

### Prediction and classification

Many of the functions for prediction and evaluation for logistic regression are the same as those used in linear regression. The `predict()` function is used to compute predictions from the logistic regression model. The predicted logit can also be obtained from the `.fitted` column in the data produced by the `augment()` function.

Below is the code to fit the model in @tbl-aidrive-comfort-fit and produce the predicted log odds. The predictions for the first 10 observations are shown.

```{r}
#| echo: true 
#| eval: false

aidrive_comfort_fit <- glm(aidrive_comfort ~ age + tech_easy + income_fct, 
                           data = gss24_ai, 
                           family = "binomial")

predict(aidrive_comfort_fit)
```

```{r}
aidrive_comfort_fit <- glm(aidrive_comfort ~ age + tech_easy + income_fct, 
                           data = gss24_ai, 
                           family = "binomial")

predict(aidrive_comfort_fit)[1:10]
```

The predicted probabilities can be computed directly using the argument `type = "response"` in `predict()`.

```{r}
#| echo: true
#| eval: false 

predict(aidrive_comfort_fit, type = "response")
```

```{r}
predict(aidrive_comfort_fit, type = "response")[1:10]
```

The predicted classes can be computed "manually" using the predicted probabilities and **dplyr** functions. Below is code to compute the predicted probabilities, predict the classes based on a threshold of 0.5, and add these as columns to the original `gss24_ai` data frame. The predicted probability and class for the first 10 observations are shown below.

```{r}
#| echo: true

gss24_ai <- gss24_ai |>
  mutate(pred_prob = predict(aidrive_comfort_fit, type = "response"),
         pred_class = factor(if_else(pred_prob > 0.5, "1", "0")))
```

```{r}
gss24_ai |>
  select(pred_prob, pred_class) |>
  slice(1:10)
```

### Confusion matrix and ROC curve

We use the predicted probabilities and predicted classes to make the confusion matrix is using the `conf_mat()` function from the **yardstick** package [@yardstick].

```{r}
#| echo: true 

aidrive_conf_mat <- gss24_ai |>
conf_mat(aidrive_comfort, pred_class)

aidrive_conf_mat
```

The `autoplot()` function produces the same confusion matrix with some additional formatting applied. For example, the argument `type = "heatmap"` produces a confusion matrix in which the cells are shaded in based on the number of observations.

```{r}
#| echo: true 
autoplot(aidrive_conf_mat, type = "heatmap")
```

The `roc_curve` function is used to compute the data for the ROC curve. Ten data points from the ROC curve data are shown below.

```{r}
#| echo: true 

aidrive_roc_data <- gss24_ai |>
roc_curve(aidrive_comfort, pred_prob, event_level = "second")
```

```{r}
set.seed(12345) 
aidrive_roc_data |> 
sample_n(10)
```

The argument `event_level = "second"` is needed to specify that the predicted probability is associated with the probability that $y_i = 1$ (rather than the probability $y_i = 0$). The `.threshold` column contains the classification thresholds.

The ROC curve data can be plotted using the `autoplot()` function. The resulting graph is `ggplot` object, so additional **ggplot2** layers such as `labs` and `annotate` can be applied to the ROC curve.

```{r}
#| echo: true 

autoplot(aidrive_roc_data) + 
labs(title = "ROC Curve")
```

### Model evaluation

The `roc_auc()` function is used to compute the Area Under the Curve using the predicted probabilities and predicted classes. It follows a similar syntax as `roc_curve()`.

```{r}
#| echo: true 

gss24_ai |> 
roc_auc(aidrive_comfort, pred_prob, event_level = "second")
```

The last model comparison statistics, AIC and BIC, are computed from the `glance()` function.

```{r}
#| echo: true 
glance(aidrive_comfort_fit)
```

Note that the output from `glance()` does not include $R^2$ and Adj. $R^2$ as it does in linear regression, because we do not use the ANOVA-based statistics in logistic regression.

## Modeling building workflow in R {#sec-logistic-workflow}

Let's put together what we learned about model selection in @sec-ch-model-eval and evaluating logistic regression models in @sec-logistic-model-eval to illustrate an example model building workflow for logistic regression. Here, we will split the data into training and testing sets, and use cross validation with AUC as the criteria for choosing between two candidate models.

The first model is the one we've analyzed in this chapter that includes the predictors `age`, `tech_easy` and `income`. The second model will use these predictors along with `polviews` and `tech_harm`.

First, we define the training and testing sets. We'll use simple random sampling to assign 80% of the observations to the training set and 20% of the observations to the testing set.

```{r}
#| echo: true

set.seed(12345)
aidrive_split <- initial_split(gss24_ai, prop = 0.8)
aidrive_train <- training(aidrive_split)
aidrive_test <- testing(aidrive_split)
```

We can use the training data to evaluate the conditions for logistic regression, linearity and independence. We evaluated these conditions in @sec-logistic-conditions and determined they were satisfied. Therefore, we proceed with modeling and split the data into 5 folds for cross validation.

```{r}
#| echo: true

set.seed(12345)

folds <- vfold_cv(aidrive_train, v = 5)
```

Next, we conduct 5-fold cross validation for Model 1 with the predictors `age`, `tech_easy`, and `income`. We collect a summary of the metrics in cross validation in the objective `aidrive_cv_1_metrics`. Because we are fitting logistic regression models, `collect_metrics()` uses AUC to measure the model performance.

```{r}
#| echo: true

# cross validation workflow for Model 1
aidrive_workflow_1 <- workflow() |>
  add_model(logistic_reg()) |>
  add_formula(aidrive_comfort ~ age + tech_easy + income_fct) 
  
aidrive_cv_1 <- aidrive_workflow_1 |> 
  fit_resamples(resamples = folds) 

aidrive_cv_1_metrics <- collect_metrics(aidrive_cv_1, summarize = TRUE) 
```

We repeat the process of cross validation for Mode 2 that includes the predictors `age`, `tech_easy`, `income_fct`, `polviews_fct`, and `tech_harm`. The performance metrics from cross validation are stored in the object `aidrive_cv_2_metrics`.

```{r}
#| echo: true

# cross validation workflow for Model 2
aidrive_workflow_2 <- workflow() |>
  add_model(logistic_reg()) |>
  add_formula(aidrive_comfort ~ age + tech_easy + income_fct + 
                polviews_fct + tech_harm)
  
aidrive_cv_2 <- aidrive_workflow_2 |> 
  fit_resamples(resamples = folds) 

aidrive_cv_2_metrics <- collect_metrics(aidrive_cv_2, summarize = TRUE) 
```

Now let's look at the average AUC across the 5-fold cross validation for each model. When we do cross validation, the AUC is computed based on a ROC curve fit on the assessment data in each fold. This gives us a view of how well each model performs on new data.

**Model 1**

```{r}
#| echo: true
aidrive_cv_1_metrics |> filter(.metric == "roc_auc")
```

**Model 2**

```{r}
#| echo: true

aidrive_cv_2_metrics |> filter(.metric == "roc_auc")
```

The average AUC is 0.649 for Model 1 and 0.674 for Model 2. Therefore, we select Model 2 that includes the additional predictors `tech_harm` and `polviews_fct`, because it has the higher average AUC across the five folds. This is consistent with the conclusion from AIC and BIC in @sec-logistic-aic-bic. In practice, we will likely consider more than two models, for example, we might consider models with interaction terms or transformations. We will conduct cross validation and compare average AUC for each candidate model. For simplicity, we will only compare two models here and proceed with Model 2 as the final model.

Now that we have selected the final model we refit this model on the entire training set, and we use the testing set to compute AUC as a final evaluation of how well the model performs on new data. We compute the predicted probabilities and construct the ROC curve on the testing data.

```{r}
#| echo: true

# refit model on full training set
aidrive_comfort_final <- glm(aidrive_comfort ~ age + tech_easy + 
                               income_fct + tech_harm + polviews_fct, 
                             data = aidrive_train, 
                             family = "binomial")

# compute predicted probabilities 
aidrive_test <- aidrive_test |>
  mutate(pred_prob = predict(aidrive_comfort_final, newdata = aidrive_test, type = "response"))

# make roc curve 
aidrive_test |> 
    roc_curve(aidrive_comfort, 
              pred_prob, 
            event_level = "second")  |>
  autoplot()
```

```{r}
auc_test <- aidrive_test |> 
  roc_auc(aidrive_comfort, 
              pred_prob, 
            event_level = "second") |>
  pull(.estimate)
```

The AUC for the testing data is `r round(auc_test, 3)`. Given the analysis task of modeling individuals' opinions, this model performs reasonably well in classifying individuals who are comfortable with driverless cars versus those who are not.

As a final step, we refit the model using all observations in the data. At this point, we are ready to use the model for interpretation, drawing inferential conclusions, and to put into production for prediction and classification.

```{r}
#| echo: true

glm(aidrive_comfort ~ age + tech_easy + 
                               income_fct + tech_harm + polviews_fct, 
                             data = gss24_ai, 
                             family = "binomial") |>
  tidy() |>
  kable(digits = 3)
```

## Summary

In this chapter, we expanded on the introduction to logistic regression in @sec-ch-logistic, as we used the logistic regression model to compute predicted log odds, odds, and probabilities. We then used the predicted values to classify observations into $\hat{Y} = 0$ or $\hat{Y} = 1$ . We constructed confusion matrices to evaluate the model performance at individual thresholds, and evaluated classification results using statistics such as sensitivity and specificity, among others. We used the ROC curve to select a classification threshold and computed the area under the curve (AUC) to more holistically evaluate the model fit. We also computed AIC and BIC for model comparison. We applied the model building practices from @sec-ch-model-eval to compare two models using a cross validation workflow.

In the @sec-ch-special-topics, we conclude with some advanced modeling techniques and special topics for analysis in practice.
