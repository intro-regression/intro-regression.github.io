# Simple linear regression {#sec-ch-slr .numbered}

```{r}
#| include: false
source("_common.R")
```

## Learning goals {.unnumbered}

-   Use exploratory data analysis to assess whether a simple linear regression is an appropriate model to describe the relationship between two variables
-   Estimate the slope and intercept for a simple linear regression model
-   Interpret the slope and intercept in the context of the data
-   Use the model to compute predictions and residuals
-   Evaluate model performance using RMSE and $R^2$
-   Conduct simple linear regression using R

## Introduction: Movie ratings {#sec-slr-intro}

```{r}
#| echo: false
library(tidyverse)
library(tidymodels)
library(broom)
library(knitr)
library(kableExtra)
library(fivethirtyeight) #fandango dataset
library(patchwork)
library(scales)
library(skimr)

ggplot2::theme_set(ggplot2::theme_bw(base_size = 14))

# make movie_scores data frame

#movie_scores <- fandango |>
#  rename(critics_score = rottentomatoes, 
 #        audience_score = rottentomatoes_user)  |>
  # select(film, year, critics_score, audience_score)

# movie_scores |> write_csv("data/movie-scores.csv")

movie_scores <- read_csv("data/movie-scores.csv")
```

Reviews from movie critics can be helpful when determining whether a movie is high quality and well-made; however, it can sometimes be challenging to determine whether regular audience members will like a movie based on critics reviews. We would like a way to better understand the relationship between what movie critics and regular movie goers think about a movie, and ultimately predict how an audience will rate a movie based on its score from movie critics.

To do so, we will analyze data that contains the critics scores and audience scores for `r nrow(movie_scores)` movies released in `r min(movie_scores$year)` and `r max(movie_scores$year)`. The scores are for every movie released in these years that have "a rating on Rotten Tomatoes, a RT User rating, a Metacritic score, a Metacritic User score, an IMDb score, and at least 30 fan reviews on Fandango" [@fivethirtyeight]. The analysis in this chapter focuses on scores from [Rotten Tomatoes](#0), a website for information and ratings on movies and television shows. The data were originally analyzed in the article "Be Suspicious of Online Movie Ratings, Especially Fandango's" [@hickey2015suspicious] on the former data journalism website [FiveThirtyEight](#0). The data are available in `movie_scores.csv`. The data set was adapted from the `fandago` data frame in the **fivethirtyeight** R package [@kim2018fivethirtyeight].

We will focus on two variables for this analysis:

-   `critics_score`: The percentage of critics who have a favorable review of the movie. This is known as the "Tomatometer" score on the Rotten Tomatoes website. The possible values are 0 - 100.

-   `audience_score`: The percentage of users (regular movie-goers) on Rotten Tomatoes who have a favorable review of the movie. The possible values are 0 - 100.

::: {.objective latex=""}
Our goal is to use simple linear regression to model the relationship between the critics score and audience score. We want to use the model to

-   describe how the audience score is expected to change as the critics score changes.
-   predict the audience score for a movie based on its critics score.
:::

Recall from @sec-what-is-regression, the **response variable** is the outcome of interest, meaning the variable we are interested in predicting and understanding its variability. It is also known as the *outcome* or *dependent variable* and is represented as $Y$. The **predictor variable(s)** is the variable (or variables) used to understand variability in the response. It is also known as the *explanatory* or *independent variable* and represented as $X$. The observed values of the response and predictor are represented as $y_i$ and $x_i$, respectively.

::: {.yourturn latex=""}
What is the response variable for the movie scores analysis? What is the predictor variable?[^04-slr-1]
:::

[^04-slr-1]: The response variable is `audience`, the audience score. The predictor variable is `critics`, the critics score.

## Exploratory data analysis {#sec-slr-eda}

Recall from @sec-ch-eda that we begin analysis with **exploratory data analysis (EDA)** to better understand the data, the distributions of key variables, and relationships in the data before fitting the regression model. The exploratory data analysis here focuses on the two variables that will be in the regression model, `critics` and `audience`. In practice, however, we may want to explore other variables in the data set (for example, `year` in this analysis) to provide additional context later on as we interpret results from the regression model. We begin with univariate EDA ([@sec-univar-eda]), exploring one variable at a time, then we'll conduct bivariate EDA ([@sec-bivar-eda]) to look at the relationship between the critics scores and audience scores.

### Univariate EDA

The univariate distributions of `critics_score` and `audience_score` are visualized in @fig-movies-univariate and summarized in @tbl-movies-univariate.

```{r}
#| label: fig-movies-univariate
#| fig-cap: "Univariate distributions of `critics_score` and `audience_score`"
#| fig-subcap: 
#|   - Critics score
#|   - Adience score
#| echo: false

ggplot(data = movie_scores, aes(x = critics_score)) + 
  geom_histogram(binwidth = 10, fill = "steelblue", color = "black" ) + 
  labs(y = "Count")

ggplot(data = movie_scores, aes(x = audience_score)) + 
  geom_histogram(binwidth = 10, fill = "steelblue", color = "black") + 
  labs( y = "Count")
```

```{r}
#| label: tbl-movies-univariate
#| tbl-cap: "Summary statistics for `critics_score` and `audience_score`"
#| echo: false

movie_scores |>
  skim(critics_score, audience_score) |>
  select(skim_variable, numeric.mean, numeric.sd, numeric.p0, 
         numeric.p25, numeric.p50, numeric.p75, numeric.p100, n_missing) |>
  kable(col.names = c("Variable", "Mean", "SD", "Min", "Q1", 
                      "Median (Q2)", "Q3", "Max","Missing"), 
        digits = 1)
```

The distribution of `critics_score` is left-skewed, meaning the movies in the data set are generally more favorably reviewed by critics (more observations with higher critics scores). Given the apparent skewness, the center is best described by the median score of 63.5 points. The interquartile range (IQR), the spread of the middle 50% of the distribution, is `r 89 - 31.2` points $(Q_3 - Q_1 = 89 - 31.2)$, so there is a lot of variability in the critics scores for the movies in the data. There are no apparent outliers, but we observe from the raw data that there are two notable observations of movies that have perfect critics scores of 100. There are no missing values of critics score.

::: {.yourturn latex=""}
Use the histogram in @fig-movies-univariate-2 and summary statistics in @tbl-movies-univariate to describe the distribution of the response variable `audience_score`.[^04-slr-2]
:::

[^04-slr-2]: The distribution of `audience_score` is unimodal and left-skewed. The median score is 66.5 and the IQR is 31 (81 - 50). We note that the center is higher and there is less variability in the middle 50% of the distribution compared to `critics_score` .

### Bivariate EDA

Now let's look at the relationship between `critics_score` and `audience_score`. From @sec-bivar-quant-eda, we use visualizations and summary statistics to examine the relationship between two quantitative variables. A scatterplot of the the audience score versus critics score is shown in @fig-movies-bivariate. The predictor variable is on the $x$-axis (horizontal axis), and the response variable is on the $y$-axis (vertical axis).

```{r}
#| label: fig-movies-bivariate
#| echo: false
#| fig-cap: "Scatterplot of `critics_score` and `audience_score`"

movie_cor <- round(cor(movie_scores$critics_score, movie_scores$audience_score), 2)

ggplot(data = movie_scores, mapping = aes(x = critics_score, y = audience_score)) +
  geom_point(alpha = 0.5) + 
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw() + 
  annotate(geom = "label", x = 5, y =92,
    label = paste0("r = ", movie_cor),
    hjust = "left", color = "black", 
    size = 7
  )
```

There is a positive, linear relationship between the critics scores and audience scores for the movies in our data. The correlation between these two variables is `r round(cor(movie_scores$critics_score, movie_scores$audience_score),2)`, indicating the relationship is strong. Therefore, we can generally expect the audience score to be higher for movies with higher critics scores. There are no apparent outliers, but there does appear to be more variability in the audience score for movies with lower critics scores than for those with higher critics scores.

## Linear regression {#sec-fit-regression}

In @sec-slr-eda, we used visualizations and summary statistics to describe the relationship between two variables. The exploratory data analysis, however, does not tell us what the audience score is predicted to be for a given value of the critics score or how much the audience score is expected to change as the critics score changes. Therefore, we will fit a linear regression model to quantify the relationship between the two variables. Recall the general form of the linear regression model in @eq-parametric-model. More specifically, when we have one predictor variable, we will fit a model of the form

$$
Y = \beta_0 + \beta_1 X + \epsilon \hspace{8mm} \epsilon \sim N(0, \sigma^2_{\epsilon})
$$ {#eq-slr}

@eq-slr, called a **simple linear regression (SLR) model**, is the equation to model the relationship between one quantitative response variable and one predictor variable. For now we will focus on models with one quantitative predictor variable. In later chapters, we will introduce models with two or more predictors ([@sec-ch-mlr]), categorical predictors ([@sec-mlr-interpret-categorical]), and models with a categorical response variable ([@sec-ch-logistic]).

We are generally interested in using regression models for two types of tasks:

-   **Prediction:** Finding the expected value of the response variable for given values of the predictor variable(s).
-   **Inference:** Drawing conclusions about the relationship between the response and predictor variables.

::: {.yourturn latex=""}
Suppose we fit a simple linear regression line to summarize the relationship between `critics_score` and `audience_scores` for movies.

-   What is an example of a **prediction** question that can be answered using a simple linear regression model?
-   What is an example of an **inference** question that can be answered using a simple linear regression model?[^04-slr-3]
:::

[^04-slr-3]: **Example prediction question:** What do we expect the audience score to be for movies with a critics score of 75?\
    **Example inference question** Is the critics score a useful predictor of the audience score?

### Statistical (theoretical) model {#sec-slr-specify}

We expand on the concepts introduced in @sec-what-is-regression for the simple linear regression model. Suppose there is a response variable $Y$ and a predictor variable $X$. The values of the response variable $Y$ can be generated in the following way:

$$
Y = \text{Model} + \text{Error}
$$ {#eq-slr-step0}

More specifically, we define the model as a function of the predictor $X$, $\text{Model} = f(X)$, and error $\epsilon$, such that

$$
Y = f(X) + \epsilon
$$ {#eq-general-model-2}

The function $f(X)$ that describes the relationship between the response and predictor variables is the **regression model**. This is the model we will fit in later sections using equations and software. The error, $\epsilon$, is how much the actual value of the response $Y$ deviates from the value produced by the regression model, $f(X)$. <!--# Maybe add the piece from the glm book about stochastic part / random part?-->There is some randomness in $\epsilon$, because not all observations with the same value of $X$ have the same value of $Y$. For example, not all movies with a critics score of 70 have the same audience score.

@eq-general-model-2 is the general form of the equation to generate values of $Y$ given values of $X$. In the context of simple linear regression, the function $f(X)$ in @eq-general-model-2 is

$$
f(X) = \mu_{Y|X} = \beta_0 + \beta_1X
$$ {#eq-slr-function}

where $\mu_{Y|X}$ is the mean value of $Y$ at a particular value of $X$, and $\beta_0$ and $\beta_1$ are the model coefficients. The error terms $\epsilon$ from @eq-general-model-2 are normally distributed with a mean of 0 and variance $\sigma_{\epsilon}^2$, represented as $N(0, \sigma^2_{\epsilon})$ (more on this in @sec-slr-foundation. The specification of the simple linear regression model written in terms of individual observations $(x_i, y_i)$ is

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i \hspace{7mm} \epsilon_i \sim N(0, \sigma_{\epsilon}^2)
$$ {#eq-slr-complete}

such that $y_i$ is the response for the $i^{th}$ observation, $x_i$ is he predictor for the $i^{th}$ observation, and $\epsilon_i$ is the error for the $i^{th}$ observation. @eq-slr-complete is the **statistical model**, also called the *data-generating model* or *population-level model*. It is the theoretical form of the model that describes exactly how to generate the values of the response $Y$ given values of the predictor in the population. The model coefficients are the intercept $\beta_0$ and the slope $\beta_1$. The$\sigma^2_{\epsilon}$ is called the standard error. In practice we don't know the exact values of $\beta_0$, $\beta_1$, and $\sigma_{\epsilon}^2$, so our goal is to use sample data to estimate these values. We will focus on the coefficients $\beta_0$ and $\beta_1$ in this chapter. We discuss estimating $\sigma^2_{\epsilon}$ in @sec-estimate-reg-std-err.

::: {.yourturn latex=""}
In simple linear regression, we use sample data to estimate a model to understand trends in the population.

What is the population in the movie scores analysis? What is the sample?[^04-slr-4] *Recall the definition of population and sample in @sec-regression-analysis.*
:::

[^04-slr-4]: The population is all movies on the Rotten Tomatoes website. The sample is the set of `r nrow(movie_scores)` movies in the data set.

### Evaluating whether SLR is appropriate {#sec-slr-appropriate}

Before doing any more calculations, we need to determine if the simple linear regression model is a reasonable choice to summarize the relationship between the response variable and predictor variable based on what we know about the data and what we've observed from the exploratory data analysis. Determining this early on can help prevent going in a wrong analysis direction if a linear regression model is obviously not a good choice for the data.

We can ask the following questions to evaluate whether simple linear regression is appropriate:

-   Will a linear regression model be practically useful? Does quantifying and interpreting the relationship between the variables make sense in this scenario?
-   Is the shape of the relationship reasonably linear?
-   Do the observations in the data represent the population of interest, or are there biases in the data that could limit conclusions drawn from the analysis? <!--# Differentiate this from the first two bullet points. Make clear if the first two points are violated, we definitely shouldn't do SLR. If this one is violated, we can still do SLR but need to be mindful about the scope of conclusions.-->

Mathematical equations or statistical software can be used to fit a linear regression model between any two quantitative variables. Therefore it is upon the judgment of the data scientist to determine if it is reasonable to proceed with a linear regression model or if doing so might result in misleading conclusions about the data. If the answer is "no" to any of the questions above, consider if a different analysis technique is better for the data, or proceed with caution if using regression. If we proceed with regression, be transparent about some of the limitations of the conclusions.

From @sec-slr-intro, the goal of this analysis is understand the relationship between the critics scores and audience score for movies on Rotten Tomatoes. Therefore, there is a practical use for fitting the regression model. We observed from @fig-movies-bivariate that the relationship between the two variables is approximately linear, so it could reasonably be summarized a model of the form of @eq-slr-complete. Lastly, the data set includes all movies in `r min(movie_scores$year)` and `r max(movie_scores$year)` that has a sufficient number of ratings on popular movie ratings websites, so we can reasonably conclude the sample is representative of the population of movies on Rotten Tomatoes. Therefore, we are comfortable drawing conclusions about the population based on the analysis of our sample data. <!--# Not sure if this last point is a leap-->

The form of the simple linear regression model for the movie scores data is

$$
\text{audience\_score} = \beta_0 + \beta_1~\text{critics\_score} + \epsilon, \hspace{5mm}\epsilon  \sim N(0, \sigma^2_{\epsilon})
$$ {#eq-movies-stat-model}

Now that we have the form of the model, let's discuss how to estimate and interpret the model coefficients, the slope $\beta_1$ and the intercept $\beta_0$. We will estimate $\sigma^2_{\epsilon}$ in @sec-ch-slr-inf.

## Estimating the model coefficients {#sec-slr-estimation}

Ideally, we would have data from the entire population of movies rated on Rotten Tomatoes in order to calculate the exact values for $\beta_0$ and $\beta_1$. In reality we don't have access to the data from the entire population, but we can use the sample to obtain the estimated **regression equation** in @eq-regression

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i
$$ {#eq-regression}

where $\hat{\beta}_0$ is the estimated intercept, $\hat{\beta}_1$ is the estimated slope, and $\hat{y}_i$ is the predicted (estimated) response.

Specifically for the movie scores analysis, the estimated regression equation is

```{r}
#| echo: false

slr_model <- lm(audience_score ~ critics_score, data = movie_scores)

#slope = 0.5187
#intercept = 32.3155

```

$$
\widehat{\text{audience\_score}}_i = 32.316 + 0.519~\text{critics\_score}_i
$$ {#eq-movies-slr}

In this equation 32.316 is $\hat{\beta}_0$, the estimated value for the intercept, 0.519 is $\hat{\beta}_1$, the estimated value for the slope $\beta_1$, and $\widehat{\text{audience\_score}}_i$ is the expected audience score when the critics score is equal to $\text{critics\_score}_i$. Notice that @eq-regression and @eq-movies-slr do not have have error term, $\epsilon$. The output from the regression equation is $\hat{f(X)} = \hat{\mu}_{Y|X}$, the expected *mean* value of the response given a value of the predictor. Therefore, when we discuss the values of the response estimated using simple linear regression, what we are really talking about is what the value of the response variable is expected to be, *on average*, for a given value of the predictor variable.

```{r}
#| label: fig-movies-line
#| eval: false
#| echo: false
#| fig-cap: Least squares regression line for the relationship between critics and audience scores
ggplot(data = movie_scores, mapping = aes(x = critics_score, y = audience_score)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```

From @fig-movies-bivariate, we know that the value of the response is not necessarily the same for all observations with the same value of the predictor. For example, we wouldn't expect (nor do we observe) the same audience score for every movie with a critics score of 70. We know there are other factors other than the critics score that are related to how an audience reacts to a movie. Our analysis, however, only takes into account the critics score, so we do not capture these additional factors in our regression equation [@eq-movies-slr]. This is where the error terms come back in.

Once we computed estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ for the regression equation, we can calculate how far the predicted values of the response produced by the regression equation differ from the actual values of the response variable observed in the data. This difference is called the **residual**, denoted $e_i$. <!--# Introduce residuals before explicitly before estimating the regression line. Maybe it's own subsection?-->

<!--# Key terms box: The residual is the difference between the observed and predicted values of the response for a given observation.-->

@eq-residual shows the equation of the residual for the $i^{th}$ observation.

$$
e_i = \text{observed}_i - \text{predicted}_i =  y_i - \hat{y}_i
$$ {#eq-residual}

In the case of the movie scores data, the residual is the difference between the actual audience score and the audience score predicted by @eq-movies-slr. For example, the 2015 movie *Avengers: Age of Ultron* received a critics score of $y_i = 74$. Therefore, using @eq-movies-slr, the estimated (predicted) audience score is.

$$
\hat{y}_i = 32.316 + 0.519 \times 74 = 70.722.
$$

The observed audience score is 86, so the residual is

$$
e_i = y_i - \hat{y}_i = 86 - 70.722 = 15.278
$$

::: {.yourturn latex=""}
Would you rather see a movie that has a positive or negative residual? Explain your response.[^04-slr-5]
:::

[^04-slr-5]: **Example answer:** I would rather see a movie with a positive residual, because that means the audience actually rated the movie more favorably than what was expected based on the model.

### Least squares regression {#sec-least-squares}

There are many possible regression lines (infinitely many, in fact) that we could use to summarize the relationship between `critics_score` and `audience_scores`. We see some fo the potential lines represented in @fig-many-lines. So how did we determine the line that "best" fits the data is the one described by @eq-movies-slr? We'll use the residuals to help us answer this question.

```{r}
#| label: fig-many-lines
#| echo: false
#| fig-cap: "Potential regression lines for the relationship between `critics_score` and `audience_score`"

ggplot(data = movie_scores, mapping = aes(x = critics_score, y = audience_score)) +
  geom_point(alpha = 0.4) + 
  geom_abline(intercept = 32.296, slope = 0.5187, color = "red", linewidth = 1) +
  geom_abline(intercept = 25, slope = 0.7, color = "gray") +
  geom_abline(intercept = 21, slope = 0.9, color = "gray") +
  geom_abline(intercept = 35, slope = 0.3, color = "gray") +
  labs(x = "Critics Score" , 
       y = "Audience Score")
```

The residuals, represented by the vertical dotted lines in @fig-scatterplot-resid, are a measure of the "error", the difference between the observed value of the response and the value predicted from a regression model. The line that "best" fits the data is the one that generally results in the smallest overall error. One way to find the line with the smallest overall error is to add up all the residuals for each possible line in @fig-many-lines and choose the one that has the smallest sum. Notice, however, that for lines that seem to closely align with the trend of the data, there is approximately equal distribution of points above and below the line. Thus as we're trying to compare lines that pretty closely fit the data, we'd expect the residuals to add up to a value very close to zero. This would make it difficult, then, to determine a best fit line.

```{r}
#| label: fig-scatterplot-resid
#| echo: false
#| fig-cap: "Regression line of the relationship between `critics_score` and `audience_score` with residuals"

m <- lm(audience_score ~ critics_score, data = movie_scores)
ggplot(data = movie_scores, mapping = aes(x = critics_score, y = audience_score)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method ="lm", color = "red", se = FALSE) + 
  geom_segment(aes(x = critics_score, xend = critics_score, y = audience_score, yend= predict(m, movie_scores)), color = "steel blue",linetype = 3 ) +
  labs(x = "Critics Score" , 
       y = "Audience Score")+
  theme(legend.position = "none")
```

Instead of using the sum of the residuals, we will instead consider the sum of the squared residuals in @eq-sum-sq-resid

$$
\sum_{i=1}^n e_i^2 = e_1^2 + e_2^2 + \dots + e_n^2
$$ {#eq-sum-sq-resid}

where $n$ is the number of observations in the data. The line that "best" fits the data, then, is the line that minimizes @eq-sum-sq-resid. This is called the **least squares regression model.**

<!--# Key terms box: The least squares regression line is the line, $\hat{\beta}_0 + \hat{\beta}_1 ~X$ , that minimizes the sum of the squared residuals.-->

<!-- add something about why not the absolute value-->

Let's expand @eq-sum-sq-resid. Recall that $e_i$, the residual of the $i^{th}$ observation, is $y_i - \hat{y}_i$ where $\hat{y}_i$ is the estimated response. Then,

$$
\begin{aligned}
e_i &= y_i - \hat{y}_i \\
&= y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i)
\end{aligned}
$$ {#eq-least-sq-pt1}

Thus, putting @eq-least-sq-pt1 into @eq-sum-sq-resid, we have

$$
\begin{aligned}
\sum_{i=1}^n e_i^2 &= e_1^2 + e_2^2 + \dots + e_n^2 \\
&= [y_1 - (\hat{\beta}_0 + \hat{\beta}_1x_1)]^2 + [y_2 - (\hat{\beta}_0 + \hat{\beta}_1x_2)]^2 + & \\
& \dots + [y_n - (\hat{\beta}_0 + \hat{\beta}_1x_n)]^2
\end{aligned}
$$ {#eq-least-sq-pt2}

Using calculus, the $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize @eq-least-sq-pt2 are

$$
\hat{\beta}_1 = r\frac{s_Y}{s_X} \hspace{10mm} \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$ {#eq-least-sq-estimates}

where $\bar{x}$ and $\bar{y}$ are the mean values of the predictor and response variables, respectively, $s_X$ and $s_Y$ are the standard deviations of the predictor and response variables, respectively, and $r$ is the correlation between the response and predictor variables. See [Appendix @sec-least-sq-math] for the full details of the derivation from @eq-least-sq-pt2 to @eq-least-sq-estimates.

@eq-slr-computations show the calculations of slope and intercept for the movie scores model based on the summary statistics in @tbl-movies-univariate. Note that the small differences in the values compared to @eq-movies-slr are due to rounding (versus coefficients computed by software).

$$
\begin{aligned}
\hat{\beta}_1 &= 0.78 \times \frac{20.0}{30.2} = 0.517 \\ \hat{\beta}_0 &=  63.9 - 0.517 \times  60.8 = 32.467
\end{aligned}
$$ {#eq-slr-computations}

::: {.analysis_in_practice latex=""}
Below are a few properties of least-squares regression models.

-   The regression line goes through the center of mass point, the coordinates corresponding to average $X$ and average $Y$: $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$

-   The slope has the same sign as the correlation coefficient: $\hat{\beta}_1 = r\frac{s_Y}{s_X}$

-   The sum of the residuals is zero: $\sum_{i=1}^n e_i = 0$

-   The residuals and values of the predictor are uncorrelated
:::

## Interpreting the model coefficients {#sec-slr-interpret-coef}

The **slope** $\hat{\beta}_1$ is the estimated change in the response for each unit increase in the predictor variable. What do we mean by "estimated change"? Recall that the output from the regression equation is ${\mu}_{Y|X}$ the estimated mean of the response $Y$ for a given value of the predictor $X$. Thus, the slope or the "steepness" of the regression line, is a measure of how much the response variable is expected to change, on average, for each unit increase of the predictor.

It is good practice to write the interpretation of the slope in the context of the data, so that it can be more easily understood by others reading the analysis results. "In the context of the data" means that the interpretation includes

-   meaningful descriptions of the variables, if the variable names would be unclear to an outside reader
-   units for each variable
-   an indication of the population for which the model applies.

The slope in @eq-movies-slr of 0.519 is interpreted as the following:

> *For each additional point in the critics score, the audience score for movies on Rotten Tomatoes is expected to increase by 0.519 points, on average.*

The **intercept** is the estimated value of the response variable when the predictor variable equals zero $(x_i = 0)$. On a scatterplot of the response and predictor variable, this is the point where the regression line crosses the $y$-axis. Similar to the slope, the "estimated value" is more specifically the estimated mean value of the response variable when the predictor equals 0 ( $\hat{\mu}_{Y|X = 0})$.

The intercept in @eq-movies-slr of 32.316 is interpreted as the following:

> *The expected audience score for movies on Rotten Tomatoes with a critics score of 0 is 32.316 points.*

We always need to estimate the intercept in @eq-regressio to get the line that best fit using least squares regression. The intercept, however, does not always have a meaningful interpretation. We ask the following questions to determine if the intercept has a meaningful interpretation:

1.  Is it plausible for the predictor variable to take values at or near zero?

2.  Are there observations in the data with values of the predictor at or near zero?

If the answer to either question is no, then it is not meaningful, and potentially misleading, to interpret the intercept.

::: {.yourturn latex=""}
Is the interpretation of the intercept in @eq-movies-slr meaningful? Briefly explain.[^04-slr-6]
:::

[^04-slr-6]: The interpretation of the intercept is meaningful, because it is plausible for a movie to have a critics score of 0 and there are observations with scores around 5, which is near 0 on the 0 - 100 point scale.

::: {.analysis_in_practice latex=""}
Avoid using causal language and making declarative statements (e.g., "The audience score for a movie with a critics score of 0 points [**will**]{.underline} be 32.316 points.") when interpreting the slope and intercept. Remember the slope and intercept are estimates describing what is [**expected**]{.underline}in the relationship between the response and predictor to be based on the sample data and linear regression model. They do not tell us exactly what will happen in the data.

<br>\
There is an area of statistics called *causal inference* about model that can be used to make causal statements from observational (non-experimental) data. See @sec-causal for a brief introduction to causal inference.
:::

## Prediction {#sec-slr-predict}

One of the primary purposes of a regression model is to use for prediction. When a regression model is used for prediction, the estimated value of the response variable is computed based on a given value of the predictor. We've seen this in earlier sections when calculating the residuals. Let's take a look at the model predictions for two movies released in 2023.

The movie *Barbie* was released in theaters on July 21, 2023. This movie was widely praised by critics, and it has a critics score of 88 at the time the data were obtained. Based on @eq-movies-slr, the predicted audience score is

$$
\begin{aligned}
\widehat{\text{audience\_scores}} &= 32.316 + 0.519 \times 88 \\ 
&= \textbf{77.988} \\
\end{aligned}
$$

From the snapshot of the Barbie Rotten Tomatoes page (@fig-barbie), we see the actual audience score is 83[^04-slr-7]. Therefore, the model under predicted the audience score by about 5 points (83 - 77.988). Perhaps this isn't surprising given this film's massive box office success! <!--# do i need a citation for this statement?-->

[^04-slr-7]: Source: https://www.rottentomatoes.com/m/barbie Accessed on August 29, 2023.

![Source: https://www.rottentomatoes.com/m/barbie (accessed August 29, 2023)](images/04-barbie.png){#fig-barbie fig-align="center"}

::: {.yourturn latex=""}
The movie *Asteroid City* was released in theaters on June 23, 2023. The critics score for this movie was 75[^04-slr-8].

-   What is the predicted audience score?

-   The actual audience score is 62. Did the model over or under predict? What is the residual? [^04-slr-9]
:::

[^04-slr-8]: Source: https://www.rottentomatoes.com/m/asteroid_city Accessed on August 29, 2023.

[^04-slr-9]: The predicted audience score is 32.316 + 0.519 \* 75 = **71.241**. The model over predicted. The residual is 62 - 71.241 = **-9.241.**

<!--# can i remove this paragraph and just use the callout box about extraploation-->

The regression model is most reliable when predicting the response for values of the predictor within the range of the sample data used to fit the regression model. Using the model to predict for values far outside this range is called **extrapolation*.*** The sample data provide information about the relationship between the response and predictor variables for values within the range of the predictor in the data. We can not safely assume that the linear relationship quantified by our model is the same for values of the predictor far outside of this range. Therefore, extrapolation often results in unreliable predictions that could be misleading if the linear relationship does not hold outside the range of the sample data.

::: {.analysis_in_practice latex=""}
Only use the regression model to compute predictions for values of the predictor that are within (or very close) to the range of values in the sample data used to fit the model. Extrapolation, using a model to compute predictions for value so the predictor far outside the range in the data, can result in unreliable predictions.
:::

## Model evaluation {#sec-model-assessment}

We have shown how a simple linear regression model can be used to describe the relationship between a response and predictor variable and to predict new values of the response. Now we will look at two statistics that will help us evaluate how well the model fits the data and how well it explains variability in the response.

### Root Mean Square Error

The **Root Mean Square Error (RMSE)**, shown in @eq-slr-rmse, is a measure of the average difference between the observed and predicted values of the response variable.

$$
RMSE = \sqrt{\frac{\sum_{i=1}^ne_i^2}{n}} = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n}}
$$ {#eq-slr-rmse}

This measure is especially useful if prediction is the primary modeling objective. The RMSE takes values from 0 to $\infty$ (infinity) and has the same units as the response variable.

::: {.yourturn latex=""}
Do higher or lower values of RMSE indicate a better model fit?[^04-slr-10]
:::

[^04-slr-10]: Lower values indicate a better fit, with 0 indicating the predictor variable perfectly predicts the response.

There is no universal threshold of RMSE to determine whether the model is a good fit. In fact, the RMSE is often most useful when comparing the performance of multiple models. Take the following into account when using RMSE to evaluate model fit.

1.  What is the range $(\text{max} - \text{min})$ of the response variable in the data? How does the RMSE compare to the range? For example, $RMSE = 10$ indicates very good model performance if the response variable ranges from 10000 to 20000, but very poor model performance if the response variable ranges from 0 to 20.
2.  What is a reasonable error threshold based on the subject matter and analysis objectives? We may be willing to use a model with higher RMSE for a low-stakes analysis objective (for example, the model is used to inform the choices of movie-goers) than a high-stakes objective (the model is used to inform how a movie studio's multi-million dollar marketing budget will be allocated).

```{r}
#| echo: false
rmse <- sqrt(sum(slr_model$residuals^2)/nrow(movie_scores))
```

::: {.yourturn latex=""}
The RMSE for the movie scores model is `r round(rmse,3)`. The range for the audience score is `r max(movie_scores$audience) - min(movie_scores$audience)`. What is your evaluation of the model fit based on RMSE? Explain your response.[^04-slr-11]
:::

[^04-slr-11]: **Example answer:** An error of 12.452 is about a 17% error based on the range of the audience scores. Because the audience scores range 0 to 100, this error seems relatively large.

### Analysis of variance and $R^2$ {#sec-slr-sum-sq}

The **coefficient of determination**, $R^2$, the percentage of variability in the response variable that is explained by the predictor variable. In terms of the movie scores data, it is the percentage of variability in the audience score that is accounted for by changes in the critics score. Before talking more about how $R^2$ is used for model evaluation, let's discuss how this percentage is calculated.

There is variability in the response variable, as we see in the exploratory data analysis in @fig-movies-univariate and @tbl-movies-univariate. **Analysis of Variance (ANOVA)**, shown in @eq-anova, is the process of partitioning the various sources of variability.

$$
\text{Total variability} = \text{Explained variability} + \text{Unexplained variability}
$$ {#eq-anova}

From @eq-anova, the variability in the response variable is from two sources:

1.  **Explained variability (Model)**: This is the variability in the response variable that can be explained by the model. In the case of simple linear regression, it is the variability in the response variable that can be explained by the predictor variable. In the movie scores analysis, this is the variability in `audience_score` that is explained by the `critics_score`.

2.  **Unexplained variability (Residuals)**: This is the variability in the response variable that is left unexplained after the model is fit. This can be understood by assessing the variability in the residuals. In the movie scores analysis, this is the variability due to the factors other than critics score.

The variability in the response variable and the contribution from each source is quantified using sum of squares. In general, the **sum of squares (SS)** is a measure of how far the observations are from a given point, for example the mean. Using sum of squares, we can quantify the components of @eq-anova.

Let $SST$ = Sum of Squares Total, $SSM$ = Sum of Squares Model, and $SSR$ = Sum of Squares Residuals. Then,

$$
\begin{aligned}
SST &= SSM + SSR \\[10pt]
\sum_{i=1}^n (y_i - \bar{y})^2 &= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n(y_i - \hat{y}_i)^2
\end{aligned}
$$ {#eq-anova-ss}

<!--# add link to appendix to see mathematically-->

**Sum of Squares Total (SST)** $= \sum_{i=1}^n(y_i - \bar{y})^2$, is the total variability, an overall measure of how far the observed values of the response variable are from the mean value of the response $\bar{y}$. The formula for SST may look familiar, as it is $(n-1)s_y^2$ , which equals$(n-1)$ times the variance of $y$. SST can be partitioned into two pieces, Sum of Squares Model (SSM) and Sum of Squares Residuals (SSR).

**Sum of Squares Model (SSM)** $= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2$, is the explained variability, an overall measure of how much the predicted value of the response variable (the expected mean value of the response given the predictor) differs from the overall mean value of the response. This indicates how much the observed response's deviation from the mean is accounted for by knowing the value of the predictor.

Lastly, the **Sum of Squares Residual (SSR)** $= \sum_{i=1}^n(y_i - \hat{y}_i)^2$, is the unexplained variability, an overall measure of how much the observed values of the response differ from the predicted values. This is the same sum of squared residuals used to estimate the least-squares regression model in @sec-least-squares.

We use the sum of squares to calculate the **coefficient of determination** $R^2$

$$
R^2 = \frac{SSM}{SST} = 1 - \frac{SSR}{SST}
$$ {#eq-rsq}

<!--# add images breaking down variability-->

```{r}
#| echo: false

rsq <- glance(slr_model)$r.squared
```

@eq-rsq, shows that $R^2$ is a the proportion of variability in the response (SST) that is explained by the model (SSM). Note that $R^2$ is calculated as proportion between 0 and 1, but is reported as a percentage between 0% and 100%.

The $R^2$ for the model in @eq-movies-slr is `r round(rsq, 3)`. It is interpreted as the following:

> *About `r round(rsq, 3) * 100`% of the variability in the audience score for movies on Rotten Tomatoes can be explained by the model (critics score).*

::: {.yourturn latex=""}
Do higher or lower values of $R^2$ indicate a better model fit?[^04-slr-12]
:::

[^04-slr-12]: Higher values of $R^2$ indicate a better model fit, as it means more of the variability in the response is being explained by the model.

Similar to RMSE, there is no universal threshold for what makes a "good" $R^2$ value. When using $R^2$ to determine if the model is a good fit, take into account what might be a reasonable to expect given the subject matter.

## Simple linear regression in R {#sec-slr-R}

### Fitting the least-squares model {#sec-slr-fit-R}

We fit linear regression models using the `lm` function, which is part of the **stats** package [@stats] built into R. We then use the `tidy` function from the **broom** package [@broom-2] to display the results in a *tidy* data format ([@sec-tidy-data]). The code to find the linear regression model using the `movie_scores` data with `audience_score` as the response and `critics_score` as the predictor (@eq-movies-slr) is below.

```{r}
#| label: slr-model-1
#| echo: true
#| eval: true

lm(audience_score ~ critics_score, data = movie_scores)
```

Next, we want to display the model results in a tidy format. We build upon the code above by saving the model in an object called `movie_fit` and displaying the object. We will also use `movie_fit` to calculate predictions.

```{r}
#| label: slr-model-2
#| echo: true
#| eval: true

movie_fit <- lm(audience_score ~ critics_score, data = movie_scores) 

tidy(movie_fit) 

```

Notice the resulting the model is the same as @eq-movies-slr, which we calculated based on @eq-least-sq-estimates. We will discuss the other columns in the output in @sec-ch-slr-inf.

We can also use `kable()` from the **knitr** package [@knitr] to display the tidy results in an neatly formatted table and control the number of digits in the output.

```{r}
#| echo: true

tidy(movie_fit) |>
  kable(digits = 3)
```

### Prediction

Below is the code to predict the audience score for *Barbie* as shown earlier in the section. We create a tibble that contains the critics score for *Barbie*, then use `predict()` and the model object to compute the prediction.

```{r}
#| label: slr-predict-barbie
#| echo: true

barbie_movie <- tibble(critics_score = 88) 
predict(movie_fit, barbie_movie)

```

We can also produce predictions for multiple movies by putting multiple values of the predictor in the tibble. In the code below we produce predictions for *Barbie* and *Asteroid City*. We begin by storing the critics scores for both movies in a tibble. Then we use `predict()`, as before.

```{r}
#| label: slr-predict-multiple
#| echo: true

new_movies <- tibble(critics_score = c(88, 75)) 
predict(movie_fit, new_movies) 
```

### $R^2$ and RMSE

The `glance()` function in the **broom** package produces model summary statistics, including $R^2$.

```{r}
#| echo: true 
glance(movie_fit)
```

The code below will only return $R^2$ from the output of `glance()`.

```{r}
#| echo: true

glance(movie_fit)$r.squared
```

RMSE is computed using the `rmse()` function from the **yardstick** package [@yardstick]. First, we use `augment()` from the **broom** package to compute the predicted value for each observation in the data set. These values are stored if the column `.fitted`. We may notice that many other columns are produced by `augment()` as well; these are discussed in @sec-ch-slr-conditions. We input the augmented data into `rmse()`.

```{r}
#| echo: true


movies_augment <- augment(movie_fit) 

rmse(movies_augment, truth = audience_score, estimate = .fitted) 
```

## Summary

In this chapter, we introduced simple linear regression. We showed how to use exploratory data analysis to evaluate whether linear regression is appropriate to model the relationship between two variables. Next, we computed the slope and intercept (the model coefficients) and interpreted these values in in the context of the data. We used the model to compute predictions and evaluated the model performance using $R^2$ and RMSE. We finished the chapter by conducting simple linear regression in R.

This chapter has helped set the foundation for all the regression methods presented throughout the remainder of the text. In @sec-ch-slr-inf, we'll use the simple linear regression model to draw conclusions about the relationship between the response and predictor variables.

<!--Partitioning variability and $R^2$-->

<!--Reference notes: https://sta210-fa21.netlify.app/slides/08-partition-var.html#1-->

<!--Introduce $R^2$ here, but does it make sense to move this the inference chapter?-->

<!--Could also introduce root mean square error here.-->

<!-- chapter summary-->

<!--Computing lab assignments-->

<!--Homework assignments-->

<!--north carolina assignment?-->
