<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Maria Tackett">
<title>Appendix A — Mathematics of linear regression – Introduction to Regression Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./appendix-logistic.html" rel="next">
<link href="./13-special-topics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-31c3c5ec0f84e3846e28bd1517aa8340.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script defer="" data-domain="https://matackett.github.io/regression-book/" src="https://plausible.io/js/script.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./appendix-linear.html">Appendices</a></li><li class="breadcrumb-item"><a href="./appendix-linear.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Mathematics of linear regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Regression Analysis</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/matackett/regression-book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 1: Getting started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regression in the data science workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-review-r.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data analysis in R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Exploratory data analysis</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 2: Simple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Simple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-slr-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inference for simple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-slr-conditions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model conditions and diagnostics</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 3: Multiple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-mlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multiple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-mlr-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Inference for multiple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-mlr-transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Variable transformations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-model-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 4: Beyond linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-logistic-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Logistic regression: Prediction and evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-special-topics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Special topics</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linear.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Mathematics of linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Mathematics of logistic regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data sets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-least-sq-math" id="toc-sec-least-sq-math" class="nav-link active" data-scroll-target="#sec-least-sq-math"><span class="header-section-number">A.1</span> Least-squares estimators for simple linear regression</a></li>
  <li><a href="#sec-mlr-matrix" id="toc-sec-mlr-matrix" class="nav-link" data-scroll-target="#sec-mlr-matrix"><span class="header-section-number">A.2</span> Matrix representation of linear regression</a></li>
  <li>
<a href="#sec-mlr-estimation-matrix" id="toc-sec-mlr-estimation-matrix" class="nav-link" data-scroll-target="#sec-mlr-estimation-matrix"><span class="header-section-number">A.3</span> Estimating the Coefficients</a>
  <ul class="collapse">
<li><a href="#sec-least-sq-matrix" id="toc-sec-least-sq-matrix" class="nav-link" data-scroll-target="#sec-least-sq-matrix"><span class="header-section-number">A.3.1</span> Least-squares estimation</a></li>
  <li><a href="#sec-geometry" id="toc-sec-geometry" class="nav-link" data-scroll-target="#sec-geometry"><span class="header-section-number">A.3.2</span> Geometry of least-squares regression</a></li>
  </ul>
</li>
  <li><a href="#sec-hat-matrix" id="toc-sec-hat-matrix" class="nav-link" data-scroll-target="#sec-hat-matrix"><span class="header-section-number">A.4</span> Hat matrix</a></li>
  <li><a href="#sec-assumptions-matrix" id="toc-sec-assumptions-matrix" class="nav-link" data-scroll-target="#sec-assumptions-matrix"><span class="header-section-number">A.5</span> Assumptions of linear regression</a></li>
  <li><a href="#sec-beta-dist-matrix" id="toc-sec-beta-dist-matrix" class="nav-link" data-scroll-target="#sec-beta-dist-matrix"><span class="header-section-number">A.6</span> Distribution of model coefficients</a></li>
  <li><a href="#sec-multicollinearity-matrix" id="toc-sec-multicollinearity-matrix" class="nav-link" data-scroll-target="#sec-multicollinearity-matrix"><span class="header-section-number">A.7</span> Multicollinearity</a></li>
  <li><a href="#sec-appendix-linear-mle" id="toc-sec-appendix-linear-mle" class="nav-link" data-scroll-target="#sec-appendix-linear-mle"><span class="header-section-number">A.8</span> Maximum Likelihood Estimation</a></li>
  <li>
<a href="#sec-var-transformations-math" id="toc-sec-var-transformations-math" class="nav-link" data-scroll-target="#sec-var-transformations-math"><span class="header-section-number">A.9</span> Variable transformations</a>
  <ul class="collapse">
<li><a href="#sec-transform-y-math" id="toc-sec-transform-y-math" class="nav-link" data-scroll-target="#sec-transform-y-math"><span class="header-section-number">A.9.1</span> Transformation on the response variable</a></li>
  <li><a href="#sec-transform-x-math" id="toc-sec-transform-x-math" class="nav-link" data-scroll-target="#sec-transform-x-math"><span class="header-section-number">A.9.2</span> Transformation on predictor variable(s)</a></li>
  <li><a href="#sec-transform-x-y-math" id="toc-sec-transform-x-y-math" class="nav-link" data-scroll-target="#sec-transform-x-y-math"><span class="header-section-number">A.9.3</span> Transformation on response and predictor variables</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/matackett/regression-book/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./appendix-linear.html">Appendices</a></li><li class="breadcrumb-item"><a href="./appendix-linear.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Mathematics of linear regression</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-appendix-linear" class="quarto-section-identifier">Appendix A — Mathematics of linear regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="sec-least-sq-math" class="level2" data-number="A.1"><h2 data-number="A.1" class="anchored" data-anchor-id="sec-least-sq-math">
<span class="header-section-number">A.1</span> Least-squares estimators for simple linear regression</h2>
<p>Below are the mathematical details for deriving the least-squares estimators for slope (<span class="math inline">\(\beta_1\)</span>) and intercept (<span class="math inline">\(\beta_0\)</span>) using calculus. We obtain the estimators <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by finding the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively, that minimize the sum of squared residuals (<a href="#eq-ssr" class="quarto-xref">Equation&nbsp;<span>A.1</span></a>).</p>
<p>Suppose we have a data set with <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>. Then the sum of squared residuals is</p>
<p><span id="eq-ssr"><span class="math display">\[
SSR = \sum\limits_{i=1}^{n}[y_i - \hat{y}_i]^2 = [y_i - (\beta_0 + \beta_1 x_i)]^2 = [y_i - \beta_0 - \beta_1 x_i]^2
\tag{A.1}\]</span></span></p>
<p>To find the value of <span class="math inline">\(\beta_0\)</span> that minimizes <a href="#eq-ssr" class="quarto-xref">Equation&nbsp;<span>A.1</span></a>, we begin by taking the partial derivative of <a href="#eq-ssr" class="quarto-xref">Equation&nbsp;<span>A.1</span></a> with respect to <span class="math inline">\(\beta_0\)</span>. Similarly, we take the partial derivative with respect to <span class="math inline">\(\beta_1\)</span> to find the value of <span class="math inline">\(\beta_1\)</span> that minimizes <a href="#eq-ssr" class="quarto-xref">Equation&nbsp;<span>A.1</span></a>. The partial derivatives are</p>
<p><span id="eq-par-deriv"><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial \text{SSR}}{\partial \beta_0} = -2 \sum\limits_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i) \\[10pt]
&amp;\frac{\partial \text{SSR}}{\partial \beta_1} = -2 \sum\limits_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1 x_i)
\end{aligned}
\tag{A.2}\]</span></span></p>
<p>Therefore, we want to find <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that satisfy the following:</p>
<p><span id="eq-par-deriv-estimates"><span class="math display">\[
\begin{aligned}
&amp;-2 \sum\limits_{i=1}^{n}(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\[10pt]
&amp;-2 \sum\limits_{i=1}^{n}x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{aligned}
\tag{A.3}\]</span></span></p>
<p>Let’s focus on <span class="math inline">\(\hat{\beta}_0\)</span> for now and find <span class="math inline">\(\hat{\beta}_0\)</span> that satisfies the first equality in <span class="quarto-unresolved-ref">?eq-par-deriv-estimators</span>. The mathematical steps are below in <a href="#eq-est-beta0" class="quarto-xref">Equation&nbsp;<span>A.4</span></a></p>
<p><span id="eq-est-beta0"><span class="math display">\[
\begin{aligned}&amp;-2 \sum\limits_{i=1}^{n}(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}(y_i - \sum\limits_{i=1}^{n} \hat{\beta}_0 - \sum\limits_{i=1}^{n} \hat{\beta}_1 x_i) = 0 \\[10pt]
&amp;\Rightarrow  \sum\limits_{i=1}^{n}y_i - n\hat{\beta}_0 - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i = 0 \\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}y_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i = n\hat{\beta}_0  \\[10pt]
&amp;\Rightarrow \frac{1}{n}\Big(\sum\limits_{i=1}^{n}y_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i\Big) = \hat{\beta}_0  \\[10pt]
&amp;\Rightarrow \bar{y} - \hat{\beta}_1 \bar{x} = \hat{\beta}_0 \\
\end{aligned}
\tag{A.4}\]</span></span></p>
<div class="math_rules">
<p>The last line of <a href="#eq-est-beta0" class="quarto-xref">Equation&nbsp;<span>A.4</span></a> is derived from the fact that <span class="math inline">\(\bar{y} = \frac{1}{n}\sum_{i=1}^ny_i\)</span> and <span class="math inline">\(\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\)</span>.</p>
</div>
<p>From <a href="#eq-est-beta0" class="quarto-xref">Equation&nbsp;<span>A.4</span></a>, we know the <span class="math inline">\(\hat{\beta}_0\)</span> that satisfies the first equality in <a href="#eq-par-deriv-estimates" class="quarto-xref">Equation&nbsp;<span>A.3</span></a> is</p>
<p><span id="eq-beta0"><span class="math display">\[\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} \tag{A.5}\]</span></span></p>
<p>The formula for <span class="math inline">\(\hat{\beta}_0\)</span> contains <span class="math inline">\(\hat{\beta}_1\)</span>, so now let’s find the value of <span class="math inline">\(\hat{\beta}_1\)</span> that satisfies the second equality in <a href="#eq-par-deriv-estimates" class="quarto-xref">Equation&nbsp;<span>A.3</span></a>. The mathematical steps are below in <a href="#eq-beta1-pt1" class="quarto-xref">Equation&nbsp;<span>A.6</span></a>.</p>
<p><span id="eq-beta1-pt1"><span class="math display">\[
\begin{aligned}
&amp;-2 \sum\limits_{i=1}^{n}x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0  \\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i - \hat{\beta}_0\sum\limits_{i=1}^{n}x_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = 0 \\[10pt]
\text{(Fill in }\hat{\beta}_0\text{)}&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i - (\bar{y} - \hat{\beta}_1\bar{x})\sum\limits_{i=1}^{n}x_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = 0 \\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i = (\bar{y} - \hat{\beta}_1\bar{x})\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 \\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i =  \bar{y}\sum\limits_{i=1}^{n}x_i - \hat{\beta}_1\bar{x}\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2\\[10pt]
(\text{Given }\bar{x} = \sum_{i=1}^n x_i)&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i  = n\bar{y}\bar{x} - \hat{\beta}_1n\bar{x}^2 + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2\\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x} = \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 - \hat{\beta}_1n\bar{x}^2\\[10pt]
&amp;\Rightarrow \sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x} = \hat{\beta}_1\Big(\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2\Big)\\[10pt]
&amp;\Rightarrow \frac{\sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x}}{\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2} = \hat{\beta}_1
\end{aligned}
\tag{A.6}\]</span></span></p>
<p>We will use the following rules to write <a href="#eq-beta1-pt1" class="quarto-xref">Equation&nbsp;<span>A.6</span></a> in a form that is more recognizable:</p>
<p><span id="eq-cov"><span class="math display">\[
\sum_{i=1}^n x_iy_i - n\bar{y}\bar{x} = \sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) = (n-1)\text{Cov}(x,y)
\tag{A.7}\]</span></span></p>
<p><span id="eq-var-x"><span class="math display">\[
\sum_{i=1}^n x_i^2 - n\bar{x}^2 = \sum_{i=1}^n(x_i - \bar{x})^2 = (n-1)s_x^2
\tag{A.8}\]</span></span></p>
<p>where <span class="math inline">\(\text{Cov}(x,y)\)</span> is the covariance of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and <span class="math inline">\(s_x^2\)</span> is the sample variance of <span class="math inline">\(x\)</span> (<span class="math inline">\(s_x\)</span> is the sample standard deviation).</p>
<p>Applying <a href="#eq-cov" class="quarto-xref">Equation&nbsp;<span>A.7</span></a> and <a href="#eq-var-x" class="quarto-xref">Equation&nbsp;<span>A.8</span></a> to <a href="#eq-beta1-pt1" class="quarto-xref">Equation&nbsp;<span>A.6</span></a>, we have</p>
<p><span id="eq-est-beta1-pt2"><span class="math display">\[
\begin{aligned}
\hat{\beta}_1 &amp;= \frac{\sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x}}{\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2} \\[10pt]
&amp;= \frac{\sum\limits_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}\\[10pt]
&amp;= \frac{(n-1)\text{Cov}(x,y)}{(n-1)s_x^2}\\[10pt]
&amp;= \frac{\text{Cov}(x,y)}{s_x^2}
\end{aligned}
\tag{A.9}\]</span></span></p>
<div class="math_rules">
<p>The correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is <span class="math display">\[r = \frac{\text{Cov}(x,y)}{s_x s_y}\]</span></p>
<p>Therefore, <span class="math display">\[\text{Cov}(x,y) = r s_xs_y\]</span></p>
<p>where <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the sample standard deviations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, respectively.</p>
</div>
<p>Plugging in the formula for <span class="math inline">\(Cov(x,y)\)</span> into <a href="#eq-est-beta1-pt2" class="quarto-xref">Equation&nbsp;<span>A.9</span></a>, we have</p>
<p><span class="math display">\[
\hat{\beta}_1 = \frac{\text{Cov}(x,y)}{s_x^2} = r\frac{s_ys_x}{s_x^2} = r\frac{s_y}{s_x}
\]</span></p>
<p>We have found values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that satisfy <a href="#eq-par-deriv-estimates" class="quarto-xref">Equation&nbsp;<span>A.3</span></a>. Now we need to confirm that we have found the minimum value (rather than a maximum value or saddle point). To do, we use the second partial derivative. If it is positive, then we know we have found a minimum.</p>
<p>The second partial derivatives are</p>
<p><span id="eq-ssr-second-deriv"><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial^2 \text{SSR}}{\partial \beta_0^2} = \frac{\partial}{\partial \beta_0}\Big(-2 \sum\limits_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)\Big) = 2n &gt; 0 \\[10pt]
&amp;\frac{\partial^2 \text{SSR}}{\partial \beta_1^2} = \frac{\partial}{\partial \beta_1}\Big(-2 \sum\limits_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1 x_i)\Big) = 2\sum_{i=1}^nx_i^2 &gt; 0
\end{aligned}
\tag{A.10}\]</span></span></p>
<p>Both partial derivatives are greater than 0, so we have shown that the estimators <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, in fact, minimize SSR.</p>
<div class="results">
<p>The least-squares estimators for the intercept and slope are</p>
<p><span class="math display">\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
\hspace{20mm} \hat{\beta}_1 = r\frac{s_y}{s_x}
\]</span></p>
</div>
<!--# Linear algebra for regression-->
<!--# Matrix representation of simple linear regression-->
<!--# Analysis of variance

From @sec-slr-sum-sq, we have the following

$$
\begin{aligned}
SST &= SSM + SSR \\
\sum_{i=1}^{n}(y_i - \bar{y})^2 &= \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n}(y_i -\hat{y}_i)^2
\end{aligned}
$$ {#eq-ss}

where $n$ is the number of observations. We will show why @eq-ss is true mathematically by starting with the fact that $(y_i - \bar{y}) = (\hat{y}_i - \bar{y}) + (y_i - \hat{y}_i)$

$$
\begin{aligned}
(y_i - \bar{y})^2 &= [(\hat{y}_i - \bar{y}) + (y_i - \hat{y}_i)]^2\\[10pt]
& = (\hat{y}_i - \bar{y})^2 + 2(\hat{y}_i - \bar{y})(y_i - \hat{y}_i) + (y_i - \hat{y}_i)^2 
\end{aligned}
$$

We can sum over both sides to get

$$
\sum_{i=1}^n(y_i - \bar{y})^2 = \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 + 2\sum_{i=1}^n(\hat{y}_i - \bar{y})(y_i - \hat{y}_i) + \sum_{i=1}^n(y_i - \hat{y}_i)^2 
$$ {#eq-ss-expand}

For now, let's focus on the middle term $2\sum_{i=1}^n(\hat{y}_i - \bar{y})(y_i - \hat{y}_i)$

$$
\begin{aligned}
2\sum_{i=1}^n(\hat{y}_i - \bar{y})(y_i - \hat{y}_i) &= 2\sum_{i=1}^n(\hat{y}_iy_i - \hat{y}_i^2 - \bar{y}y_i + \bar{y}\hat{y}_i)\\[10pt]
& = 2\sum_{i=1}^n\hat{y}_i(y_i - \hat{y}_i) - 2\bar{y}\sum_{i=1}^n(y_i - \hat{y}_i) \\[10pt]
&=2\sum_{i=1}^n\hat{y}_ie_i - 2\bar{y}\sum_{i=1}^ne_i\\[10pt]
&= 0 \quad(\text{ given }\sum_{i=1}^ne_i = 0)
\end{aligned}
$$ {#eq-ss-middle-term}

Plugging @eq-ss-middle-term back into @eq-ss-expand, we have

$$
\begin{aligned}
\sum_{i=1}^n(y_i - \bar{y})^2 &= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 + 0 + \sum_{i=1}^n(y_i - \hat{y}_i)^2 \\[10pt]
&= \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 +  \sum_{i=1}^n(y_i - \hat{y}_i)^2\\[10pt]
\end{aligned}
$$

Thus $SST = SSM + SSR$
-->
</section><section id="sec-mlr-matrix" class="level2" data-number="A.2"><h2 data-number="A.2" class="anchored" data-anchor-id="sec-mlr-matrix">
<span class="header-section-number">A.2</span> Matrix representation of linear regression</h2>
<p>The matrix representation for linear regression introduced in this section will be used for the remainder of this appendix and in <a href="appendix-logistic.html" class="quarto-xref"><span>Appendix B</span></a>. We will provide some linear algebra and matrix algebra details throughout, but we assume understanding of basic linear algebra concepts. Please see Chapter 1 <a href="https://www-bcf.usc.edu/~gareth/ISL/"><em>An Introduction to Statistical Learning</em></a> and online resources for an in-depth introduction to linear algebra.</p>
<p>Suppose we have a data set with <span class="math inline">\(n\)</span> observations. The <span class="math inline">\(i^{th}\)</span> observation is represented as <span class="math inline">\((x_{i1}, \ldots, x_{ip}, y_i)\)</span>, such that <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the predictor variables and <span class="math inline">\(y_i\)</span> is the response variable. We assume the data can be modeled using the least-squares regression model of the form in <a href="#eq-basic-model" class="quarto-xref">Equation&nbsp;<span>A.11</span></a> (see <a href="08-mlr-inference.html" class="quarto-xref"><span>Chapter 8</span></a> for more detail).</p>
<p><span id="eq-basic-model"><span class="math display">\[y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon \hspace{8mm} \epsilon \sim N(0, \sigma^2_{\epsilon}) \tag{A.11}\]</span></span></p>
<p>The regression model in <a href="#eq-basic-model" class="quarto-xref">Equation&nbsp;<span>A.11</span></a> can be represented using vectors and matrices.</p>
<p><span id="eq-linear-matrix"><span class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \hspace{8mm} \boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2_{\epsilon}\mathbf{I}) \tag{A.12}\]</span></span></p>
<p>Let’s break down the components of <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a>.</p>
<p><span id="eq-linear-matrix-expand"><span class="math display">\[
\underbrace{
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} }_
{\mathbf{y}} \hspace{3mm}
=
\hspace{3mm}
\underbrace{
\begin{bmatrix}
1 &amp;x_{11} &amp; \dots &amp; x_{1p}\\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
1 &amp;  x_{n1} &amp; \dots &amp;x_{np}
\end{bmatrix}
}_{\mathbf{X}}
\hspace{2mm}
\underbrace{
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{bmatrix}
}_{\boldsymbol{\beta}}
\hspace{3mm}
+
\hspace{3mm}
\underbrace{
\begin{bmatrix}
\epsilon_1 \\
\vdots\\
\epsilon_n
\end{bmatrix}
}_{\boldsymbol{\epsilon}}
\tag{A.13}\]</span></span></p>
<p>From <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a> and <a href="#eq-linear-matrix-expand" class="quarto-xref">Equation&nbsp;<span>A.13</span></a>, we have the following components of the linear regression model in matrix form:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{y}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of the observed responses.</li>
<li>
<span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times (p + 1)\)</span> matrix called the <strong>design matrix</strong>. The first column is always <span class="math inline">\(\mathbf{1}\)</span>, a column vector of 1’s, that corresponds to the intercept. The remaining columns contain the observed values of the predictor variables.</li>
<li>
<span class="math inline">\(\boldsymbol{\beta}\)</span> is a <span class="math inline">\((p+1) \times 1\)</span> vector of the model coefficients.</li>
<li>
<span class="math inline">\(\boldsymbol{\epsilon}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of the error terms.</li>
</ul>
<p>As before the error terms are normally distributed, centered at <span class="math inline">\(\mathbf{0}\)</span>, a column vector of 0’s, with a variance <span class="math inline">\(\sigma^2_{\epsilon}\mathbf{I}\)</span>, where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix.</p>
<div class="math_rules">
<p>The variance of the error terms <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is <span class="math display">\[\sigma^2_{\epsilon}\mathbf{I} = \sigma^2_{\epsilon} \begin{bmatrix}
1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 1 &amp;\dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; 1 \\
\end{bmatrix} =
\begin{bmatrix}
\sigma^2_{\epsilon} &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_{\epsilon} &amp;\dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_{\epsilon} \\
\end{bmatrix}\]</span></p>
<p>This is the matrix notation showing that the error terms are independent and have the same variance <span class="math inline">\(\sigma^2_{\epsilon}\)</span>.</p>
</div>
<p>Based on <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a>, the equation for the vector of estimated response values, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, is</p>
<p><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta}
\]</span></p>
</section><section id="sec-mlr-estimation-matrix" class="level2" data-number="A.3"><h2 data-number="A.3" class="anchored" data-anchor-id="sec-mlr-estimation-matrix">
<span class="header-section-number">A.3</span> Estimating the Coefficients</h2>
<section id="sec-least-sq-matrix" class="level3" data-number="A.3.1"><h3 data-number="A.3.1" class="anchored" data-anchor-id="sec-least-sq-matrix">
<span class="header-section-number">A.3.1</span> Least-squares estimation</h3>
<p>In matrix notation, the error terms can be written as <span id="eq-errors-matrix"><span class="math display">\[
\boldsymbol{\epsilon} = \mathbf{y} - \mathbf{X}\boldsymbol{\beta}
\tag{A.14}\]</span></span></p>
<p>As with simple linear regression in <a href="#sec-least-sq-math" class="quarto-xref"><span>Section A.1</span></a>, the least-squares estimator of the vector of coefficients, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, is the vector that minimizes the sum of squared residuals in <a href="#eq-sum-sq-matrix" class="quarto-xref">Equation&nbsp;<span>A.15</span></a>. <span id="eq-sum-sq-matrix"><span class="math display">\[
SSR = \sum\limits_{i=1}^{n} \epsilon_{i}^2 = \boldsymbol{\epsilon}^\mathsf{T}\boldsymbol{\epsilon} = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})
\tag{A.15}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{\epsilon}^\mathsf{T}\)</span>, the transpose of the vector <span class="math inline">\(\boldsymbol{\epsilon}\)</span>.</p>
<p>Let’s walk through the steps to minimize <a href="#eq-sum-sq-matrix" class="quarto-xref">Equation&nbsp;<span>A.15</span></a>. We start by expanding the equation.</p>
<p><span id="eq-ssr-matrix-expand"><span class="math display">\[
\begin{aligned}
SSR &amp;= (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})\\[10pt]
&amp; = (\mathbf{Y}^\mathsf{T}\mathbf{Y} - \mathbf{Y}^\mathsf{T} \mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{Y} +\boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta})
\end{aligned}
\tag{A.16}\]</span></span></p>
<p>Note that <span class="math inline">\((\mathbf{Y^\mathsf{T}}\mathbf{X}\boldsymbol{\beta})^\mathsf{T} = \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{Y}\)</span>. These are both constants (i.e.&nbsp;<span class="math inline">\(1\times 1\)</span> vectors), so we have<span class="math inline">\(\mathbf{Y^\mathsf{T}}\mathbf{X}\boldsymbol{\beta} = (\mathbf{Y^\mathsf{T}}\mathbf{X}\boldsymbol{\beta})^\mathsf{T}=  \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{Y}\)</span>. Plugging this equality into <a href="#eq-ssr-matrix-expand" class="quarto-xref">Equation&nbsp;<span>A.16</span></a>, we have</p>
<p><span id="eq-ssr-matrix-expand-2"><span class="math display">\[
SSR = \mathbf{Y}^\mathsf{T}\mathbf{Y} - 2 \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{Y} + \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}
\tag{A.17}\]</span></span></p>
<p>Next, we find the partial derivative of <a href="#eq-ssr-matrix-expand-2" class="quarto-xref">Equation&nbsp;<span>A.17</span></a> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<div id="matrix-calculus" class="math_rules">
<p>Let <span class="math inline">\(\mathbf{x} = \begin{bmatrix}x_1 &amp; x_2 &amp; \dots &amp; x_p\end{bmatrix}^\mathsf{T}\)</span>be a <span class="math inline">\(k \times 1\)</span> vector and <span class="math inline">\(f(\mathbf{x})\)</span> be a function of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Then <span class="math inline">\(\nabla_\mathbf{x}f\)</span>, the <strong>gradient</strong> of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[\nabla_\mathbf{x}f = \begin{bmatrix}\frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_2} &amp; \dots &amp; \frac{\partial f}{\partial x_p}\end{bmatrix}^\mathsf{T}
\]</span></p>
<p><br></p>
<p><strong>Property 1</strong></p>
<p>Let <span class="math inline">\(\mathbf{x}\)</span> be a <span class="math inline">\(k \times 1\)</span> vector and <span class="math inline">\(\mathbf{z}\)</span> be a <span class="math inline">\(k \times 1\)</span> vector, such that <span class="math inline">\(\mathbf{z}\)</span> is not a function of <span class="math inline">\(\mathbf{x}\)</span> . The gradient of <span class="math inline">\(\mathbf{x}^\mathsf{T}\mathbf{z}\)</span> with respect to <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[\nabla_\mathbf{x} \hspace{1mm} \mathbf{x}^\mathsf{T}\mathbf{z} = \mathbf{z}\]</span></p>
<p><br></p>
<p><strong>Property 2</strong></p>
<p>Let <span class="math inline">\(\mathbf{x}\)</span> be a <span class="math inline">\(k \times 1\)</span> vector and <span class="math inline">\(\mathbf{A}\)</span> be a <span class="math inline">\(k \times k\)</span> matrix, such that <span class="math inline">\(\mathbf{A}\)</span> is not a function of <span class="math inline">\(\mathbf{x}\)</span> . The gradient of <span class="math inline">\(\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x}\)</span> with respect to <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[
\nabla_\mathbf{x} \hspace{1mm} \mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} = (\mathbf{A}\mathbf{x} + \mathbf{A}^\mathsf{T} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\mathsf{T})\mathbf{x}
\]</span>If <span class="math inline">\(\mathbf{A}\)</span> is symmetric, then <span class="math inline">\((\mathbf{A} + \mathbf{A}^\mathsf{T})\mathbf{x} = 2\mathbf{A}\mathbf{x}\)</span></p>
</div>
<p>Using the matrix calculus, the partial derivative of <a href="#eq-ssr-matrix-expand-2" class="quarto-xref">Equation&nbsp;<span>A.17</span></a> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> is</p>
<p><span id="eq-ssr-deriv"><span class="math display">\[
\begin{aligned}
\frac{\partial SSR}{\partial \boldsymbol{\beta}} &amp;= \frac{\partial}{\partial \boldsymbol\beta}(\mathbf{Y}^\mathsf{T}\mathbf{Y} - 2\boldsymbol{\beta}^\mathsf{T} \mathbf{X}^\mathsf{T}\mathbf{Y} + \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta})\\[10pt]
&amp; = -2\mathbf{X}^\mathsf{T}\mathbf{Y} + 2\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}
\end{aligned}
\tag{A.18}\]</span></span></p>
<p>Note that <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> is symmetric.</p>
<p>Thus, the least-squares estimator is the <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that satisfies</p>
<p><span id="eq-ssr-deriv-2"><span class="math display">\[
-2\mathbf{X}^\mathsf{T}\mathbf{Y} + 2\mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{0}
\tag{A.19}\]</span></span></p>
<p>The steps to find this <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> are below. <span id="eq-ssr-deriv-3"><span class="math display">\[
\begin{aligned}
&amp;- 2 \mathbf{X}^\mathsf{T}\mathbf{Y} + 2 \mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} = 0 \\[10pt]
&amp; \Rightarrow 2 \mathbf{X}^\mathsf{T}\mathbf{Y} = 2 \mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]&amp; \Rightarrow \mathbf{X}^\mathsf{T}\mathbf{Y} = \mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]&amp; \Rightarrow (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{Y} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]&amp; \Rightarrow (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{Y} = \hat{\boldsymbol{\beta}}\end{aligned}
\tag{A.20}\]</span></span><!--# is it correct to call it the second derivative here?--></p>
<p>Similar to <a href="#sec-least-sq-math" class="quarto-xref"><span>Section A.1</span></a>, we check the second derivative to confirm that we have found a minimum. In matrix representation, the second derivative is the <strong>Hessian matrix</strong>.</p>
<div class="math_rules">
<p>The <strong>Hessian</strong> <strong>matrix</strong>, <span class="math inline">\(\nabla_\mathbf{x}^2f\)</span> is a <span class="math inline">\(k \times k\)</span> matrix of partial second derivatives</p>
<p><span class="math display">\[
\nabla_{\mathbf{x}}^2f = \begin{bmatrix} \frac{\partial^2f}{\partial x_1^2} &amp; \frac{\partial^2f}{\partial x_1 \partial x_2} &amp; \dots &amp; \frac{\partial^2f}{\partial x_1\partial x_p} \\
\frac{\partial^2f}{\partial\ x_2 \partial x_1} &amp; \frac{\partial^2f}{\partial x_2^2} &amp; \dots &amp; \frac{\partial^2f}{\partial x_2 \partial x_p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2f}{\partial x_p\partial x_1} &amp; \frac{\partial^2f}{\partial x_p\partial x_2} &amp; \dots &amp; \frac{\partial^2f}{\partial x_p^2} \end{bmatrix}
\]</span></p>
<p>If the Hessian matrix is…</p>
<ul>
<li><p>positive definite, then we found a minimum.</p></li>
<li><p>negative definitive, then we found a maximum.</p></li>
<li><p>neither, then we found a saddle point.</p></li>
</ul>
</div>
<p>Thus, the Hessian of <a href="#eq-sum-sq-matrix" class="quarto-xref">Equation&nbsp;<span>A.15</span></a> is</p>
<p><span id="eq-sum-sq-hessian"><span class="math display">\[
\begin{aligned}
\frac{\partial^2 SSR}{\partial \boldsymbol{\beta}^2} &amp;= \frac{\partial}{\partial \boldsymbol{\beta}}(-2\mathbf{X}^\mathsf{T}\mathbf{Y} + 2\mathbf{X}^\mathbf{T}\mathbf{X}\boldsymbol{\beta}) \\[10pt]
&amp; = 2\mathbf{X}^\mathsf{T}\mathbf{X}
\end{aligned}
\tag{A.21}\]</span></span></p>
<p><a href="#eq-sum-sq-hessian" class="quarto-xref">Equation&nbsp;<span>A.21</span></a> is proportional to <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span>, which is a positive definite matrix. Therefore, we found a minimum.</p>
<div class="results">
<p>The least-squares estimator in matrix notation is</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{Y}
\]</span></p>
</div>
</section><section id="sec-geometry" class="level3" data-number="A.3.2"><h3 data-number="A.3.2" class="anchored" data-anchor-id="sec-geometry">
<span class="header-section-number">A.3.2</span> Geometry of least-squares regression</h3>
<p>In <a href="#sec-least-sq-matrix" class="quarto-xref"><span>Section A.3.1</span></a>, we used matrix calculus to find <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, the least-squares estimators of the model coefficients. In this section, we present the geometry of least-squares regression to derive the estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. <a href="#fig-geometric-interpretation" class="quarto-xref">Figure&nbsp;<span>A.1</span></a> is a visualization of the geometric representation of least-squares regression.</p>
<div id="fig-geometric-interpretation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-geometric-interpretation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/appendix-regression-geometry.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;A.1: Geometry of least-squares regression"><img src="images/appendix-regression-geometry.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-geometric-interpretation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;A.1: Geometry of least-squares regression
</figcaption></figure>
</div>
<p>Let <span class="math inline">\(\text{Col}(\mathbf{X})\)</span> be the <strong>column space</strong> of the design matrix <span class="math inline">\(\mathbf{X}\)</span>, the set all possible linear combinations of the columns of <span class="math inline">\(\mathbf{X}\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We cannot derive the values of <span class="math inline">\(\mathbf{y}\)</span> using only linear combinations of <span class="math inline">\(\mathbf{X}\)</span> (recall the error term <span class="math inline">\(\boldsymbol{\epsilon}\)</span> in <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a>). Therefore, <span class="math inline">\(\mathbf{y}\)</span> is not in <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>. We want to find another vector <span class="math inline">\(\mathbf{Xb}\)</span> in <span class="math inline">\(\text{Col}(\mathbf{X})\)</span> that minimizes the distance to <span class="math inline">\(\mathbf{y}\)</span>. We call the vector <span class="math inline">\(\mathbf{Xb}\)</span> a <strong>projection</strong> of <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>.</p>
<p>For any vector <span class="math inline">\(\mathbf{Xb}\)</span> in <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>, the vector <span class="math inline">\(\mathbf{e} = \mathbf{y} - \mathbf{Xb}\)</span> is the difference between <span class="math inline">\(\mathbf{y}\)</span> and the projection <span class="math inline">\(\mathbf{Xb}\)</span>. We know <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> from the data, so we need to find <span class="math inline">\(\mathbf{b}\)</span> that minimizes the length of <span class="math inline">\(\mathbf{e} = \mathbf{y} - \mathbf{Xb}\)</span>. It is minimized when <span class="math inline">\(\mathbf{e}\)</span> is <strong>orthogonal</strong> (also called <em>normal</em>) to <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>. <!--# is that the correct way to use the word "normal"?--></p>
<div class="math_rules">
<p>If <span class="math inline">\(\mathbf{A}\)</span>, an <span class="math inline">\(n \times k\)</span> matrix, is orthogonal to an <span class="math inline">\(n \times 1\)</span> vector <span class="math inline">\(\mathbf{c}\)</span>, then <span class="math inline">\(\mathbf{A}^\mathsf{T}\mathbf{c} = \mathbf{0}\)</span></p>
</div>
<p>Because <span class="math inline">\(\mathbf{e}\)</span> is orthogonal to <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>, then <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{e} = \mathbf{0}\)</span>. We plug in <span class="math inline">\(\mathbf{e} = \mathbf{y} - \mathbf{Xb}\)</span> and solve for <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p><span id="eq-geometric-est"><span class="math display">\[
\begin{aligned}
&amp;\mathbf{X}^\mathsf{T}(\mathbf{y} - \mathbf{Xb}) = \mathbf{0}\\[10pt]
\Rightarrow &amp;\mathbf{X}^\mathsf{T}\mathbf{y} - \mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{b} = 0 \\[10pt]
\Rightarrow &amp;\mathbf{X}^\mathsf{T}\mathbf{y} = \mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{b}\\[10pt]
\Rightarrow &amp;(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y} = \mathbf{b}
\end{aligned}
\tag{A.22}\]</span></span></p>
<p>Using the geometric interpretation of least-squares regression, we found that the vector <span class="math inline">\(\mathbf{b}\)</span> that minimizes <span class="math inline">\(\mathbf{e} = \mathbf{y} - \mathbf{Xb}\)</span> is <span class="math inline">\(\hat{\mathbf{b}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}\)</span>. This is equal to the estimator found in <a href="#eq-ssr-deriv-3" class="quarto-xref">Equation&nbsp;<span>A.20</span></a>.</p>
</section></section><section id="sec-hat-matrix" class="level2" data-number="A.4"><h2 data-number="A.4" class="anchored" data-anchor-id="sec-hat-matrix">
<span class="header-section-number">A.4</span> Hat matrix</h2>
<p>The fitted values of least-squares regression are <span class="math inline">\(\mathbf{y} = \mathbf{X}\hat{\boldsymbol{\beta}}\)</span>. Plugging in <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> from <a href="#eq-ssr-deriv-3" class="quarto-xref">Equation&nbsp;<span>A.20</span></a>, we have</p>
<p><span id="eq-hat-matrix"><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \underbrace{\mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}}_{\mathbf{H}}\mathbf{y}
\tag{A.23}\]</span></span></p>
<p>From <a href="#eq-hat-matrix" class="quarto-xref">Equation&nbsp;<span>A.23</span></a>, <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\)</span> is called the <strong>hat matrix</strong>. The hat matrix is an <span class="math inline">\(n \times n\)</span> matrix that maps <span class="math inline">\(\mathbf{y}\)</span>, the vector of observed responses, onto <span class="math inline">\(\hat{\mathbf{y}}\)</span>, the vector of fitted values (<span class="math inline">\(\hat{\mathbf{y}} = \mathbf{Hy}\)</span>). More precisely, <span class="math inline">\(\mathbf{H}\)</span> is a <strong>projection matrix</strong> that projects <span class="math inline">\(\mathbf{y}\)</span> onto the column space <span class="math inline">\(\text{Col}(\mathbf{X})\)</span> (see <a href="#sec-geometry" class="quarto-xref"><span>Section A.3.2</span></a>). Because <span class="math inline">\(\mathbf{H}\)</span> is a projection matrix, it has the following properties:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{H}\)</span> is symmetric (<span class="math inline">\(\mathbf{H}^\mathsf{T} = \mathbf{H}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{H}\)</span> is idempotent (<span class="math inline">\(\mathbf{H}^2 = \mathbf{H}\)</span>).</p></li>
<li><p>If a vector <span class="math inline">\(\mathbf{v}\)</span> is in <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>, then <span class="math inline">\(\mathbf{Hv}  = \mathbf{v}\)</span>.</p></li>
<li><p>If a vector <span class="math inline">\(\mathbf{v}\)</span> is orthogonal to <span class="math inline">\(\text{Col}(\mathbf{X})\)</span>, then <span class="math inline">\(\mathbf{Hv}= \mathbf{0}\)</span>.</p></li>
</ul>
<p>From <a href="#eq-hat-matrix" class="quarto-xref">Equation&nbsp;<span>A.23</span></a>, the hat matrix only depends on the design matrix <span class="math inline">\(\mathbf{X}\)</span>, i.e., it only depends on the values of the predictors. It does not depend on the vector of responses. The diagonal elements <span class="math inline">\(\mathbf{H}\)</span> are the values of <strong>leverage</strong>. More specifically, <span class="math inline">\(h_{ii}\)</span> is the leverage for the <span class="math inline">\(i^{th}\)</span> observation. Recall from <a href="06-slr-conditions.html#sec-slr-leverage" class="quarto-xref"><span>Section 6.4.2</span></a> that in simple linear regression, the leverage is a measure of how far an observation’s value of the predictor is from the average value of the predictor across all observations.</p>
<p><span id="eq-leverage-slr"><span class="math display">\[
h_{ii} =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}
\tag{A.24}\]</span></span></p>
<p>In multiple linear regression, the leverage is a measure of how far the <span class="math inline">\(i^{th}\)</span> observation is from the center of the <span class="math inline">\(x\)</span> space. <!--# do i need to explain this further? Is this the correct notation for x or should it be bold?--> It is computed as</p>
<p><span id="eq-leverage-mlr"><span class="math display">\[
h_{ii} = \mathbf{x}_i^\mathsf{T}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{x}_i
\tag{A.25}\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{x}_i^\mathsf{T}\)</span> is the <span class="math inline">\(i^{th}\)</span> row of the design matrix <span class="math inline">\(\mathbf{X}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The sum of the leverages for all observation <span class="math inline">\(p+1\)</span> where <span class="math inline">\(p\)</span> is the number of predictors in the model. Thus, <span class="math inline">\(\text{rank}(\mathbf{H}) = \sum_{i=1}^n h_{ii} = p+1\)</span>. The average value of leverage is <span class="math inline">\(\bar{h} = \frac{p+1}{n}\)</span>. Observations with leverage greater than <span class="math inline">\(2\bar{h}\)</span> are considered to have large leverage. <!--# am i using "high" or "large"--></p>
<p>Using <span class="math inline">\(\mathbf{H}\)</span>, we can write the residuals as</p>
<p><span id="eq-residuals-hat"><span class="math display">\[
\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{Hy} = (\mathbf{I} - \mathbf{H})\mathbf{y}
\tag{A.26}\]</span></span></p>
<p><a href="#eq-residuals-hat" class="quarto-xref">Equation&nbsp;<span>A.26</span></a> shows one feature of observations with large leverage. Observations with large leverage tend to have small residuals. In other words, the model tends to pull towards observations with large leverage.</p>
</section><section id="sec-assumptions-matrix" class="level2" data-number="A.5"><h2 data-number="A.5" class="anchored" data-anchor-id="sec-assumptions-matrix">
<span class="header-section-number">A.5</span> Assumptions of linear regression</h2>
<p>In <a href="05-slr-inference.html#sec-slr-foundation" class="quarto-xref"><span>Section 5.3</span></a>, we introduced four assumptions of linear regression. Given the matrix form of the linear regression on model in <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a>, <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> such that <span class="math inline">\(\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2_{\epsilon}\mathbf{I})\)</span> , these assumptions are the following:</p>
<ol type="1">
<li>The distribution of the response <span class="math inline">\(\mathbf{y}\)</span> given <span class="math inline">\(\mathbf{X}\)</span> is normal.</li>
<li>The expected value of <span class="math inline">\(\mathbf{y}\)</span> is <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>. There is a linear relationship between the response and predictor variables.</li>
<li>The variance <span class="math inline">\(\mathbf{y}\)</span> given <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(\sigma^2_{\epsilon}\mathbf{I}\)</span>.</li>
<li>The error terms in <span class="math inline">\(\boldsymbol{\epsilon}\)</span> are independent. This also means the observations are independent of one another.</li>
</ol>
<p>From these assumptions, we write the distribution of <span class="math inline">\(\mathbf{y}\)</span> given the regression model in <a href="#eq-dist-y-matrix" class="quarto-xref">Equation&nbsp;<span>A.27</span></a>.</p>
<p><span id="eq-dist-y-matrix"><span class="math display">\[
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2_{\epsilon}\mathbf{I})
\tag{A.27}\]</span></span></p>
<p>In <a href="#sec-mlr-matrix" class="quarto-xref"><span>Section A.2</span></a>, we showed Assumption 4 from the distribution of the error terms. Here we will show Assumptions 1 - 3.</p>
<div class="math_rules">
<p>Suppose <span class="math inline">\(\mathbf{z}\)</span> is a (multivariate) normal random variable such that <span class="math inline">\(\mathbf{z} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>. A linear transformation of <span class="math inline">\(\mathbf{z}\)</span> is also multivariate normal, such that</p>
<p><span class="math display">\[
\mathbf{A}\mathbf{z} + \mathbf{B} \sim N(\mathbf{A}\boldsymbol{\mu} + \mathbf{B}, \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^\mathsf{T})
\]</span></p>
</div>
<p>The distribution of the error terms <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is normal, and the vector of responses <span class="math inline">\(\mathbf{y}\)</span> is linear combination of the error terms. More specifically, <span class="math inline">\(\mathbf{y}\)</span> is computed as the error terms, shifted by <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>. Thus, from the math property above, we know that <span class="math inline">\(\mathbf{y}\)</span> is normally distributed.</p>
<div class="math_rules">
<p><strong>Expected value of a vector</strong></p>
<p>Let <span class="math inline">\(\mathbf{z} = \begin{bmatrix}z_1 &amp; \dots &amp; z_p\end{bmatrix}^\mathsf{T}\)</span> be a <span class="math inline">\(p \times 1\)</span> vector of random variables. Then <span class="math inline">\(E(\mathbf{z}) = E\begin{bmatrix}z_1 &amp; \dots &amp; z_p\end{bmatrix}^\mathsf{T} = \begin{bmatrix}E(z_1) &amp; \dots &amp; E(z_p)\end{bmatrix}^\mathsf{T}\)</span></p>
<p><br></p>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(n \times p\)</span> matrix of constants, <span class="math inline">\(\mathbf{C}\)</span> a <span class="math inline">\(n \times 1\)</span> vector of random variables, and <span class="math inline">\(\mathbf{z}\)</span> a <span class="math inline">\(p \times 1\)</span> vector of random variables. Then</p>
<p><span class="math display">\[
E(\mathbf{Az} + \mathbf{C}) = \mathbf{A}E(\mathbf{z}) +E(\mathbf{C})
\]</span></p>
</div>
<p>Next, let’s show Assumption 2 that the expected value of <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta}\)</span> in linear regression. We can get the expected value of <span class="math inline">\(\mathbf{y}\)</span> from the mathematical properties above used for Assumption 1. Here, we show Assumption 2 directly from the properties of expected values.</p>
<p><span id="eq-exp-y-matrix"><span class="math display">\[
\begin{aligned}
E(\mathbf{y}) &amp;= E(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) \\[10pt]
&amp; = E(\mathbf{X}\boldsymbol{\beta}) + E(\boldsymbol{\epsilon})\\[10pt]
&amp; = \mathbf{X}\boldsymbol{\beta} + \mathbf{0} \\[10pt]
&amp; = \mathbf{X}\boldsymbol{\beta}
\end{aligned}
\tag{A.28}\]</span></span></p>
<div class="math_rules">
<p>Let <span class="math inline">\(\mathbf{z}\)</span> be a <span class="math inline">\(p \times 1\)</span> vector of random variables. Then</p>
<p><span class="math display">\[
Var(\mathbf{z}) = E[(\mathbf{z} - E(\mathbf{z}))(\mathbf{z} - E(\mathbf{z}))^\mathsf{T}]
\]</span></p>
</div>
<p>Lastly, we show Assumption 3 that <span class="math inline">\(Var(\mathbf{y}) = \sigma^2_{\epsilon}\mathbf{I}\)</span> in linear regression. Similar to the expected value, we can get the variance from the mathematical property used to show Assumption 1. Here, we show Assumption 3 directly from the properties of variance. <span id="eq-var-y-matrix"><span class="math display">\[
\begin{aligned}
Var(\mathbf{y}) &amp;= E[(\mathbf{y} - E(\mathbf{y}))(\mathbf{y} - E(\mathbf{y}))^\mathsf{T}] \\[10pt]
&amp; = E[(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} - \mathbf{X}\boldsymbol{\beta})(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}] \\[10pt]
&amp; = E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathsf{T}] \\[10pt]
(\text{given }E(\boldsymbol{\epsilon} = \mathbf{0})) &amp; = E[(\boldsymbol{\epsilon} - E(\boldsymbol{\epsilon}))(\boldsymbol{\epsilon} - E(\boldsymbol{\epsilon}))^\mathsf{T}]\\[10pt]
&amp; = Var(\boldsymbol{\epsilon}) \\[10pt]
&amp; = \sigma^2_{\epsilon}\mathbf{I}
\end{aligned}
\tag{A.29}\]</span></span></p>
</section><section id="sec-beta-dist-matrix" class="level2" data-number="A.6"><h2 data-number="A.6" class="anchored" data-anchor-id="sec-beta-dist-matrix">
<span class="header-section-number">A.6</span> Distribution of model coefficients</h2>
<p>In <a href="08-mlr-inference.html#sec-mlr-inf-foundation" class="quarto-xref"><span>Section 8.4.1</span></a>, we introduced the distribution of a model coefficient <span class="math inline">\(\hat{\beta}_j \sim N(\beta_j, SE_{\hat{\beta}_j}^2)\)</span>. <!--# make sure I"m using the variance for the distribution of the coefficient in the inference chapter--> In matrix notation, the distribution of all the estimated coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is</p>
<p><span id="eq-dist-beta-matrix"><span class="math display">\[
\hat{\boldsymbol{\beta}} \sim N\Big(\boldsymbol{\beta}, \sigma^2_{\epsilon}(\mathbf{X}^\mathsf{T},\mathbf{X})^{-1}\Big)
\tag{A.30}\]</span></span></p>
<p>Similar to <a href="#sec-assumptions-matrix" class="quarto-xref"><span>Section A.5</span></a>, let’s derive each part of this distribution. We’ll start with <span class="math inline">\(E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}\)</span>.</p>
<p><span id="eq-beta-hat-expected-matrix"><span class="math display">\[
\begin{aligned}
E(\hat{\boldsymbol{\beta}}) &amp;= E((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}) \\[10pt]
&amp; = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}E(\mathbf{y})\\[10pt]
&amp; = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}E(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon})
\\[10pt]
&amp;=(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}E(\mathbf{X}\boldsymbol{\beta}) + (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}E(\boldsymbol{\epsilon}) \\[10pt]
&amp; = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} + \mathbf{0}\\[10pt]
&amp;= \boldsymbol{\beta}
\end{aligned}
\tag{A.31}\]</span></span></p>
<p>Next, we show that <span class="math inline">\(Var(\hat{\boldsymbol{\beta}}) = \sigma^2_{\epsilon}\mathbf{I}\)</span>.</p>
<p><span id="eq-var-beta-matrix"><span class="math display">\[
\begin{aligned}
Var(\hat{\boldsymbol{\beta}}) &amp;= E[(\hat{\boldsymbol{\beta}} - E(\hat{\boldsymbol{\beta}}))(\hat{\boldsymbol{\beta}} - E(\hat{\boldsymbol{\beta}}))^\mathsf{T}] \\[10pt]
&amp; = E[((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y} - \boldsymbol{\beta})((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y} - \boldsymbol{\beta})^\mathsf{T}] \\[10pt]
&amp; = E[((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) - \boldsymbol{\beta})((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) - \boldsymbol{\beta})^\mathsf{T}] \\[10pt]
&amp;\dots \text{After distributing and simplifying}\dots \\[10pt]
&amp; = E[(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathsf{T}\mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}] \\[10pt]
&amp; = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\underbrace{E(\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathsf{T})}_{\sigma^2_{\epsilon}\mathbf{I}}\mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\\[10pt]
&amp;= \sigma^2\mathbf{I}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\\[10pt]
&amp; = \sigma^2_{\epsilon}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}
\end{aligned}
\tag{A.32}\]</span></span></p>
<p>Lastly, we show that the distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> is normal. We’ll start by rewriting <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> in terms of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p><span id="eq-rewrite-beta-matrix"><span class="math display">\[
\begin{aligned}
\hat{\boldsymbol{\beta}} &amp;= (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y} \\[10pt]
&amp; = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) \\
&amp; = \boldsymbol{\beta} + (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\boldsymbol{\epsilon}
\end{aligned}
\tag{A.33}\]</span></span></p>
<p>From <a href="#eq-rewrite-beta-matrix" class="quarto-xref">Equation&nbsp;<span>A.33</span></a>, we see that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is a linear combination of the error terms <span class="math inline">\(\boldsymbol{\epsilon}\)</span>. From the math property in <a href="#sec-assumptions-matrix" class="quarto-xref"><span>Section A.5</span></a>, because <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is normally distributed, the linear combination is also normally distributed. Thus, the distribution of the coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is normal. <!--# make sure this is correct reasoning.--></p>
</section><section id="sec-multicollinearity-matrix" class="level2" data-number="A.7"><h2 data-number="A.7" class="anchored" data-anchor-id="sec-multicollinearity-matrix">
<span class="header-section-number">A.7</span> Multicollinearity</h2>
<p>Recall the design matrix for a linear regression model with <span class="math inline">\(p\)</span> predictors in <a href="#eq-linear-matrix-expand" class="quarto-xref">Equation&nbsp;<span>A.13</span></a></p>
<p><span id="eq-design-matrix"><span class="math display">\[
\mathbf{X} =
\begin{bmatrix}
1 &amp;x_{11} &amp; \dots &amp; x_{1p}\\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
1 &amp;  x_{n1} &amp; \dots &amp;x_{np}
\end{bmatrix}
\tag{A.34}\]</span></span></p>
<p>The design matrix <span class="math inline">\(\mathbf{X}\)</span> has full column rank <span class="math inline">\(\text{rank}(\mathbf{X}) = (p + 1)\)</span> in linear regression. This means there are no linear dependencies among the columns, and thus no column is a perfect linear combination of the others. <!--# am I using linear dependency correct here?--> The equation for the least-squares estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> in <a href="#eq-ssr-deriv-3" class="quarto-xref">Equation&nbsp;<span>A.20</span></a> includes the term <span class="math inline">\((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\)</span>. If <span class="math inline">\(\mathbf{X}\)</span> is not full rank, then <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> is not full rank, and is therefore singular (not invertible). Thus, if there are linear dependencies in <span class="math inline">\(\mathbf{X}\)</span>, we are unable to compute an the least-squares estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<div class="math_rules">
<p>Let <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_p\)</span> be a sequence of vectors. Then, <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_p\)</span> are <strong>linearly dependent</strong><em>,</em> if there exists scalars <span class="math inline">\(a_1, a_2, \ldots, a_p\)</span> such that</p>
<p><span class="math display">\[
a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + \dots + a_p\mathbf{x}_p = \mathbf{0}
\]</span></p>
<p>where <span class="math inline">\(a_1, a_2, \ldots, a_p\)</span> are not all 0.</p>
</div>
<p>In practice, we rarely have perfect linear dependencies. In fact, this is mathematically why we only include <span class="math inline">\(k-1\)</span> terms in the model for a categorical predictor with <span class="math inline">\(k\)</span> levels. Ideally the columns of <span class="math inline">\(\mathbf{X}\)</span> would be orthogonal, indicating the predictors are completely independent on one another. In practice, we expect there to be some dependence between predictors (we see this from the non-zero off diagonals in <span class="math inline">\(Var(\hat{\boldsymbol{\beta}})\)</span>). If two or more variables are highly correlated, then there will be near linear dependence in the columns. This near-linear dependence is called <strong>multicollinearity.</strong></p>
<p>In <a href="08-mlr-inference.html#sec-mlr-multicollinearity" class="quarto-xref"><span>Section 8.6</span></a> we discussed the practical issues that come from the presence of multicollinearity. These primarily stem from the fact that when there is multicollinearity, <span class="math inline">\(\mathbf{X}^\mathsf{T}\mathbf{X}\)</span> is near-singular (almost a singular matrix) to a, thus making <span class="math inline">\((\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\)</span> very large and unstable. Consequently, <span class="math inline">\(Var(\hat{\boldsymbol{\beta}})\)</span> is then large and unstable, thus making hard to find stable values for the least-squares estimators.</p>
</section><section id="sec-appendix-linear-mle" class="level2" data-number="A.8"><h2 data-number="A.8" class="anchored" data-anchor-id="sec-appendix-linear-mle">
<span class="header-section-number">A.8</span> Maximum Likelihood Estimation</h2>
<p>In <a href="10-model-eval.html#sec-aic-bic" class="quarto-xref"><span>Section 10.2.3</span></a>, we introduced the likelihood function to understand the model performance statistics AIC and BIC. We also used a likelihood function to estimate the coefficients in logistic regression (more on this in <a href="appendix-logistic.html" class="quarto-xref"><span>Appendix B</span></a>). The <strong>likelihood function</strong> is a measure of how likely it is we observe our data given particular value(s) of model parameters. When working with likelihood functions, we have fixed data (our observed sample data) and we can try out different values for the model parameters (<span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2_{\epsilon}\)</span> in the context of regression).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="math_rules">
<p>Let <span class="math inline">\(\mathbf{z}\)</span> be a <span class="math inline">\(p \times 1\)</span> vector of random variables, such that <span class="math inline">\(\mathbf{z}\)</span> follows a <strong>multivariate normal distribution</strong> with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and variance <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then the probability density function of <span class="math inline">\(\mathbf{z}\)</span> is</p>
<p><span class="math display">\[f(\mathbf{z}) = \frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\Big\{-\frac{1}{2}(\mathbf{z} - \boldsymbol{\mu})^\mathsf{T}\boldsymbol{\Sigma}^{-1}(\mathbf{z}- \boldsymbol{\mu})\Big\}\]</span></p>
</div>
<p>In <a href="#sec-assumptions-matrix" class="quarto-xref"><span>Section A.5</span></a>, we showed that the vector of responses <span class="math inline">\(\mathbf{y}\)</span> can be written as <span class="math inline">\(\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2_{\epsilon}\mathbf{I})\)</span>. Using this distribution to describe the relationship between the response and predictor variables, the likelihood function for the regression parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2_{\epsilon}\)</span> is</p>
<p><span id="eq-likelihood-matrix"><span class="math display">\[
L(\boldsymbol{\beta}, \sigma^2_{\epsilon} | \mathbf{X}, \mathbf{y}) = \frac{1}{(2\pi)^{n/2}\sigma^n_{\epsilon}}\exp\Big\{-\frac{1}{2\sigma^2_{\epsilon}}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\Big\}
\tag{A.35}\]</span></span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations, the mean is <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> and the variance is <span class="math inline">\(\sigma^2_{\epsilon}\mathbf{I}\)</span>.</p>
<p>The data <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are fixed (we use the same sample data in the analysis!), so we can plug in values for <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2_{\epsilon}\)</span> to determine the likelihood of obtaining those values for the parameters given the observed data. In <a href="#sec-least-sq-matrix" class="quarto-xref"><span>Section A.3.1</span></a>, we used least-squares estimation (minimizing <span class="math inline">\(SSR\)</span> ) find the estimated coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Another approach to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> (and <span class="math inline">\(\sigma^2_{\epsilon}\)</span>) is to find <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> (and <span class="math inline">\(\hat{\sigma}^2_{\epsilon}\)</span>) that maximizes the likelihood function in <a href="#eq-likelihood-matrix" class="quarto-xref">Equation&nbsp;<span>A.35</span></a>. This is called <strong>maximum likelihood estimation</strong>.</p>
<p>To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e.&nbsp;the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function is</p>
<p><span id="eq-log-likelihood-matrix"><span class="math display">\[
\begin{aligned}\log L(\boldsymbol{\beta}, \sigma^2_\epsilon &amp;| \mathbf{X}, \mathbf{y}) \\ &amp; = -\frac{n}{2}\log(2\pi) - n \log(\sigma_{\epsilon}) - \frac{1}{2\sigma^2_{\epsilon}}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\end{aligned}
\tag{A.36}\]</span></span></p>
<p>Given a fixed value of <span class="math inline">\(\sigma^2_{\epsilon}\)</span>, the log-likelihood in <a href="#eq-log-likelihood-matrix" class="quarto-xref">Equation&nbsp;<span>A.36</span></a> is maximized when <span class="math inline">\((\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\)</span> is minimized. This is equivalent to minimizing the <span class="math inline">\(SSR\)</span> in <a href="#eq-sum-sq-matrix" class="quarto-xref">Equation&nbsp;<span>A.15</span></a>. Thus, the least-squares estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is also the maximum likelihood estimator ( <span class="math inline">\(\hat{\beta} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}\)</span>) when the error terms are defined as in <a href="#eq-linear-matrix" class="quarto-xref">Equation&nbsp;<span>A.12</span></a>.</p>
<p>We previously found <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using least-squares estimation and the geometry of regression in <a href="#sec-least-sq-matrix" class="quarto-xref"><span>Section A.3.1</span></a>, so why does it matter that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is also the maximum likelihood estimator? First, maximum likelihood estimation is used to find the coefficients for generalized linear models and many other statistical models that go beyond linear regression (we’ll see how its used for logistic regression in <a href="appendix-logistic.html" class="quarto-xref"><span>Appendix B</span></a>). In fact, <span class="citation" data-cites="casella2024statistical">Casella and Berger (<a href="references.html#ref-casella2024statistical" role="doc-biblioref">2024</a>)</span> stated maximum likelihood estimation is “by far, the most popular technique for deriving estimators.” [pp.&nbsp;315]. <!--# check page number in the 2nd edition--> Second, maximum likelihood estimators have nice statistical properties. These properties are beyond the scope of this text, but we refer interested readers to Chapter 7 of <span class="citation" data-cites="casella2024statistical">Casella and Berger (<a href="references.html#ref-casella2024statistical" role="doc-biblioref">2024</a>)</span> for an in-depth discussion of maximum likelihood estimators and their properties.</p>
</section><section id="sec-var-transformations-math" class="level2" data-number="A.9"><h2 data-number="A.9" class="anchored" data-anchor-id="sec-var-transformations-math">
<span class="header-section-number">A.9</span> Variable transformations</h2>
<p>In <a href="09-mlr-transformations.html" class="quarto-xref"><span>Chapter 9</span></a>, we introduced regression models with transformations on the response and/or predictor variables. Here we will share some of the mathematical details behind the interpretation of the coefficients in these models.</p>
<section id="sec-transform-y-math" class="level3" data-number="A.9.1"><h3 data-number="A.9.1" class="anchored" data-anchor-id="sec-transform-y-math">
<span class="header-section-number">A.9.1</span> Transformation on the response variable</h3>
<p>In <a href="09-mlr-transformations.html#sec-transform-logy" class="quarto-xref"><span>Section 9.2</span></a>, we introduced a linear regression model with a log transformation on the response variable</p>
<p><span id="eq-model-logy"><span class="math display">\[
\log(Y) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p + \epsilon
\tag{A.37}\]</span></span></p>
<div class="math_rules">
<p>In this text, <span class="math inline">\(\log(a)\)</span> is the natural log of <span class="math inline">\(a\)</span> (also denoted as <span class="math inline">\(\ln(a)\)</span>).</p>
<ul>
<li><p><span class="math inline">\(\log(a) + \log(b) = \log(ab)\)</span>.</p></li>
<li><p><span class="math inline">\(\log(a) - \log(b) = \log(\frac{a}{b})\)</span></p></li>
<li><p><span class="math inline">\(e^{\log(a)} = a\)</span></p></li>
<li><p><span class="math inline">\(e^{b\log(a)} = a^b\)</span></p></li>
</ul>
</div>
<p>From <a href="07-mlr.html" class="quarto-xref"><span>Chapter 7</span></a>, we have that the change in <span class="math inline">\(\log(Y)\)</span> when <span class="math inline">\(X_j\)</span> increase by 1 unit is <span class="math inline">\(\beta_j\)</span>. Thus <span class="math inline">\(\beta_j = \log(Y|{X_j+1}) - \log(Y|X_j)\)</span>, where <span class="math inline">\(Y|X_{j}+1\)</span> is the value of <span class="math inline">\(Y\)</span> when we plug in <span class="math inline">\(X_j+1\)</span> and <span class="math inline">\(Y|{X_j}\)</span> is the value of <span class="math inline">\(Y\)</span> when we plug in <span class="math inline">\(X_j\)</span>. In practice, we interpret the model coefficients <span class="math inline">\(\beta_j\)</span> in terms of the original variable <span class="math inline">\(Y\)</span>, so we can use the rules of logarithms and exponents to rewrite this in terms of the original units of the response variable.</p>
<p><span class="math display">\[
\begin{aligned}
\beta_j &amp;= \log(Y|X_j+1) - \log(Y|X_j) \\[10pt]
&amp; = \log\Big(\frac{Y|X_j+1}{Y|X_j}\Big) \\[10pt]
\Rightarrow \beta_j \times Y|X_j &amp;= Y|X_j+1
\end{aligned}
\]</span></p>
<p>Thus given the model in <a href="#eq-model-logy" class="quarto-xref">Equation&nbsp;<span>A.37</span></a>, when <span class="math inline">\(X_j\)</span> increases by 1 unit, we expect <span class="math inline">\(Y\)</span> to multiply by <span class="math inline">\(\beta_j\)</span>, assuming all other predictors are held constant.</p>
</section><section id="sec-transform-x-math" class="level3" data-number="A.9.2"><h3 data-number="A.9.2" class="anchored" data-anchor-id="sec-transform-x-math">
<span class="header-section-number">A.9.2</span> Transformation on predictor variable(s)</h3>
<p>Next, we consider the models introduced in <a href="09-mlr-transformations.html#sec-transform-logx" class="quarto-xref"><span>Section 9.3</span></a> that have a log transformation on a predictor variable.</p>
<p><span id="eq-model-logx"><span class="math display">\[
Y = \beta_0 + \beta_1X_1 + \dots + \beta_j\log(X_j) + \dots + \beta_pX_p + \epsilon
\tag{A.38}\]</span></span></p>
<p>Now, we because the predictor <span class="math inline">\(X_j\)</span> has been transformed to the logarithmic scale, we write interpretations in terms of a multiplicative change in <span class="math inline">\(X_j\)</span>. More specifically, given a constant <span class="math inline">\(C\)</span>, <span class="math inline">\(\log(CX_j) = \log(C) + \log(X_j)\)</span>. Let <span class="math inline">\(Y|CX_j\)</span> be the expected value of <span class="math inline">\(Y\)</span> when we plug in <span class="math inline">\(CX_j\)</span> and <span class="math inline">\(Y|X_j\)</span> be the expected value of <span class="math inline">\(Y\)</span> when we plug in <span class="math inline">\(X_j\)</span>. Assuming all other predictors held constant, we have</p>
<p><span class="math display">\[
\begin{aligned}
Y|CX_j - Y|CX &amp;= \beta_j\log(CX_j) - \beta_j\log(X_j) \\
&amp; = \beta_j[\log(CX_j) - \log(X_j) \\
&amp; = \beta_j[\log(C) + \log(X_j) - \log(X_j)] \\
&amp;= \beta_j\log(C) \\[10pt]
\Rightarrow Y|CX_j &amp;=Y|X_j + \beta_j\log(C)
\end{aligned}
\]</span></p>
<p>Thus, given the model in <a href="#eq-model-logx" class="quarto-xref">Equation&nbsp;<span>A.38</span></a>, when predictor <span class="math inline">\(X_j\)</span> is multiplied by a constant <span class="math inline">\(C\)</span>, we expect <span class="math inline">\(Y\)</span> to change (increase or decrease) by <span class="math inline">\(\beta_j\log(C)\)</span>, holding all other predictors constant.</p>
</section><section id="sec-transform-x-y-math" class="level3" data-number="A.9.3"><h3 data-number="A.9.3" class="anchored" data-anchor-id="sec-transform-x-y-math">
<span class="header-section-number">A.9.3</span> Transformation on response and predictor variables</h3>
<p>Lastly, we show the mathematics behind the interpretation of a model coefficient <span class="math inline">\(\beta_j\)</span> for the linear regression models introduced in <a href="09-mlr-transformations.html#sec-transform-logx-logy" class="quarto-xref"><span>Section 9.4</span></a> with a log transformation on the response variable and a predictor variable. As in <a href="#sec-transform-y-math" class="quarto-xref"><span>Section A.9.1</span></a>, we want to write the interpretation in terms of the original response variable <span class="math inline">\(Y\)</span>.</p>
<p><span id="eq-model-log-xy"><span class="math display">\[
\log(Y) = \beta_0  + \beta_1X_1 + \dots + \beta_j\log(X_j) + \dots + \beta_pX_p
\tag{A.39}\]</span></span></p>
<p>Combining the results from <a href="#sec-transform-y-math" class="quarto-xref"><span>Section A.9.1</span></a> and <a href="#sec-transform-x-math" class="quarto-xref"><span>Section A.9.2</span></a> and holding all other predictors constant, we have</p>
<p><span id="eq-log-xy-math"><span class="math display">\[
\begin{aligned}
\log(Y|CX_j) -\log(Y|X_j) &amp;= \beta_j\log(C)\\[10pt]
\log\Big(\frac{Y|CX_j}{Y|X_j}\Big)  &amp;= \beta_j\log(C) \\[10pt]
\frac{Y|CX_j}{Y|X_j}  &amp;= C^{\beta_j} \\[10pt]
\Rightarrow Y|CX_j &amp;= Y|X_jC^{\beta_j}
\end{aligned}
\tag{A.40}\]</span></span></p>
<p>Therefore, given the model in <a href="#eq-model-log-xy" class="quarto-xref">Equation&nbsp;<span>A.39</span></a> with a transformed response variable and transformed predictor, when <span class="math inline">\(X_j\)</span> is multiplied by a constant <span class="math inline">\(C\)</span>, why is expected to multiply by <span class="math inline">\(C^{\beta_j}\)</span>, holding all other predictors constant.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-casella2024statistical" class="csl-entry" role="listitem">
Casella, George, and Roger Berger. 2024. <em>Statistical Inference</em>. 2nd ed. CRC Press.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>This is the <em>span</em> of <span class="math inline">\(\mathbf{X}\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that when there is a single predictor, <a href="#eq-leverage-slr" class="quarto-xref">Equation&nbsp;<span>A.24</span></a> and <a href="#eq-leverage-mlr" class="quarto-xref">Equation&nbsp;<span>A.25</span></a> produce the same result.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Note that the likelihood function is <u>not</u> the same as a probability function. In a probability function, we fix the model parameters and plug in different values for the data.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/introregressio\.netlify\.app");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./13-special-topics.html" class="pagination-link" aria-label="Special topics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Special topics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./appendix-logistic.html" class="pagination-link" aria-label="Mathematics of logistic regression">
        <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Mathematics of logistic regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/matackett/regression-book/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>