[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Regression Analysis",
    "section": "",
    "text": "Welcome\nWelcome to forthcoming textbook Introduction to Regression Analysis: A Data Science Approach from CRC Press.\n\n\n\n\n\n\nCautionWork in progress\n\n\n\nThis book is a work in progress. If you have questions, comments, or feedback, please email Maria Tackett at maria.tackett@duke.edu.\n\n\n\nLicense\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Audience\nStatistical models  have become an increasingly prominent part of everyday life. For years, algorithms have provided recommendations on almost anything ranging from the music we will like or products we want to purchase. Algorithms have also been used in more consequential decision-making, such as identifying bank accounts for potential fraud or informing the length of sentencing in the United States court system.  More recently, artificial intelligence, in particular Large Language Models, has changed how we approach work, school, and everyday tasks. Given the broad impact statistical models have on modern life, it is important that the people who develop these models and the people who will use models to inform decision-making understand how these models work and the implications of decisions made during the model development process.\nThis book is an introduction to the world of modeling, with a focus on linear and logistic regression models. These models are widely used in industry and academia, and they provide a foundation for many models commonly used in machine learning and data science. The ideas presented in this book around how we approach model building, interpret results, and use models to inform decision making nicely extend to advanced modeling techniques. We’ll see a glimpse of this in Chapter 13  Special topics. Overall, this book provides an in-depth  introduction to modeling that prepares the reader to use regression models in practice and to study advanced modeling techniques.\nThe following are the key points we aim for readers to learn about regression as they read the text.\nThis book is intended for readers who have had an introduction to data science or statistics and are seeking a more in-depth study of regression analysis. It aims to equip readers with knowledge and skills to use regression analysis in practice in academia or industry. It is primarily written to serve as a textbook for an applied regression analysis course but can also be used for self-study for readers interested in a robust understanding of regression analysis for their work or research.\nWe assume the reader has taken an introductory level statistics or data science course or is familiar with the topics found in texts such as Introduction to Modern Statistics (Çetinkaya-Rundel and Hardin 2024) or Statistical Inference via Data Science: A ModernDive into R and the tidyverse (Ismay and Kim 2019). Throughout the book, we will briefly review some introductory topics, as needed. We refer readers to these and similar texts for a more in-depth introduction to statistics and data science.\nWe also assume readers interested in the computing aspects of this book have some familiarity with R and the tidyverse. 2  Data analysis in R provides a computing review that can also serve as a brief introduction. We refer readers to R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) for a resource on computing using the tidyverse.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#structure",
    "href": "00-preface.html#structure",
    "title": "Preface",
    "section": "Structure",
    "text": "Structure\nThe book is divided into four parts: Getting started, Simple linear regression, Multiple linear regression, and Beyond linear regression.\n\nPart 1: Getting started - This part introduces foundational concepts that are  utilized throughout the book. Chapter 1  Regression in the data science workflow is an introduction to regression analysis and how it fits in the data science landscape. It also introduces a data science workflow that is used in the assignments in the supplemental materials. Chapter 2  Data analysis in R provides a review (or brief introduction) to R and the tidyverse. It focuses on many of the data manipulation and data analysis functions that are used in the text. It also introduces Quarto, a system for technical documents that is used in the assignments. Chapter 3  Exploratory data analysis introduces exploratory data analysis and it is used to understand distributions of individual variables and the relationships between multiple variables. It also discusses strategies for cleaning data and handling unusual features in the data.\nPart 2: Simple linear regression - This part covers the details of regression in the context of simple linear regression. The concepts introduced in this section extend to the multiple linear regression models introduced in Part 3. Chapter 4  Simple linear regression introduces the simple linear regression model, estimating and interpreting model coefficients, prediction, and model evaluation. Chapter 5  Inference for simple linear regression introduces simulation-based and theory-based inference for model coefficients. Chapter 6  Model conditions and diagnostics discusses model conditions and diagnostics.\nPart 3: Multiple linear regression - This part extends the concepts from Part 2 to multiple linear regression models with two or more predictors. Chapter 7  Multiple linear regression introduces the multiple linear regression model, estimating and interpreting model coefficients, working with different types of predictors, interaction effects, and prediction. Chapter 8  Inference for multiple linear regression introduces simulation-based and theory-based inference for coefficients in a multiple linear regression model, along with model conditions and diagnostics. Chapter 9  Variable transformations introduces models with transformations on the response and/or predictor variables. Chapter 10  Model selection covers model assessment, model selection, and cross validation.\nPart 4: Beyond linear regression - This part introduces models for data that do not meet the conditions for linear regression. Chapter 11  Logistic regression introduces logistic regression for binary response variables. It discusses estimating and interpreting model coefficients, simulation-based and theory-based inference for model coefficients, model conditions, and model diagnostics. Chapter 12  Logistic regression: Prediction and evaluation covers prediction and model evaluation for logistic regression. Chapter 13  Special topics is an introduction to a collection of models that are extensions of linear and logistic regression. These models include multinomial logistic regression, random intercepts models, decision trees, and models for causal inference.\n\nThe book also includes appendices covering the mathematics underlying linear and logistic regression. These appendices utilize the matrix representation of the models, so they are intended for readers with some familiarity with linear algebra.\nUsing this book for a course\nThe content and structure of this book are based on undergraduate regression analysis courses that have been taught by the author at Duke University. Each course follows a 15-week semester.\nApplied Regression Analysis: Undergraduate regression analysis course focused on application. Pre-requisites are introductory statistics or introductory probability.\n\n\nChapter 1  Regression in the data science workflow, Chapter 2  Data analysis in R, Chapter 3  Exploratory data analysis (2 weeks)\n\n\nChapter 2  Data analysis in R primarily covered in a computing lab or through review assignments.\n\n\n\nChapter 4  Simple linear regression, Chapter 5  Inference for simple linear regression, Chapter 6  Model conditions and diagnostics (2 weeks)\n\nChapter 7  Multiple linear regression (1 week)\n\nChapter 8  Inference for multiple linear regression (2 weeks)\n\nChapter 9  Variable transformations (1 week)\n\nChapter 10  Model selection (2 weeks)\n\nChapter 11  Logistic regression (2 weeks)\n\nChapter 12  Logistic regression: Prediction and evaluation (1 week)\n\nChapter 13  Special topics (1 week)\n\nTypically cover one topic from this chapter\n\n\n\nRegression Analysis with Theory: Undergraduate regression analysis course focused on application and mathematical theory. Pre-requisites are introductory statistics or introductory probability and linear algebra.\n\n\nChapter 1  Regression in the data science workflow, Chapter 2  Data analysis in R, Chapter 3  Exploratory data analysis (1 week)\n\n\nChapter 2  Data analysis in R primarily covered in a computing lab or review assignments.\n\n\n\nChapter 4  Simple linear regression (1 week)\n\nChapter Matrix representation of linear regression, Chapter Estimating the Coefficients (1 week)\n\nIntroduced in context of simple linear regression.\n\n\n\nChapter 7  Multiple linear regression (1 week)\n\nChapter 5  Inference for simple linear regression, Chapter 8  Inference for multiple linear regression, Chapter Distribution of model coefficients (2 weeks)\n\nChapter 6  Model conditions and diagnostics, Chapter Model conditions and diagnostics (1 week)\n\nChapter Hat matrix, Chapter Multicollinearity (1 week)\n\nChapter 9  Variable transformations, Chapter Variable transformations (1 week)\n\nChapter Maximum Likelihood Estimation (1 week)\n\nChapter 11  Logistic regression (2 weeks)\n\nChapter 12  Logistic regression: Prediction and evaluation (1 week)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#key-features",
    "href": "00-preface.html#key-features",
    "title": "Preface",
    "section": "Key features",
    "text": "Key features\nBeginning with 3  Exploratory data analysis, the chapters are written as case studies based on a real-world data and stated analysis objective. The case studies in this book show the variety of contexts in which regression analysis can be applied. The chapters begin with an introduction to the data and exploratory data analysis that is focused on the variables and relationships relevant to the chapter. The data sets used in each chapter are available in Appendix C — Data sets. An analysis objective anchors the chapter and readers are walked through the analysis process as they learn and apply new concepts.\nThere are call out boxes throughout the book to help guide the reading experience.\n\nAnalysis objective: Highlights the main analysis question we seek to answer using the methods introduced in the chapter.\nAnalysis in practice: Provides practical tips and insights about using regression analysis in work and research.\nMath details: Includes concepts and mathematical facts that are used for computations or derivations.\nYour turn: Encourages the reader to check understanding using practice questions on the concepts introduced in the chapter. Answers (or example responses) are posted as footnotes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#computing",
    "href": "00-preface.html#computing",
    "title": "Preface",
    "section": "Computing",
    "text": "Computing\nComputing is an important and necessary aspect of conducting regression analysis in practice. Therefore, R output and code are included in each chapter. The code is the book primarily follows the tidyverse (Wickham et al. 2019) and tidymodels (Kuhn and Wickham 2020) syntax; they are introduced in Chapter 2  Data analysis in R.\nInspired by the structure of An Introduction to Statistical Learning (James et al. 2021), each chapter contains a section about the code used to apply the new concepts. This format is intended to help readers focus on conceptual understanding before diving into the code. Additionally, it is intended to make the text more accessible to readers not using R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#supplemental-materials",
    "href": "00-preface.html#supplemental-materials",
    "title": "Preface",
    "section": "Supplemental materials",
    "text": "Supplemental materials\nThe companion website (https://introregression-resources.netlify.app) contains homework assignments, computing labs, and other resources that accompany the text. These are based on a workflow using R and Quarto and have been used in undergraduate regression analysis courses at Duke University. Many of these assignments have also been adapted and used by instructors at other institutions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#about-the-author",
    "href": "00-preface.html#about-the-author",
    "title": "Preface",
    "section": "About the author",
    "text": "About the author",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#acknowledgements",
    "href": "00-preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\nÇetinkaya-Rundel, Mine, and Johanna Hardin. 2024. Introduction to Modern Statistics. 2nd ed. OpenIntro.\n\n\nIsmay, Chester, and Albert Y Kim. 2019. Statistical Inference via Data Science: A Moderndive into r and the Tidyverse.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.” https://www.tidymodels.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse” 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Regression in the data science workflow",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-goals",
    "href": "01-intro.html#learning-goals",
    "title": "1  Regression in the data science workflow",
    "section": "",
    "text": "Define regression analysis and how it is used to answer questions about real-world phenomena\nDescribe the data science workflow\nDefine reproducibility and how it is implemented in the data science workflow",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-regression-analysis",
    "href": "01-intro.html#sec-regression-analysis",
    "title": "1  Regression in the data science workflow",
    "section": "\n1.1 Regression analysis",
    "text": "1.1 Regression analysis\nWe use data to gain insights and knowledge about real-world phenomena. Often, there is a population we are interested in studying (e.g., all adults in the United States), but we do not have access to data on the full population. Or we want to make predictions for what is expected to happen in the population in the future. In either case, we analyze data about a sample (subset) drawn from the population to gain insights or make predictions about the population. The sample is made of the observations in the data obtained in the collect stage of Figure 1.3.\nOne way this is done, in general, is through statistical models. Statistical models are a broad array of mathematical models that are used to describe how data are generated in the population (also called data generating models). This text will primarily focus on a class of statistical models called regression analysis.\n\n\nPopulation: Entire group being studied\nSample: Subset of the population that is used in the analysis\nStatistical model: Mathematical models used to describe how data are generated in a population\n\n\n\n1.1.1 What is regression analysis?\nRegression analysis (also called regression models) is a set of statistical models that describe multivariable relationships, the relationships between two or more variables. In particular, they describe the relationship between a response variable and one or more predictor variables. The response variable (also called outcome or dependent variable) is the outcome of interest. In other words, it is the variable about which we wish to understand variability or predict. The predictor variables (also called explanatory variables or independent variables) are the variables used to explain variability or predict the response variable. We denote the response variables \\(Y\\) and the predictor variables as \\(X_1, X_2, \\ldots, X_p\\), where \\(p\\) is the number of predictors.\n\n\nRegression analysis: Set of statistical models to describe multivariable relationships between a response and one or more predictor variables\nResponse variable: Variable whose variability we wish to understand or we wish to predict\nPredictor variable(s): Variable(s) used to explain variability in the response or to predict new values of the response variable\n\n\nIn regression analysis, we assume the values of the response variable be generated by the following process\n\\[\nY = \\text{Model} + \\text{Error}\n\\tag{1.1}\\]\nMore specifically, the model is a function of the predictor variables, \\(\\text{Model} = f(X_1, X_2, \\ldots, X_p)\\) . We input some values of the predictor variables and the output is \\(f(X_1, X_2, \\ldots, X_p)\\). The Error, then, is the difference between the observed value \\(Y\\) and the output from \\(f(X_1, X_2, \\ldots, X_p)\\). Much of regression is choosing the \\(\\text{Model}\\) and using the \\(\\text{Error}\\) to evaluate the model’s performance and how well it explains the relationships in the data.\nThere are two types of regression models that differ on how they define \\(f(X_1, X_2, \\ldots, X_p)\\). Breiman (2001) referred to these as the “two cultures” of statistical modeling.\nIn parametric regression models, we assume the model function takes a specific form and then use techniques to estimate the parameters of the model. For example, we may assume the model takes the form\n\\[\nf(X_1, X_2, \\ldots, X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\tag{1.2}\\]\nwhere \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the model coefficients. Thus, the full form of the model from Equation 1.1 is\n\\[\n\\begin{aligned}\n&Y = f(X_1, X_2, \\ldots, X_p) + \\epsilon \\\\[10pt]\n\\Rightarrow &Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\n\\end{aligned}\n\\tag{1.3}\\]\nwhere \\(\\epsilon\\) is the error.\nIn parametric regression, we use regression methods to fit the model, find estimated values for these model coefficients. We then input these estimated coefficients into Equation 1.2 to produce predicted values of the response variable, denoted \\(\\hat{Y}\\).\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + \\dots + \\hat{\\beta}_pX_p\n\\]\nWhen we fit models using parametric methods, we make inherent assumptions about the underlying form of the relationships in the data. In Chapter 6, we show ways to check a set of model conditions to evaluate whether these assumptions are actually true in the data. One advantage to parametric models is that we can use them to interpret the relationship between the response and predictor variables. In Chapter 4 and Chapter 7, we will talk extensively about how we might interpret the estimated coefficients, and how for example, we can use this equation to predict values of life expectancy. For now, the goal is to see that when using parametric methods, we derive a mathematical equation of the relationship between the response variable and predictor variables then estimate the coefficients in that equation.\nThe vast majority of this book focuses on parametric models, specifically linear regression (models for quantitative response variables) and logistic regression (models for categorical response variables).\nThe second type of regression model is non-parametric regression models. These models differ from parametric models in that there is no assumed structure of the function between the response and predictor variables. The function \\(f(X_1, X_2, \\ldots, X_p)\\) is defined by finding a way to closely model the trend in the sample data without fitting it so closely that it couldn’t apply to new data. The advantage to this approach is that we do not impose an assumed structure on the relationship between the response and predictor variables. The primary disadvantage, however, is that these models are often uninterpretable, making it difficult to describe the relationships in the data.\n\n\nParametric regression: Methods in which the model structure is predefined and the data are used to estimate coefficients of the model\nNon-parametric regression: Methods in which there is no predefined structure for the model, and the model is defined by getting close (to a point) to the observations\nFit a model: Use data to estimate a model\n\n\n\n1.1.2 Why fit a regression model?\nIn general, there are two primary purposes for regression analysis: inference and prediction. Inference is the process of drawing conclusion and insights from the model. Prediction is the process of computing the expected value of the response variable based on values of the predictor variable(s).\nBoth tasks are often performed in a single analysis. It is generally beneficial to be able to use a model to generate both predictions and insights from the data. The analysis questions from plan stage of the data science workflow (Figure 1.3) are used to determine the primary purpose of an analysis. As we will see throughout the book, many analysis decisions are made based on whether the analysis is focused on inference or prediction (or both).\n\n\nInference: Process of drawing conclusions and insights from a model\nPrediction: Process of computing the expected value of the response variable based on value(s) of the predictor variable(s)\n\n\nHere, we are interested in models that are intperpretable and are useful for gaining insights about the data. Even as evaluate models with prediction as the primary objective, we still take the interpretability into account. Therefore, we will often balance predictive accuracy with ease of interpretation. This is often important for models that are used in research, as the goal is to use data to understand a particular phenomena. In other contexts, such as industry for example, the importance of predictive accuracy may far exceed the that of interpretability. In this case, non-parametric models, such as those commonly used in machine learning, may be preferred over regression methods. We will discuss a set of non-parametric models in ?sec-cecision-trees and refer readers to Introduction to Statistical Learning (James et al. 2021) for a more in-depth introduction to non-parametric models.\n\n1.1.3 Example: Country’s life expectancy\nWe apply what we’ve discussed about regression models thus far to an example motivated by Zarulli et al. (2021), who studied the relationship between life expectancy and a country’s healthcare system. Life expectancy is how long an individual is expected to live on the day they are born based on factors about their surrounding environment.  There are large differences in life expectancy around the world, so it is important to understand the factors that impact life expectancy in order to help improve outcomes for individuals in countries with lower life expectancy.\nIn this section, we will use data from Zarulli et al. (2021) to examine the relationship between life expectancy, education, and income inequality. The data were originally obtained by researchers from the Human Development Database (https://hdr.undp.org/data-center) and the World Health Organization (https://apps.who.int/nha/database).\nThe data set is available in life-expectancy.csv. It contains information about life expectancy, health-care related factors, and other societal factors for 140 countries. We will use the following variables in this section. The definitions are from the respective original data sources.\n\nlife_exp: The average number of years that a newborn could expect to live, if he or she were to pass through life exposed to the sex- and age-specific death rates prevailing at the time of his or her birth, for a specific year, in a given country, territory, or geographic area. (from the World Health Organization)\nincome_inequality: Measure of the deviation of the distribution of income among individuals or households within a country from a perfectly equal distribution. A value of 0 represents absolute equality, a value of 100 absolute inequality (Gini coefficient). (from the Human Development Database)\n\neducation: Indicator of whether a country’s education index is above (High) or below (Low) the median index for the 140 countries in the data set.\n\nEducation index: Average of mean years of schooling (of adults) and expected years of school (of children), both expressed as an index obtained by scaling wit the corresponding maxima. (from the Human Development Database)\n\n\n\nSee Zarulli et al. (2021) for a full codebook.\nFigure 1.1 shows the distributions and Table 1.1 shows the summary statistics for life_expectancy and income_inequality. These are used to better understand the data, a process called exploratory data analysis (EDA). EDA is introduced more fully in Chapter 3.\n\n\n\n\n\n\n\n\n\n(a) Life expectancy\n\n\n\n\n\n\n\n\n\n(b) Income inequality\n\n\n\n\n\n\nFigure 1.1: Distributions of variables life expectancy analysis\n\n\n\n\n\nTable 1.1: Summary statistics of life expectancy and income inequality”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\nlife_exp\n71.6\n8.1\n51.6\n65.4\n72.8\n78.0\n84.1\n0\n\n\nincome_inequality\n19.8\n9.7\n5.4\n11.5\n18.1\n28.3\n44.2\n0\n\n\n\n\n\n\n\n\n\nWhat do you observe about the distributions of life_exp and income_inequality based on Figure 1.1 and Table 1.1?1\n\nWe know the distribution of education based on its definition. Fifty percent of the observations are categorized as Low and 50% are categorized as High.\nSuppose we wish to fit a regression model to predict a country’s life expectancy using income inequality and education. In parametric regression, we can specify the form of the model as\n\\[\n\\begin{aligned}\n\\text{life\\_exp} &= f(\\text{income\\_inequality},\\text{education}) + \\epsilon \\\\[10pt]\n& =  \\beta_0 + \\beta_1~\\text{income\\_inequality} + \\beta_2~\\text{education} + \\epsilon\n\\end{aligned}\n\\tag{1.4}\\]\n\nIn Section 1.1.2, we introduced the two primary purposes of regression analysis, prediction and inference.\n\nWhat is an example a prediction question that can be answered from Equation 1.4?\nWhat is an example of an inference question that can be answered from Equation 1.4?2\n\n\n\nWe use the data to fit the model and find estimates for \\(\\beta_0, \\beta_1, \\text{ and }\\beta_2\\). Equation 1.5 is the equation of the estimated model, and Figure 1.2 is a visual representation of this model. We discuss the process of choosing the model form, estimating the coefficients, and interpreting the model beginning in Chapter 4.\n\\[\n\\widehat{\\text{life\\_exp}} = 81.096 -0.562 \\times \\text{income\\_inequality} + 2.386 \\times \\text{education}\n\\tag{1.5}\\]\n\n\n\n\n\n\n\nFigure 1.2: Visual representation of model estimated regression model",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-regression-in-ds-workflow",
    "href": "01-intro.html#sec-regression-in-ds-workflow",
    "title": "1  Regression in the data science workflow",
    "section": "\n1.2 Regression in a data science workflow",
    "text": "1.2 Regression in a data science workflow\nRegression is typically one step in a larger data science workflow. In this section, we will zoom out so we can see where regression fits in the context of this workflow.\n\n1.2.1 Data science approach\nData science is a relatively new and rapidly evolving discipline, and there is no single definition of what data science is and how it compares to related disciplines. This book approaches regression through the lens of data science, so let’s take a moment to explore how others have defined data science and use it to describe what it means to use take a “data science approach” to regression.\nIn the 1962 paper The Future of Data Analysis, statistician John Tukey describes the field of data analysis, which many argue is actually a description of what we call data science today.  He says data analysis includes\n\n“procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.” (Tukey 1962, 2)\n\nHe later on goes to argue that data analysis is, in fact, a science, because it contains the three characteristics of a science\n\nintellectual content\norganization into an understandable form,\n\nreliance upon the test of experience as the ultimate standard of validity. (Tukey 1962, 5)\n\n\nThe way we think about and work with data has changed dramatically since Tukey’s 1962 definition of “data analysis”. Today, data are ubiquitous and have become integrated into every day life, and very large data sets (called “big data”) have become increasingly common. Additionally, the “machinery” that Tukey refers to has become much more sophisticated and widely accessible, vastly increasing the data analysis capabilities available to researchers, industry professionals, and learners alike. \nGiven these differences in today’s data landscape, let’s look at some more recent definitions of data science.\nWikipedia defines data science as\n\n“an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.” (Wikipedia contributors 2025)\n\nIn Modern Data Science with R, Baumer, Kaplan, and Horton (2017) define data science as “science of extracting meaningful information from data”. In R for Data Science, Wickham, Çetinkaya-Rundel, and Grolemund (2023) say it is “an exciting discipline that allows you to transform raw data into understanding, insight, and knowledge.” In Data Science: A First Introduction, Timbers, Campbell, and Lee (2022) define data science as the “process of generating insight from data through reproducible and auditable processes”. In Telling Stories with Data, Alexander (2023) concludes that data science is\n\n“the process of developing and applying a principled, tested, reproducible, end-to-end workflow that focuses on quantitative measures in and of themselves, and as a foundation to explore questions”\n\nLastly, Alby (2023) emphasizes the domain knowledge in their definition of data science in the book Data Science in Practice,\n\n“Data science is currently still a consolidating application area of artificial intelligence in which interdisciplinary knowledge from statistics, computer science, and expert knowledge from other disciplines are converged to further develop methods for unlocking data patterns in these fields.” (pg. 17)\n\n\nThese definitions tend to emphasize different aspects of data science; however, there are common themes across all of them. Taking these definitions into account, as we introduce regression analysis using a “data science approach”, we are conducting regression such that the following hold:\n\nWe apply knowledge, skills, and tools from statistics and computer science.\nWe use domain knowledge to interpret results and glean useful insights from data.\nWe conduct analysis using a reproducible workflow.\n\nPillars  (1) and (2) are illustrated throughout the book as we walk through case study examples in each chapter. Pillar (3) is best demonstrated as these methods are put into practice, so we introduce the concept o f a reproducible workflow in Section 1.2.3, discuss tools for conducting reproducible analyses in Section 2.7, and practice implementing it as part of the assignments in the supplemental materials. \n\nData science approach to regression\nWe conduct regression analysis such that the following hold:\n\nWe apply knowledge, skills, and tools from statistics and computer science.\nWe use domain knowledge to interpret results and glean useful insights from data.\nWe conduct analysis using a reproducible workflow.\n\n\n\n1.2.2 Data science workflow\nThe focus of this book, regression analysis, is one part of a much larger data science workflow. It is important to understand what happens before and after regression modeling, because each step in the workflow informs the others.\n\n\n\n\n\nFigure 1.3: Data science workflow3\n\n\n\nFigure 1.3 is the general data science workflow. As illustrated in Figure 1.3, the process begins before any data analysis. The first step is plan, which starts with clearly defining the analysis (or research) question. In practice, this step is often done in collaboration with subject matter experts (e.g., business partners or research collaborators). The analysis question informs decisions made at each step of the workflow, so it is important to take the time to clearly define it at this step.  Additionally, given the vast amounts of data available, having a clear analysis question helps us focus and navigate through what could otherwise be an overwhelming amount of data. \nOnce the analysis question has been defined, the next step is collect. This is the point in which we collect the data that will be analyzed to answer the analysis question. Data scientists are often data consumers, individuals who work with data that has already been collected (Gebru et al. 2021), so in practice, this stage is focused on obtaining the correct sample data from an existing data set or data base. It is important for the data scientist understand the data provenance, a record of the origins of a data set along with any processing or changes it has undergone since its original creation. Additionally, it is important to review the codebook, documentation of the variables in the data set and their definitions. As we learn more about the data and what the variables measure, we may need to refine the analysis question posed in the previous step to one that can be feasibly answered from the available data. Having an understanding about the contents and history of the data also provides important context as we interpret analysis results.4\nNow it’s time to start working with the data. We import it into the statistical software, then begin to clean (or preprocess) the data. This is the point we ensure the data has imported correctly and begin to check for potential errors or missingness in the data. We talk more about this step in Section 3.3.\nNext, we wrangle and explore the data, often called exploratory data analysis. This is the point in which we really begin to understand the observations in the data, distributions of the variables in the data, and relationships between variables. We begin to think about the regression modeling, so we may create new variables from existing ones, start to consider how to handle outliers or deal with missing observations. We talk about this step extensively in Chapter 3.\n\n\n\nData consumers: individuals who work with data that have already been collected (Gebru et al. 2021)\n\n\nData provenance: Record of a dataset’s origin and history of changes\n\nCodebook: Documentation of variables in a data set\n\n\nAfter we have explored the data it is time to model the data, the primary topic of this text. This is where we apply statistical methods to more rigorously describe relationships between variables and glean insights from the data to answer the analysis questions posed in the first step. Note from Figure 1.3 that we often iterate through modeling, exploring, and wrangling, as the modeling may uncover areas in which we need to consider variable transformations (Chapter 9) or consider new relationships in the data.\nOnce we have finalized the model and drawn conclusions, it’s time to communicate the results. This is a very important step in the workflow, because as Wickham (2014) put it, “the end product of an analysis is not a model: it is rhetoric”. We often conduct data science to inform decisions, such as informing business decisions or to share new knowledge through innovative research results. Therefore, it is important that we accurately and effectively communicate the results of the work to collaborators, stakeholders, and sometimes a general audience. Throughout the book, we will discuss how to communicate interpretations and conclusions from the regression model, along with various nuances that can make our communication more or less effective.\nBecause the goal of this book is to introduce regression analysis methods, the “model” part of the data science workflow will be given out-sized attention, compared to the time it often takes in practice. We will illustrate the other parts of the workflow throughout, because they are important for fully understanding and using the regression analysis results, and to remind us that regression analysis is one part of a larger data science workflow.\n\n1.2.3 Reproducibility\nThe third pillar of the data science approach introduced in Section 1.2.1 is using a reproducible workflow. Alexander (2023) defined a reproducible workflow as one “which can be exactly redone, given all the materials used…The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables.”  Using this definition as the baseline, we will discuss the following components of a reproducible workflow:\n\nData\nCode and analysis reports\nVersion control\nOrganization\nEnvironment\n\nReproducibility is infused throughout the data science workflow in Figure 1.3, from the initial plan through communicating results using reproducible tools. The components of reproducibility are introduced separately to help organize our thinking, but they are greatly intertwined in practice. For example, it is difficult to talk about best practices for coding without taking into account the data.\n\n1.2.3.1 Data\nThe first component of a reproducible workflow is clear documentation for the data. We introduced the notion of data sheets from Gebru et al. (2021) to document data provenance and a codebook to document the variables in the data set.  The data sheet provides background for the data that is useful context as we interpret results. The codebook is documentation of the individual variables, so we are clear about what is measured as we conduct the analysis. In general, a codebook should include the following details for each column in the data set:\n\nColumn name\nData type (e.g., quantitative, categorical, date, identifier)\nPrecise definition of the variable\nPossible range of values or categories\nUnits (if applicable)\n\n\nSometimes codebooks have incomplete information. If information about the data provenance or about individual variables are missing, work with collaborators who are knowledgeable about the data collection process if available, or use exploratory data analysis (Chapter 3) to fill in as much missing information as possible.\n\n\n1.2.3.2 Code and analysis reports\nWe will use the statistical software R for the analyses in the book. The code and analyses will be written in one of two file types when using R: scripts (.R) and Quarto documents (.qmd). In general, we use scripts for processes that produce output but not analyses (e.g., creating data sets and writing functions). In contrast, we use Quarto documents for more complete analyses in which there is code, output, and narrative for the process and interpretation of results. This book focuses on complete analyses, so Quarto documents should be used. Quarto documents are more formally introduced in Section 2.7.\nPart of making code reproducible is making it readable to other users (including future you). In this text, we will do this, in part, by utilizing the tidyverse, a specific R syntax that utilizes human-readable names. The tidyverse is introduced in Section 2.3. Additionally, we use narrative in Quarto documents or comments in scripts to describe what a set of code is doing. The goal is not to explain every line of code, but to give general ideas about what analysis task is being done in a set of code and the motivation for doing it. Comments and narrative can also be helpful to explain parts of code that may not be immediately obvious to another reader (or future you) (Alexander 2023). \nWe can further improve the readability of code through the choice of naming convention for variables and functions and how the code is formatted. See Chapter 4 of R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) for further reading on code style to improve readability.\n\n1.2.3.3 Version control\nVersion control is a “system that records changes to a file or set of files over time so that you can recall specific versions later.” (Chacon and Straub 2014) The ability to systematically track changes to a file helps make work more reproducible and aids in collaboration. Instead of saving new versions of a file each time we update it (i.e., my-analysis.qmd, my-analysis-v2.qmd, my-analysis-final.qmd, my-analysis-final-updated.qmd), we practice version control by documenting the changes made to a single file and making it easier to see the history of changes. It is also useful for debugging code, as version control allows makes it easier to go back to a version of the file before the coding issues occurred.\nGit is a software used for version control. It is installed on a computer or virtual machine and configured to use with the analysis software. Throughout an analysis, we write commit messages, short and informative messages that describe the changes that have been made to a document (e.g., Added model interpretation). These changes are committed (saved)and tracked in git. It is similar to the track changes feature in a word processor with the addition of messages that briefly describe the changes at each iteration.\nGitHub (https://github.com) and GitLab (https://gitlab.com) are two online platforms for storing git-based projects. They serve as a nice space to backup git-based projects along as a centralized platform for collaboration on git-based projects. They are similar to cloud storage platforms  such as Dropbox or Google Drive, with additional functionality specific to git.\nThe assignments in the supplemental materials  are designed to for regular version control throughout. The book Happy Git and GitHub for the useR (Bryan 2020) is a great reference for the technical details about installing and using git and GitHub with R.\n\n\nVersion control: Process of systematically tracking changes to a file \ngit: Software used for version control\nGitHub/ GitLab: online platforms for storing git-based projects\nCommit messages: Short and informative messages to describing a change\nCommit: Save and store changes in git\n\n\n\n1.2.3.4 Organization\nIt is good practice to keep all files associated with a given analysis within a single folder to make it easy for you and others to navigate. These folders are called projects in RStudio and workspaces in Positron, the data analysis software platforms made by Posit. \nAn advantage of housing all files for an analysis within a single folder is that files can referenced using relative paths (e.g., data/mydata.csv instead of User/id1234/my-project/data/my-data.csv). Relative paths are important for reproducibility and collaboration, because the path names still work when the project is opened on different computers.\nOther considerations for organization are the file structure and naming conventions. There are many ways to organize and name files within a single analysis folder, so it is important to choose a file structure and naming conventions that are most easily understood by you and your collaborators. At a minimum, files should be named in such a way that can be easily read by both humans and the computer. Chapter 7 of R with Data Science (Wickham 2014) and Chapter 3 of Telling Stories with Data (Alexander 2023) provide examples and tips on file structure and naming.\n\n1.2.3.5 Environment\nThe last, and perhaps most overlooked, component of reproducibility is the computing environment in which the analysis was done. In the context of analyses using R, the environment means the version of the R packages (collections of functions) used for the analysis. Software changes over time, so documenting the environment is key for long-term reproducibility.\nThere are a few ways to do this in R. A rigorous approach is using the renv R package (Ushey and Wickham 2025) which stores the R packages in the project directory (Wickham, Çetinkaya-Rundel, and Grolemund 2023). Another option is to run the function sessionInfo() and save the results in the project directory. This function will print give information about the R version, details about the computer, packages, and their versions.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#as-much-art-as-it-is-science",
    "href": "01-intro.html#as-much-art-as-it-is-science",
    "title": "1  Regression in the data science workflow",
    "section": "\n1.3 As much art as it is science",
    "text": "1.3 As much art as it is science\n\n“All sciences have much of art in their makeup.” Tukey (1962)\n\nDoing regression analysis (and data science in general) requires a blend of art and science. Good regression analysis needs both rigorous theory-based methods and the judgment (the “artistry”) of the data scientist as they make decisions throughout the analysis process. In the same way a chef relies on their knowledge to make decisions about how much seasoning to add to dish, a data scientist relies on their understanding of statistical methods to make decisions as they prepare, explore, model, and interpret data.\nTukey (1962) argues that data analysis (or regression analysis in our case) is not just an application of statistical theory with the sole objective to “optimize” but rather requires a “heavy emphasis on judgement.” He goes on further to say that to analyze data well, “it must be a matter of judgment, and ‘theory’ whether statistical or non-statistical, will have to guide, not command” [pp. 10]. This judgment is not arbitrary, but come from three sources (summarized from Tukey (1962)):\n\njudgment based on domain knowledge\njudgment based on knowledge statistical methods\njudgment based on experience from analyzing a variety of data\n\nThroughout the book, we will demonstrate how judgment is used to make decisions in the analysis, particularly as the regression models become more complex in later chapters. As much as we will highlight parts of the decision-making process, using judgment and doing the “art” of regression analysis can’t be learned by merely reading a textbook. It is a skill that is developed after consistent practice working with data. As the world becomes increasingly driven by artificial intelligence, it is this ability to infuse the “art” along with the science that will continue to make human data scientists indispensable.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#summary",
    "href": "01-intro.html#summary",
    "title": "1  Regression in the data science workflow",
    "section": "\n1.4 Summary",
    "text": "1.4 Summary\nIn this chapter, we introduced the data science workflow and the how regression analysis fits within that workflow. We defined reproducibility and the components to take into account when developing a reproducible workflow. We introduced regression analysis, why we fit regression models (inference and prediction), and the role of judgment (“art”) in regression analysis (and data science in general).\nThe next chapter Chapter 2 is a review (or short introduction) to using the tidyverse for data wrangling, visualization, and writing reproducible reports in Quarto.\n\n\n\n\nAlby, Tom. 2023. Data Science in Practice. Chapman; Hall/CRC.\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in r. Chapman; Hall/CRC.\n\n\nBaumer, Benjamin S, Daniel T Kaplan, and Nicholas J Horton. 2017. Modern Data Science with r. Chapman; Hall/CRC.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231.\n\n\nBryan, Jenny. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nChacon, Scott, and Ben Straub. 2014. Pro Git. Apress.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” In Breakthroughs in Statistics: Methodology and Distribution, 408–52. Springer.\n\n\nUshey, Kevin, and Hadley Wickham. 2025. “Renv: Project Environments.” https://doi.org/10.32614/CRAN.package.renv.\n\n\nWickham, Hadley. 2014. “Data Science: How Is It Different to Statistics?” https://imstat.org/2014/09/04/data-science-how-is-it-different-to-statistics/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. \" O’Reilly Media, Inc.\".\n\n\nWikipedia contributors. 2025. “Data Science.” https://en.wikipedia.org/wiki/Data_science.\n\n\nZarulli, Virginia, Elizaveta Sopina, Veronica Toffolutti, and Adam Lenart. 2021. “Health Care System Efficiency and Life Expectancy: A 140-Country Study.” PLoS One 16 (7): e0253450.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Regression in the data science workflow",
    "section": "",
    "text": "Example answers: Have the countries have life expectancy of 72.8 years or higher. There are a few countries with low life expectancy less than 55 years. There appear to be two subgroups of countries in regard to income inequality; those with income inequality between 5 and 15 and those between 20 and 30. There are two countries with high income inequality over 40.↩︎\nExample prediction question: What is the life expectancy for a country with high education and no income inequality?Example inference question: How does a country’s life expectancy change as income inequality increases?↩︎\nThis workflow is inspired by the data science workflows in Wickham, Çetinkaya-Rundel, and Grolemund (2023) and Timbers, Campbell, and Lee (2022).↩︎\nSometimes the data scientist is part of the data collection process, thus making them one of the data creators (Gebru et al. 2021), in addition to being a data consumer. As a data creator, it is imperative to provide detailed documentation about the data, so that data consumers have the necessary information to make informed decisions about the data. See Gebru et al. (2021) for more on data documentation.↩︎",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression in the data science workflow</span>"
    ]
  },
  {
    "objectID": "02-review-r.html",
    "href": "02-review-r.html",
    "title": "2  Data analysis in R",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#learning-goals",
    "href": "02-review-r.html#learning-goals",
    "title": "2  Data analysis in R",
    "section": "",
    "text": "Wrangle and manipulate data using dplyr\n\nCreate visualizations using ggplot2\n\nCustomize ggplot2 visualizations\nCreate reproducible documents using Quarto",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#sec-penguins-intro",
    "href": "02-review-r.html#sec-penguins-intro",
    "title": "2  Data analysis in R",
    "section": "\n2.1 Data: Palmer penguins",
    "text": "2.1 Data: Palmer penguins\nIn this chapter, explore data about penguins collected at the Palmer Station in Antarctica. The data were collected 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program. The data are in the penguins data frame in the palmerpenguins R package (Horst, Hill, and Gorman 2020).\nWe will use the following variables:\n\nspecies: Penguins species (Adelie, Chinstrap, Gentoo)\nisland: Island in Palmer Archipelago (Biscoe, Dream, Torgersen)\nbill_length_mm: Bill length in millimeters\nflipper_length_mm : Flipper length in millimeters\nsex : Penguins sex (female, male)\nyear : Study year (2007, 2008, 2009)\n\nSee the (Horst, Hill, and Gorman 2020) for the full code book.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#installation",
    "href": "02-review-r.html#installation",
    "title": "2  Data analysis in R",
    "section": "\n2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 Installing R and RStudio\nWe will use the statistical analysis software R. Go to https://cran.rstudio.com to download and install R. R version 4.5.0 was used for the computing in this book. We use R through an integrated user environment (IDE). The IDE is a user-friendly interface with additional features that make it easier to use the underlying software. Ismay and Kim (2019) describes it this way, “R is like a car’s engine while [the IDE] is like a car’s dashboard.” \nThere are two widely used IDEs for R offered by the open-source data science company Posit: RStudio and Positron. At this point, RStudio is the most commonly used environment for R (Posit Software, PBC 2025) and is widely used by R programmers in classes, academic research, and industry. Go to https://posit.co/download/rstudio-desktop to download RStudio on a laptop or desktop computer. A cloud-based version is also available on Posit Cloud (https://posit.cloud/). Positron is a new IDE designed specifically for a data science workflow. It has more advanced data exploration and AI-assistance capabilities compared to RStudio. Go to https://positron.posit.co to download Positron1.\nOverall, the content in this chapter is the same when using RStudio and Positron, with the exception of a minor difference mentioned in Section 2.7.\n\n2.2.2 Installing R packages\nCoding in R is primarily done by calling functions that perform specific tasks. These functions are located in packages. There is a collection of “base R” functions that are automatically loaded in R. A list of packages for the most recent version of R is available at R Core Team (2025).\nEven with a lot of functionality built into R, we may want to utilize advanced or specialized functions that are available through R packages. There are two primary ways to install packages: from the Comprehensive R Archive Network (CRAN) or from GitHub repositories.\nCRAN is the official network for hosting R packages and documentation, and packages must undergo R a rigorous review and testing process to be hosted on CRAN. We use the code install.packages(\"Package Name\") to install packages from CRAN. For example, the code to install the tidyverse package is\n\ninstall.packages(\"tidyverse\")\n\nPackages that are not available on CRAN are typically available through a GitHub repository created by the package developer. This is often the cases for new packages or in-development versions of existing packages. We use the code remotes::install_github(\"Package repo\") to install packages from GitHub. The code remotes:: calls the remotes package (Csárdi et al. 2024) that contains the install_github function.2\n\nPackages only need to be installed once on a given computer or virtual machine; however, we can periodically update them as new versions are released. Once the package is installed, we use the library() function to load it into R. Though we only install packages once, we need to load packages each time we have a new instance of R.\n\nInstalling versus loading packages has been compared to the lights in the room. The light bulbs are installed once, but we still need to flip a switch to turn on the light each time we enter a room.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#sec-tidyverse",
    "href": "02-review-r.html#sec-tidyverse",
    "title": "2  Data analysis in R",
    "section": "\n2.3 Tidyverse",
    "text": "2.3 Tidyverse\nThe tidyverse (Wickham et al. 2019) is an “an opinionated collection of R packages designed for data science” (tidyverse Development Team 2025). Packages in the tidyverse have a common structure and syntax, making it more streamlined to utilize functions from multiple packages in a data analysis. As shown in the previous section, tidyverse is available on CRAN. We load tidyverse using the code below. From the output, we see the nine core packages that load as part of tidyverse.\n\nlibrary(tidyverse)\n\nThis chapter focuses on dplyr (Wickham et al. 2023), the package for data manipulation, and ggplot2 (Wickham 2016), the package data visualization. We will introduce other packages, as needed, in subsequent chapters.  See https://tidyverse.org for a full description of all the core tidyverse packages.\n\n2.3.1 Tidy data\nA central component of the tidyverse is the notion of “tidy” data. Wickham (2014) defines tidy data as data sets that have the following characteristics:\n\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\nLet’s take a look at the penguins data frame from the palmerpenguins package that was introduced in Section 2.1.\n\nlibrary(palmerpenguins)\ndata(penguins)\n\nThe first ten rows of penguins is shown in Table 2.1.\n\n\nTable 2.1: First 10 rows of penguins data\n\n\n# A tibble: 10 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 4 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\nLet’s use the criteria above to evaluate whether this is a tidy data frame.\n\n\nEach variable forms a column.\n\nYes, the variables are stored in individual columns. There are no instances of multiple variables stored in a single column or a single variable stored across multiple columns.\n\n\n\nEach observation forms a row.\n\nYes, the information for one penguin (observation) is across a single row. There are no instances of multiple penguins represented on a single row or a single penguin represented across multiple rows.\n\n\n\nEach type of observational unit forms a table.\n\nYes, the observational unit is the set of penguins included in the study. The data from these penguins makes up the data frame.\n\n\n\nAll the criteria are met, so penguins is a tidy data frame.\nAll the data sets used in the book are tidy, so we can focus on regression analysis concepts and methods. In practice, we may need to “tidy” data before doing any analysis. Wickham (2014) has recommendations for handling different issues that make data untidy.\nThe data sets in the tidyverse are stored as tibbles. Tibbles are like data frames in R with some differences in the underlying features. As Wickham, Çetinkaya-Rundel, and Grolemund (2023) puts it, a tibble is a data frame that modifies “some older behaviours to make life a little easier”. One nice feature of tibbles is that they show summary information about the data set overall and for each variable. For example, in Table 2.1 we see there are 344 rows (observations) and 8 columns (variables). We also see the format of each column, e.g. species is stored as a &lt;fct&gt; (factor). We talk more about data formats in Section 3.3.\n\n2.3.2 The pipe\n\nOne benefit of the tidyverse syntax is its readability. Though it can be more verbose than other coding styles, its naming convention and structure makes it easy  to read and understand. A key part of the readability is due to the pipe. The pipe, coded as |&gt;, is used to pass data from one function to the next. If we think of reading code aloud in English, we say “and then” when we see a pipe (Çetinkaya-Rundel et al. 2021).\nFor example, let’s suppose we write code to describe the steps to make a ham and cheese sandwich. \n\nbread_slice |&gt;\n  add_layer(ingredient = \"ham\") |&gt; \n  add_layer(ingredient = \"cheese\") |&gt; \n  add_bread_slice()\n\nWe would read this aloud in English as the following:\n\nStart with a bread slice,\nand then add a layer of ham\nand then add a layer of cheese,\nand then add another bread slice.\n\n\nA series of functions joined by pipes, like the sandwich example above, is called a pipeline. We will see pipelines throughout the next section as we introduce data manipulation with dplyr.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#data-manipulation-with-dplyr",
    "href": "02-review-r.html#data-manipulation-with-dplyr",
    "title": "2  Data analysis in R",
    "section": "\n2.4 Data manipulation with dplyr",
    "text": "2.4 Data manipulation with dplyr\nThe dplyr package is tidyverse’s primary package for data wrangling and manipulation. We often use functions from dplyr for analysis steps such as data exploration (Chapter 3), evaluating model conditions (Chapter 6), and applying variable transformations (Chapter 9).\nThe functions in dplyr perform one of the following tasks: manipulate the data by row, manipulate the data by column, or manipulate groups of rows. We will show key functions in each of these categories that are used frequently in the text. The full list of dplyr functions is available at the dplyr reference site https://dplyr.tidyverse.org.\n\n2.4.1 Data manipulation by row\nfilter()\nThe filter function is used to choose rows based on a set of criteria defined by the columns in the data. For example, we may use filter() to narrow the data to a specific subgroup of observations. The filtering criteria can be based on one variable or multiple variables. For example, let’s filter the data such that we choose the rows for penguins in the Adelie species.\n\npenguins |&gt; \n  filter(species == \"Adelie\")\n\n# A tibble: 152 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 146 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nHow many penguins are in the Adelie species?3\n\nWe can filter based on multiple variables by specifying “and” or “or” criteria. For example, let’s filter the data so that we choose rows for penguins in the Adelie species and from Dream island.\n\npenguins |&gt; \n  filter(species == \"Adelie\" & island == \"Dream\")\n\n# A tibble: 56 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Dream            39.5          16.7               178        3250\n2 Adelie  Dream            37.2          18.1               178        3900\n3 Adelie  Dream            39.5          17.8               188        3300\n4 Adelie  Dream            40.9          18.9               184        3900\n5 Adelie  Dream            36.4          17                 195        3325\n6 Adelie  Dream            39.2          21.1               196        4150\n# ℹ 50 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThere are 56 penguins from the Adelie species and Dream island. This is a smaller subset than in the previous example, because observations have to satisfy both criteria to be included in the resulting tibble.\nAlternatively, let’s suppose we wish to filter the data such that we choose penguins who are from the Adelie species or from Dream island.\n\npenguins |&gt; \n  filter(species == \"Adelie\" | island == \"Dream\")\n\n# A tibble: 220 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 214 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThere are 220 penguins in the resulting tibble. There are more penguins in this tibble than in the previous two, because penguins can meet one or both of the criteria to be included in the resulting tibble.\nLastly, we use ! to define criteria in terms of excluding observations that take a certain value. For example, let’s select the penguins in the Adelie species and are not on Dream island.\n\npenguins |&gt;\n  filter(species == \"Adelie\" & island != \"Dream\")\n\n# A tibble: 96 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 90 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\narrange()\nThe arrange function is used to sort the data based on the values in one or more columns. This can be useful as we explore the data to identify outlying observations with unusually high or unusually low values. In the code below, the observations are sorted in ascending order (smallest to largest) based on values of bill_length_mm .\n\npenguins |&gt; \n  arrange(bill_length_mm)\n\n# A tibble: 344 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Dream               32.1          15.5               188        3050\n2 Adelie  Dream               33.1          16.1               178        2900\n3 Adelie  Torgersen           33.5          19                 190        3600\n4 Adelie  Dream               34            17.1               185        3400\n5 Adelie  Torgersen           34.1          18.1               193        3475\n6 Adelie  Torgersen           34.4          18.4               184        3325\n# ℹ 338 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nFrom the first few observations, we see the values of bill_length_mm increase as we move down the list of observations. Now we can more easily see the smallest bill_length_mm in the data, 32.1 mm. Including the desc argument in the arrange function sorts the data in descending order (largest to smallest). The code below sorts the observations in descending order by bill_length_mm.\n\npenguins |&gt; \n  arrange(desc(bill_length_mm))\n\n# A tibble: 344 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n4 Chinstrap Dream            55.8          19.8               207        4000\n5 Gentoo    Biscoe           55.1          16                 230        5850\n6 Gentoo    Biscoe           54.3          15.7               231        5650\n# ℹ 338 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe largest bill length in the data set is 59.6 mm.\n\nThese examples illustrate a feature of tidyverse pipelines. Pipeline always start with a tibble (penguins in these examples) and produce a tibble by default. We can extract a vector of values using pull() at the end of pipeline.\n\nFor example, the code below extracts a vector of bill lengths sorted in ascending order.4\n\npenguins |&gt; \n  arrange(bill_length_mm) |&gt;\n  pull(bill_length_mm)\n\n\n\n [1] 32.1 33.1 33.5 34.0 34.1 34.4 34.5 34.6 34.6 35.0",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#data-manipulation-by-column",
    "href": "02-review-r.html#data-manipulation-by-column",
    "title": "2  Data analysis in R",
    "section": "\n2.5 Data manipulation by column",
    "text": "2.5 Data manipulation by column\nselect()\nThe select function narrows the data by choosing specific columns. We use this function when we want to retain or display only certain columns. For example, let’s create a new tibble that only contains the columns bill_length_mm and flipper_length_mm.\n\npenguins |&gt;\n  select(bill_length_mm, flipper_length_mm)\n\n# A tibble: 344 × 2\n  bill_length_mm flipper_length_mm\n           &lt;dbl&gt;             &lt;int&gt;\n1           39.1               181\n2           39.5               186\n3           40.3               195\n4           NA                  NA\n5           36.7               193\n6           39.3               190\n# ℹ 338 more rows\n\n\nNote that the number of rows remains unchanged from the original data.\nWe may also exclude columns by using - in front of the column name. For example, in the code below we create a new tibble that excludes the column species.\n\npenguins |&gt;\n  select(-species)\n\n# A tibble: 344 × 7\n  island  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Torger…           39.1          18.7               181        3750 male   2007\n2 Torger…           39.5          17.4               186        3800 fema…  2007\n3 Torger…           40.3          18                 195        3250 fema…  2007\n4 Torger…           NA            NA                  NA          NA &lt;NA&gt;   2007\n5 Torger…           36.7          19.3               193        3450 fema…  2007\n6 Torger…           39.3          20.6               190        3650 male   2007\n# ℹ 338 more rows\n\n\nmutate()\nThe mutate function is used to create new columns or change the values in an existing column. We often use this in the analysis to create new variables from existing ones or to impute values for missing data.\nFor example, let’s create a new variable called dream that takes value 1 if the penguin is from the Dream island and the value 0 otherwise (called an indicator variable).\n\npenguins |&gt; \n  mutate(dream = if_else(island == \"Dream\", 1, 0))\n\n# A tibble: 344 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 338 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, dream &lt;dbl&gt;\n\n\nAt the top of the tibble, we see the number of observations hasn’t changed, but we now have an additional column in the data set. Below are the values of island and dream for ten randomly selected observations.\n\n\n# A tibble: 10 × 2\n  island dream\n  &lt;fct&gt;  &lt;dbl&gt;\n1 Dream      1\n2 Biscoe     0\n3 Biscoe     0\n4 Biscoe     0\n5 Biscoe     0\n6 Dream      1\n# ℹ 4 more rows\n\n\nThis code uses the if_else() function, a useful function for applying “if/else” statements. The structure of the function is if_else([Criteria], [Result if true], [Result if false]).\nThe mutate function is also used to change the values in existing columns. Using the name of an existing column will overwrite the data in that column based on the output from mutate(). For example, there are 11 penguins in the data with missing values for sex. Therefore, we would like to input the value Missing if the data are missing and otherwise keep the original value of sex. The values of sex for ten randomly selected penguins are below.\n\npenguins |&gt;\n  mutate(sex = if_else(is.na(sex), \"Missing\", sex))\n\n\n\n# A tibble: 10 × 1\n  sex    \n  &lt;chr&gt;  \n1 Missing\n2 male   \n3 female \n4 male   \n5 male   \n6 female \n# ℹ 4 more rows\n\n\nThe code above uses is.na(), a logical function that returns TRUE if the value is missing and FALSE otherwise.\n\nCreate a tibble that only includes penguins from Biscoe island. Arrange the rows in descending order of bill_length_mm. Display the columns species and bill_length_mm. What is the species and bill length for the first observation?5\n\n\n2.5.1 Data manipulation by group\ncount()\nThe count function is used to compute the number of observations that take each unique value in a column or combination of columns. We often use this in data analysis to produce frequency tables for categorical variables.\nFor example, the code below produces the number of observations from each island.\n\npenguins |&gt;\n  count(island)\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\n\nWe may also compute the number of observations that take each combination of values for two or more variables. The code below produces the number of observations that take each combination of island and species.\n\npenguins |&gt; \n  count(island, species)\n\n# A tibble: 5 × 3\n  island    species       n\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;\n1 Biscoe    Adelie       44\n2 Biscoe    Gentoo      124\n3 Dream     Adelie       56\n4 Dream     Chinstrap    68\n5 Torgersen Adelie       52\n\n\nSome combinations of island and species are missing from the output. This means there are no observations in the data with that combination of values. For example, there are no penguins in the data who are from Biscoe island and the Chinstrap species. Include the argument .drop = FALSE to see all possible combinations of values, including those with zero observations in the data.\n\npenguins |&gt;\n  count(island, species, .drop = FALSE)\n\n# A tibble: 9 × 3\n  island species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe Adelie       44\n2 Biscoe Chinstrap     0\n3 Biscoe Gentoo      124\n4 Dream  Adelie       56\n5 Dream  Chinstrap    68\n6 Dream  Gentoo        0\n# ℹ 3 more rows\n\n\nsummarize()\nThe summarize6 function computes summary statistics for columns in the data. See the summarize() reference page at https://dplyr.tidyverse.org for a list of commonly used summary statistics.\nFor example, we want to compute the mean and standard deviation of bill_length_mm.\n\npenguins |&gt;\n  summarize(mean_bill_length = mean(bill_length_mm),\n            sd_bill_length = sd(bill_length_mm)) \n\n# A tibble: 1 × 2\n  mean_bill_length sd_bill_length\n             &lt;dbl&gt;          &lt;dbl&gt;\n1               NA             NA\n\n\nThis code did not return values but instead produced NA. By default, the functions for computing specific summary statistics, e.g., mean(), do not remove missing values (NA) from the computation. Thus, they are unable to compute summary statistics if there any values are missing in the column. We use the argument na.rm = TRUE to ignore missing values when computing summary statistics.\n\npenguins |&gt; \n  summarize(mean_bill_length = mean(bill_length_mm, na.rm = TRUE), \n            sd_bill_length = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_bill_length sd_bill_length\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             43.9           5.46\n\n\nNote, this now means we have the mean and standard deviation of bill_length_mm among the rows that do not have missing observations, not all observations. Missing data is discussed in more detail in Chapter 3. \ngroup_by()\nThe last dplyr function we introduce here is group_by(). This function is used to perform calculations separately for subgroups of the data. This is often useful when we want to compare results between groups. For example, let’s look at the mean and standard deviation of bill_length_mm by species.\n\npenguins |&gt;\n  group_by(species) |&gt;\n  summarize(mean_bill_length = mean(bill_length_mm, na.rm = TRUE), \n            sd_bill_length = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species   mean_bill_length sd_bill_length\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.5           3.08\n\n\nAll functions following group_by() in the pipeline will be performed separately for each subgroup. The ungroupfunction removes the grouping structure, so any functions following ungroup() are applied to all observations.\n\nUse median() to compute the median bill length separately for each combination of island and species. What is the median bill length for Adelie penguins on Dream island? 7\n\nThis section provided a short overview of the dplyr functions that are used frequently in this book. See Chapter 3 in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) for more on data manipulation and Chapter 5 for more on tidy data.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#data-visualizations-using-ggplot2",
    "href": "02-review-r.html#data-visualizations-using-ggplot2",
    "title": "2  Data analysis in R",
    "section": "\n2.6 Data visualizations using ggplot2",
    "text": "2.6 Data visualizations using ggplot2\nggplot2 is the primary package for data visualizations in tidyverse. The code in ggplot2 is designed to produce plots in layers, with each layer separated by +. This section is an overview of the package that largely follows the structure of Chapter 1 in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). We point readers there for a more detailed introduction to the package.\nThe general structure of code to create a visualization is as follows\nggplot(data = [data set], aes(x = [x-var], y = [y-var])) + \n  geom_xx() + \n  other options \n\nThe first part of the code establishes the base layer by specifying the tibble that will be used to make the visualization.\n\nggplot(data = penguins)\n\n\n\n\n\n\nFigure 2.1: Base layer for a ggplot2 visualization\n\n\n\n\nThe next part of the code is used to set the aesthetics that indicate how the variables are mapped onto features of the plot. It is defined in the mapping = argument inside the aes() function. Often times, we do not see the argument name mapping = in the code, but it is the second argument of ggplot().\nLet’s map the aesthetics such bill_length_mm is on the x-axis and flipper_length_mm is on the y-axis.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm))\n\n\n\n\n\n\nFigure 2.2: Define variables for a ggplot2 visualization\n\n\n\n\nNow that we have the foundation, the next layer of the code defines the type of plot to make. The plot types are defined using functions with the prefix geom_. Here we use geom_point() to make a scatterplot.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\n\nFigure 2.3: Specify geom (plot type) for ggplot2 visualization\n\n\n\n\n\n2.6.1 Geoms\nWe will focus on the scatterplot for the majority of this section, but there are many other geoms available in ggplot2. Below are some of the geoms that will be used frequently throughout the book.\nThe first plots are a histogram and a density plot. Both are used to visualize the distribution of a single quantitative variable. The code and output produce the histogram and density plot for bill_length_mm.\n# histogram\nggplot(data = penguins, aes(x = bill_length_mm)) + \n  geom_histogram()\n\n# density plot\nggplot(data = penguins, aes(x = bill_length_mm)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n(b) Density plot\n\n\n\n\n\n\nFigure 2.4: Geoms for single quantitative variable\n\n\nA common plot for the distribution of a single categorical variable is a bar graph. Below are the code and output of a bar graph of the distribution of island.\n\nggplot(data = penguins, aes(x = island)) + \n  geom_bar()\n\n\n\n\n\n\nFigure 2.5: Bar graph for single categorical variable\n\n\n\n\nSide-by-side boxplots are used to visualize the relationship between a quantitative variable and a categorical variable. The code and output for a side-by-side boxplot of bill_length_mm versus species is below.\n\nggplot(data = penguins, aes(x = species, y = bill_length_mm)) + \n  geom_boxplot()\n\n\n\n\n\n\nFigure 2.6: Boxplot for quantitative and categorical variables\n\n\n\n\nThese are a few of the many geoms available in ggplot2. A full list is available at https://ggplot2.tidyverse.org.\n\n2.6.2 Customizing plots\nThere are a multitude of ways to customize plots in ggplot2. In this section, we will focus on customization that is used in the text and is useful for effective and clear data analysis and communication. These features are illustrated on the scatterplot of bill_length_mm versus flipper_length_mm, but they can be applied to any type of ggplot2 visualization.\nLabels\nBy default, ggplot2 labels the components of the plot using the variable names from aes(). If the variables have clear names, then these default labels are generally sufficient. When we are ready to communicate results (e.g,. through reports or presentations), the plot labels should be clear to the intended audience. This means using full words (or meaningful abbreviations), units when appropriate, and a title or caption.\nThe labs() function is used to create a new layer on the visualization with updated labels. The code below is used to change the labels on the x- and y-axes and add a title to the scatterplot.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length \")\n\n\n\n\n\n\nFigure 2.7: Scatterplot with axis labels and title\n\n\n\n\n\n2.6.2.1 Aesthetics\nThere are a variety of aesthetics, visual features, that can be customized in ggplot2. Some commonly used aesthetics include color, shape, size, alpha (transparency), fill, and linetype. The same aesthetics can be applied across the entire plot, or aesthetics can be applied based on values of a variable.\nWe often apply the same aesthetic across an entire plot to make it more visually appealing or easier to read. This type of aesthetic change does not add any additional information to the visualization.\nFor example, the code below changes the color of all points on the plot by adding the color = argument in geom_point(). The colors can be defined based on a list of over 600 colors8 or HTML hex color codes.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm)) + \n  geom_point(color = \"steelblue\") +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length \")\n\n\n\n\n\n\nFigure 2.8: Color points on scatterplot\n\n\n\n\nThe second way to apply aesthetics is to map them to specific variables. This adds makes the visualization engaging and adds additional information about the data. This is useful in data analysis when we want to make comparisons across subgroups of data.\nThe aesthetics are defined inside aes() to map them to a particular variable (similar to defining the variables that go on each axis). For example, the code below colors the points based on species. We also add a subtitle and update the color label in labs().\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length\", \n       color = \"Species\",\n       subtitle = \"By species\")\n\n\n\n\n\n\nFigure 2.9: Scatterplot with points colored by species\n\n\n\n\nNow we are able to see the relationship between bill_length_mm and flipper_length_mm by species and make comparisons between species.\nWe can also map multiple aesthetics to variables. In the code below, the color of the points is based on species and the shape is based on island.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species, shape = island)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length\", \n       color = \"Species\", \n       shape = \"Island\",\n       subtitle = \"By species and island\")\n\n\n\n\n\n\nFigure 2.10: Scatterplot with color by species and shape by island\n\n\n\n\n\nThe code below produces the plot of the bill_length_mm versus species in Figure 2.6.\n\nggplot(data = penguins, aes(x = species, y = bill_length_mm)) + \n  geom_boxplot()\n\nDescribe how to update the code such that\n\nall boxes are filled in using the color cyan.\nthe color of the boxes are filled in based on species.9\n\n\n\n2.6.3 Faceting\nAnother way to view a visualization by subgroups is by faceting, creating a separate plot for each subgroup. The facet_wrap function is used to facet based on a single variable. In the code below, we modify the previous plot by faceting by island rather than changing the shape based on island.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\",\n       title = \"Bill length versus Flipper length\", \n       color = \"Species\", \n       shape = \"Island\",\n       subtitle = \"Faceted by island\") + \n  facet_wrap(vars(island))\n\n\n\n\n\n\nFigure 2.11: Scatterplot faceted by island\n\n\n\n\nBecause there is a lot of overlap across islands, the faceted plot makes it easier to see the relationship within each island and identify which species are on each island.\nWe use facet_grid() to facet based on two variables, such that one variable defines the rows and the other defines the columns. In the code below, we now update the previous plot such that the rows are defined by island the columns are defined by species.\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length\",\n       color = \"Species\",\n       subtitle = \"Faceted by island and species\") + \n  facet_grid(rows = vars(island),\n             cols = vars(species))\n\n\n\n\n\n\nFigure 2.12: Scatterplot faceted by island and species\n\n\n\n\nNote that facet_grid() creates a plot for every possible combination of the variables. The empty plots indicate combinations of island and species that do not exist in the data.\n\nRecreate the plot below of flipper_length_mm versus species faceted by year. 10\n\n\n\n\n\n\n\n\nFigure 2.13: Boxplot of flipper_length_mm versus species faceted by year\n\n\n\n\n\n\n2.6.4 Color palettes\nThus far, we have relied on the default color palette in ggplot2 when we’ve mapped an aesthetic to color. We may wish to choose colors to align with a particular color scheme or those that are more accessible. The functions scale_color_manual() and scale_fill_manual() are used to “manually” define color palettes. We use scale_color_manual() to define a color palette that maps onto the color aesthetic and scale_fill_manual() to define a color palette that maps to the fill aesthetic.\nPackages such as viridis (Garnier et al. 2024) and PNWColors (Lawlor 2020) have pre-defined color palettes. Both packages are available on CRAN. In the code below, we apply the Bay color palette from the PNWColors package to Figure 2.9, the scatterplot of bill_length_mm versus flipper_length_mm colored by species.\nThe argument values = pnw_palette(\"Bay\", 3) indicates that the colors are mapped to the Bay color palette using there colors (one for each species).\n\nlibrary(PNWColors)\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length\", \n       subtitle = \"By species\") +\n  scale_color_manual(values = pnw_palette(\"Bay\", 3))\n\n\n\n\n\n\nFigure 2.14: Scatterplot with color palette from PNWColors\n\n\n\n\nWe may also wish to create a color palette. For example, here we apply the palette my_color_palette that is defined using HTML hex codes.\n\nmy_color_palette &lt;- c(\"#993399\", \"#2B8181\", \"#CB4F04\")\n\nggplot(data = penguins, aes(x = bill_length_mm, y = flipper_length_mm, \n                            color = species)) + \n  geom_point() +\n  labs(x = \"Bill length (in millimeters)\", \n       y = \"Flipper length (in millimeters)\", \n       title = \"Bill length versus Flipper length\", \n       subtitle = \"By species and island\") +\n  scale_color_manual(values = my_color_palette)\n\n\n\n\n\n\nFigure 2.15: Scatterplot with user-define color palette\n\n\n\n\nWhen choosing a color palette, it is important to consider the accessibility of the color choices, how they may be viewed by individuals with color-blindness , and how they appear in gray scale. There are multiple ways to check the accessibility of color palettes. The Color Palette Finder on the R Graph Gallery website (https://r-graph-gallery.com/color-palette-finder) is a tool for checking the accessibility of pre-defined color palettes. The colorblindcheck R package (Nowosad 2019) allows users to check the accessibility of pre-defined or custom color palettes.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#sec-quarto",
    "href": "02-review-r.html#sec-quarto",
    "title": "2  Data analysis in R",
    "section": "\n2.7 Quarto",
    "text": "2.7 Quarto\nIn Chapter 1 we introduced reproducibility, a key component of data science in which analysis is conducted in such a way that another person (or ourselves months later) can produce the same results given the same code and data set.\nQuarto is “an open-source scientific and technical publishing system” (PBC 2025) developed by Posit for conducting reproducible data science. Quarto documents can produce many different types of formats  including PDFs, Word documents, websites, books, and others using a consistent syntax and document structure.11 Quarto is not automatically included in RStudio, so it must be installed. It can be downloaded from https://quarto.org/docs/get-started. It is automatically installed in Positron.\n\n\n\n\n\nFigure 2.16: Example Quarto document\n\n\nFigure 2.16 is an example of a Quarto (.qmd) document. All code, output, and narrative are in a single document, and the resulting output updates as changes are made to the document. This eliminates the need to copy and paste results from one document to another throughout the analysis, a process that is prone to error. Let’s take a look at the parts of the Quarto document in Figure 2.16.\nThe YAML (“Yet Another Markup Language” or “YAML Ain’t Markup Language”) is considered the “front matter” of the document. This is where we include content for the heading, such as title, author, and date. This is also where we specify the format type and other global features to be applied for the entire document. The YAML in Figure 2.16 shows global code chunk options applied so that the code is displayed when the document is rendered echo: true , but all additional warnings and messages are suppressed warning: false and message: false \nThe code chunk is where we put all code that is executed when the document is rendered. R code chunks12 can be added to the document by starting with ```{r} and ending with ```. They can also be added from the Insert menu in the tool bar of the visual editor. A variety of options can be applied to code chunks primarily to customize how they display code and results in the rendered document. Section 28.5 in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) provides an extensive overview of code chunks and the set of options to customize them.\nThe narrative is written in the white space of the document. The visual editor in RStudio and Positron makes formatting the narrative (e.g, making text bold or adding lists) similar to formatting in word processors such as Google Docs, Apple Pages, or Microsoft Word. The visual editor also makes it easy to add images and tables within the narrative.\n\n\n\n\n\n\n\n\n\n(a) Rendering in RStudio\n\n\n\n\n\n\n\n\n\n(b) Rendering in Positron\n\n\n\n\n\n\nFigure 2.17\n\n\nThroughout the analysis workflow, we regularly render the Quarto document to see the updated results and narrative. Documents are rendered by clicking the “Render” button in RStudio (Figure 2.17 (a)) and the “Preview” button in Positron (Figure 2.17 (b)). We also see the resulting PDFs in the Viewer pane of each IDE. When a document is rendered, all code and narrative are executed sequentially from beginning to end. Therefore, all necessary code must be in the Quarto document to render properly; code in the console is not executed when a document is rendered.\nThe assignments in the supplemental materials for this book are designed to be written in Quarto. See Chapter 28 in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) and quarto.org for further reading and resources on Quarto.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#summary",
    "href": "02-review-r.html#summary",
    "title": "2  Data analysis in R",
    "section": "\n2.8 Summary",
    "text": "2.8 Summary\nIn this chapter, we introduced data wrangling and data visualization using the tidyverse (specifically dplyr and ggplot2 packages), focusing on code for the analysis tasks frequently used in this book. We also showed ways to customize plots for effective communication and accessibility, and we introduced Quarto for writing reproducible reports. There are many online resources about the tidyverse and Quarto. A few primary resources for additional reference and practice are the following: R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023), tidyverse.org, and quarto.org.\nIn Chapter 3, we utilize the computing from this chapter as we begin analyzing data in the exploratory data analysis.\n\n\n\n\nÇetinkaya-Rundel, Mine, Johanna Hardin, Benjamin S Baumer, Amelia McNamara, Nicholas J Horton, and Colin Rundel. 2021. “An Educator’s Perspective of the Tidyverse.” arXiv Preprint arXiv:2108.03510.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan, and Dan Tenenbaum. 2024. “Remotes: R Package Installation from Remote Repositories, Including ’GitHub’.” https://doi.org/10.32614/CRAN.package.remotes.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2024. viridis(Lite) - Colorblind-Friendly Color Maps for r. https://doi.org/10.5281/zenodo.4679423.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.” https://doi.org/10.5281/zenodo.3960218.\n\n\nIsmay, Chester, and Albert Y Kim. 2019. Statistical Inference via Data Science: A Moderndive into r and the Tidyverse.\n\n\nLawlor, Jake. 2020. “PNWColors: Color Palettes Inspired by Nature in the US Pacific Northwest.” https://doi.org/10.32614/CRAN.package.PNWColors.\n\n\nNowosad, Jakub. 2019. Check Color Palettes for Problems with Color Vision Deficiency. https://jakubnowosad.com/colorblindcheck/.\n\n\nPBC, Posit. 2025. “Quarto: An Open-Source Scientific and Technical Publishing System.” https://quarto.org/.\n\n\nPosit Software, PBC. 2025. “Download RStudio – Posit.” https://posit.co/downloads/.\n\n\nR Core Team. 2025. “The r Base Package — Index (Version 4.6.0).” Seminar für Statistik, ETH Zürich; https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html.\n\n\ntidyverse Development Team. 2025. “Tidyverse: R Packages for Data Science.” tidyverse; https://tidyverse.org/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59: 1–23.\n\n\n———. 2016. “Ggplot2: Elegant Graphics for Data Analysis.” https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse” 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. “Dplyr: A Grammar of Data Manipulation.” https://doi.org/10.32614/CRAN.package.dplyr.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "02-review-r.html#footnotes",
    "href": "02-review-r.html#footnotes",
    "title": "2  Data analysis in R",
    "section": "",
    "text": "At this point, Positron is fairly new, so it has not been widely adopted in the classroom. We anticipate that will change over time as more users move to Positron.↩︎\nUse install.packages(\"remotes\") if the remotes package is not installed.↩︎\nThere are 152 penguins in the Adelie species.↩︎\nOnly the first 10 observations are displayed.↩︎\npenguins |&gt; filter(island == \"Biscoe\") |&gt; arrange(desc(bill_length_mm)) |&gt; select(species, bill_length_mm) . The species is Gentoo and the bill length is 59.6 mm.↩︎\nThis function can also be spelled summarise().↩︎\nCode to compute median bill length by species and island: penguins |&gt; group_by(sex, species) |&gt; summarize(median_bill_length = median(bill_length_mm, na.rm = TRUE)) . The median bill length for Adelie penguins on Dream island is 38.55 mm.↩︎\nType colors() in R to see the full list of built-in colors.↩︎\n\n\nInclude fill = \"cyan\" in geom_boxplot() .\nInclude fill = species in aes().\n\n\n↩︎\n\nggplot(data = penguins, aes(x = species, y = flipper_length_mm, fill = species)) + geom_boxplot() + facet_wrap(vars(year))↩︎\nIn fact, this book was written using Quarto in RStudio!↩︎\nQuarto supports other coding languages such as Python and Julia.↩︎",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data analysis in R</span>"
    ]
  },
  {
    "objectID": "03-eda.html",
    "href": "03-eda.html",
    "title": "3  Exploratory data analysis",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#learning-goals",
    "href": "03-eda.html#learning-goals",
    "title": "3  Exploratory data analysis",
    "section": "",
    "text": "Describe the distribution of an individual variable using visualizations and summary statistics\nDescribe the relationship between two or more variables using visualizations and summary statistics\nIdentify outliers and other interesting features in the data\nExplain how outliers and other unusual features may impact analysis results",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#sec-intro-recipes",
    "href": "03-eda.html#sec-intro-recipes",
    "title": "3  Exploratory data analysis",
    "section": "\n3.1 Introduction: Recipes",
    "text": "3.1 Introduction: Recipes\nAllrecipes  (allrecipes.com) is a popular cooking website visited by over 60 million users each month (Allrecipes 2025). The site is most known for providing recipes and tips making it easy to cook a variety of dishes from around in the world at home. The recipes are primarily submitted by non-professional home cooks, and users can rate and review the recipes. Each week, over 200 new recipes are added to the site and over 2000 recipes receive new ratings (Allrecipes 2025).\nThe analysis in this chapter will focus on 2218 recipes that were posted on Allrecipes between February 2009 and July 2025. In this data, a recipe is “posted” if it is newly published or updated. The data were adapted from the cuisines data frame in the tastyR R package (Mubia 2025a). The data in cuisines were originally scraped from Allrecipes by data scientist Brian Mubia (Mubia 2025b). The tastyR package was featured as part of TidyTuesday (Community 2024), the weekly data visualization challenge, in September 2025.\nThe data are in recipes.csv. We will use the following variables: \n\ncountry: The country / region the cuisine is from (Brazilian, Filipino, French, Italian, Other). This is a simplified version of the country variable in the cuisines data frame.\nauthor: User who posted the recipe\nchef_john: Indicator of whether recipe was posted by American Chef John Mitzewich (known as Chef John), derived from the variable author. (1: Posted by Chef John, 0: Not posted by Chef John)\nmonth: Month in which the recipe posted, derived from the variable date_published in the cuisines data frame. (1: January, 2: February, 3: March, . . ., 12: December).\n\nseason: Season in which the recipe was posted, derived from variable date_published and based on the definition of seasons in National Centers for Environmental Information (NCEI) (2025).)\n\nSpring: Months 3, 4, 5\nSummer: Months 6, 7, 8\nFall: Months 9, 10, 11\nWinter: Months 12, 1, 2\n\n\ncalories: Number of calories per serving\nprotein: Amount of protein per serving (in grams)\navg_rating: Average rating (1: worst to 5: best)\ntotal_ratings: Number of ratings\nreviews: Number of reviews\nprep_time: Preparation time (in minutes)\ncook_time: Cooking time (in minutes)\n\ntotal_time: Total time to make dish (in minutes).\n\nNote this is often prep_time + cook_time but not always. This could include other factors such as marination time or additional waiting periods.\n\n\nservings: Number of servings\n\nSee the tastyR documentation (Mubia 2025a) for the full codebook.\n\nOur goal is to use exploratory data analysis to understand the distributions of key variables, the relationships between variables, and other interesting features in the data.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#purpose-of-exploratory-data-analysis",
    "href": "03-eda.html#purpose-of-exploratory-data-analysis",
    "title": "3  Exploratory data analysis",
    "section": "\n3.2 Purpose of exploratory data analysis",
    "text": "3.2 Purpose of exploratory data analysis\nThis book is about regression analysis, using statistical models to describe the relationship between a response variable and one or more predictor variables, as well as use predict values of the response variable. This chapter focuses exploratory data analysis, an important step of the workflow that happens before modeling as show Figure 1.3. Exploratory data analysis (EDA) is the process of examining the data to better understand the observations, distributions of variables, relationships between variables, and other interesting features of the data. It helps us identify observations that are far from the majority of the data, instances of missing data, and potential errors in the data . Lastly, it is where we gain initial insights about potential relationships between variables that help inform the decisions we’ll make as we do the regression analysis.\nIn his 1977 book Exploratory Data Analysis , Statistician John Tukey described EDA as “looking at data to see what it seems to say” (Tukey et al. 1977, 2:v). This short definition emphasizes an important point about EDA. It is only used to glean initial insights from the data (“what it seems to say”) not to draw conclusions. We use statistical inference (Chapter 5 and Chapter 8) to draw conclusions from the data. Though we can’t draw conclusions from EDA, it is still a critical step in th workflow. The insights gleaned from EDA help us more fully understand the implications of our results as we apply them in practice. It also helps us as data scientists better communicate the nuances in the data to collaborators and stakeholders. We typically don’t publish all the results of EDA in a final report or presentation, but the knowledge gained from EDA is infused throughout any report or presentation as we describe the data, interpret results, and share conclusions. Simply put,\n\n“Exploratory and confirmatory [statistical inference] can – and should - proceed side by side.” (Tukey et al. 1977, 2:vii)\n\n\n\n\n\n\n\n\n\nFigure 3.1: Exploratory data analysis workflow\n\n\nFigure 3.1 shows the workflow for exploratory data analysis. We typically start by getting a general sense of the data, then explore more detailed relationships in the data. \n\nClean data\nExplore individual variables\nExplore relationships between two variables\nExplore relationships between three or more variables\n\nIn each step of the workflow, we will also make note of outliers, observations that are far from the majority of the data. The remainder of this chapter follows the progression outlined above, but the workflow may not be as linear in practice. In fact, some data cleaning may occur based on what is learned at each subsequent step. The goal is to explore, and thus use judgment and creativity to move around the data and guide the exploration.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#sec-initial-check",
    "href": "03-eda.html#sec-initial-check",
    "title": "3  Exploratory data analysis",
    "section": "\n3.3 Initial data cleaning",
    "text": "3.3 Initial data cleaning\nAfter loading the data into software, the first step of the EDA workflow (Figure 3.1) is an initial check to get an overall view of the data and some initial data cleaning. The purpose of this check is to see whether the data loaded into the software as expected, identify what the rows and columns represent, and understand the variable types and their formats. This step is important, because it allows us to begin data cleaning and make necessary changes to the data and variable formats before moving too far into the analysis.\nStatistical software typically has functionality to view a summary of the data. For example, the glimpse() function in the tidyverse R package (Wickham et al. 2019) provides a summary (or “glimpse”) of the data, like the one shown in Table 3.1. We can often view data in its original format (e.g., .xlsx or .csv) before loading it into the software; however, we still need to check the data after loading it into the software in case of information loss or other problems.\n\n\nTable 3.1: Summary of recipes data\n\n\nRows: 2,218\nColumns: 22\n$ name           &lt;chr&gt; \"Saganaki (Flaming Greek Cheese)\", \"Coney Island Knishe…\n$ country        &lt;chr&gt; \"Other\", \"Other\", \"Other\", \"Other\", \"Other\", \"Other\", \"…\n$ url            &lt;chr&gt; \"https://www.allrecipes.com/recipe/263750/flaming-greek…\n$ author         &lt;chr&gt; \"John Mitzewich\", \"John Mitzewich\", \"CHIPPENDALE\", \"Hei…\n$ date_published &lt;date&gt; 2024-02-07, 2024-11-26, 2022-07-14, 2025-01-31, 2025-0…\n$ ingredients    &lt;chr&gt; \"1 (4 ounce) package kasseri cheese, 1 tablespoon water…\n$ calories       &lt;dbl&gt; 391, 301, 64, 106, 449, 958, 378, 90, 157, 322, 4, NA, …\n$ fat            &lt;dbl&gt; 25, 17, 3, 9, 23, 24, 10, 5, 6, 16, 0, NA, 21, 2, 66, 8…\n$ carbs          &lt;dbl&gt; 15, 31, 9, 7, 58, 144, 59, 10, 25, 39, 1, NA, 16, 63, 7…\n$ protein        &lt;dbl&gt; 16, 7, 1, 1, 7, 46, 14, 1, 2, 7, 0, NA, 28, 6, 54, 17, …\n$ avg_rating     &lt;dbl&gt; 4.8, 4.6, 4.3, 5.0, 3.8, 4.4, 4.3, NA, 4.6, 5.0, 4.7, 4…\n$ total_ratings  &lt;dbl&gt; 25, 10, 126, 1, 13, 40, 3, NA, 65, 2, 182, 2, 19, 16, 9…\n$ reviews        &lt;dbl&gt; 22, 9, 104, 1, 11, 32, 3, NA, 55, 2, 138, 2, 15, 16, 84…\n$ prep_time      &lt;dbl&gt; 10, 30, 20, 10, 30, 30, 30, 40, 0, 5, 5, 5, 10, 10, 20,…\n$ cook_time      &lt;dbl&gt; 5, 75, 15, 0, 15, 165, 75, 30, 0, 5, 0, 25, 10, 50, 16,…\n$ total_time     &lt;dbl&gt; 15, 180, 180, 10, 45, 675, 585, 155, 0, 10, 5, 30, 50, …\n$ servings       &lt;dbl&gt; 2, 16, 12, 6, 15, 6, 6, 84, 24, 1, 21, 8, 4, 10, 4, 8, …\n$ year           &lt;dbl&gt; 2024, 2024, 2022, 2025, 2025, 2022, 2023, 2020, 2025, 2…\n$ month          &lt;dbl&gt; 2, 11, 7, 1, 2, 8, 12, 6, 1, 6, 8, 9, 12, 9, 12, 7, 9, …\n$ days_on_site   &lt;dbl&gt; 541, 248, 1114, 182, 164, 1085, 598, 1869, 192, 60, 106…\n$ chef_john      &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ season         &lt;chr&gt; \"Winter\", \"Fall\", \"Summer\", \"Winter\", \"Winter\", \"Summer…\n\n\n\n\nTable 3.1 shows a summary of the recipes data. The rows contain the observations (also known as cases) in the data and the columns contain the variables that describe each observation. There are 2218 rows (observations) and 22 columns (variables) in the data. Each row represents a recipe posted on Allrecipes and each column is a feature or characteristic of the recipe. At this point, we can ask ourselves if these are the expected number of rows and columns based on the original data source. If not, we need to figure out the discrepancy in the number of rows and/or columns and address it before moving forward.\nOnce we understand the structure of the data, we look at the individual variables more closely. At this point, the primary focus is on the variable type, but we may also notice if there are any missing values (typically coded as NA) and anything else notable about the variable that we want to explore further.\nThere are three types of variables: quantitative variables, categorical variables, and identifier variables. We typically assign the variable type based on the description and how the observations appear in the data. For example, we typically think of a variable such as age, an individual’s age in years, as quantitative. This is the case if the values are numeric values such as 18, 24, 35,.... However, age is a categorical variable if is defined as age ranges, such as 18-25, 26-30, 31-35,.... It is important to know the variable type, because that informs how the variable is handled in the regression analysis.\n\n\nWhat type of variable is month?\nWhat is the more reliable identifier variable - name or url?1\n\n\nIn addition to knowing the variable type, we also need to know how it is stored in the software. When the data set is loaded into the software, the software stores each column (the variables) as a specific format. Typically, this column format aligns with the variable type (this is what we want!), but occasionally the format does not align with the variable type. Therefore, the next step of the initial data check is to make sure the column formats are as we expect and address any mismatches. \nIn general, quantitative variables are stored in columns with double / floating point number formats, which are formats for columns that contain real numbers with decimals. In the output above, this is coded by the software as dbl, but you may also see this denoted by float64 or similar codes. The software treats columns with this format as numeric data. Examples of these columns in the recipes data set are avg_rating and prep_time. These are both quantitative variables and are being stored in the double/floating point number format in the software. Therefore, the column format aligns with the variable type and will be treated as we expect by the software as we do the analysis.\nColumns that contain categorical data are typically stored as character or factor formats. The software treats both of these formats as text rather than numeric data. The primary difference between character and factor types is that the data stored as factors also have an ordering applied. Let’s take a look at some categorical variables in the recipe data . The variables country and and author are two categorical variables that are correctly stored in the software as character (&lt;chr&gt;) data formats.\n\n\n\nOne of the most common mismatches between the variable’s type and how it is stored occurs when categorical variables are stored in the software under numeric formats. From Table 3.1, we see this has occurred with the variables month and chef_john. This often happens when the categories are represented by numbers (e.g., “1” instead of “January”). Though we know they are categorical from Section 3.1, they are currently stored as double column types by the software. This means the the software thinks we can do calculations, such as find the average or standard deviation; however, these types of operations would not make sense in the context of month, for example. This mismatch can become particularly troublesome in the regression models. As we’ll see in Chapter 7, quantitative and categorical variables are handled differently in regression models.\nWe address these mismatches before moving forward with the EDA by changing the format in which such variables are stored. Once we have made changes to the data, we examine the summary of the data again to check the changes we’ve made and address any remaining issues. In Table 3.2, we see that chef_john and month are now correctly stored as factors, a format suitable for categorical variables. We are now ready to move on to the next step and explore the distributions of individual variables.  \n\n\nTable 3.2: Summary of recipes data with correct formats for chef_john and month\n\n\nRows: 2,218\nColumns: 22\n$ name           &lt;chr&gt; \"Saganaki (Flaming Greek Cheese)\", \"Coney Island Knishe…\n$ country        &lt;chr&gt; \"Other\", \"Other\", \"Other\", \"Other\", \"Other\", \"Other\", \"…\n$ url            &lt;chr&gt; \"https://www.allrecipes.com/recipe/263750/flaming-greek…\n$ author         &lt;chr&gt; \"John Mitzewich\", \"John Mitzewich\", \"CHIPPENDALE\", \"Hei…\n$ date_published &lt;date&gt; 2024-02-07, 2024-11-26, 2022-07-14, 2025-01-31, 2025-0…\n$ ingredients    &lt;chr&gt; \"1 (4 ounce) package kasseri cheese, 1 tablespoon water…\n$ calories       &lt;dbl&gt; 391, 301, 64, 106, 449, 958, 378, 90, 157, 322, 4, NA, …\n$ fat            &lt;dbl&gt; 25, 17, 3, 9, 23, 24, 10, 5, 6, 16, 0, NA, 21, 2, 66, 8…\n$ carbs          &lt;dbl&gt; 15, 31, 9, 7, 58, 144, 59, 10, 25, 39, 1, NA, 16, 63, 7…\n$ protein        &lt;dbl&gt; 16, 7, 1, 1, 7, 46, 14, 1, 2, 7, 0, NA, 28, 6, 54, 17, …\n$ avg_rating     &lt;dbl&gt; 4.8, 4.6, 4.3, 5.0, 3.8, 4.4, 4.3, NA, 4.6, 5.0, 4.7, 4…\n$ total_ratings  &lt;dbl&gt; 25, 10, 126, 1, 13, 40, 3, NA, 65, 2, 182, 2, 19, 16, 9…\n$ reviews        &lt;dbl&gt; 22, 9, 104, 1, 11, 32, 3, NA, 55, 2, 138, 2, 15, 16, 84…\n$ prep_time      &lt;dbl&gt; 10, 30, 20, 10, 30, 30, 30, 40, 0, 5, 5, 5, 10, 10, 20,…\n$ cook_time      &lt;dbl&gt; 5, 75, 15, 0, 15, 165, 75, 30, 0, 5, 0, 25, 10, 50, 16,…\n$ total_time     &lt;dbl&gt; 15, 180, 180, 10, 45, 675, 585, 155, 0, 10, 5, 30, 50, …\n$ servings       &lt;dbl&gt; 2, 16, 12, 6, 15, 6, 6, 84, 24, 1, 21, 8, 4, 10, 4, 8, …\n$ year           &lt;dbl&gt; 2024, 2024, 2022, 2025, 2025, 2022, 2023, 2020, 2025, 2…\n$ month          &lt;fct&gt; 2, 11, 7, 1, 2, 8, 12, 6, 1, 6, 8, 9, 12, 9, 12, 7, 9, …\n$ days_on_site   &lt;dbl&gt; 541, 248, 1114, 182, 164, 1085, 598, 1869, 192, 60, 106…\n$ chef_john      &lt;fct&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ season         &lt;fct&gt; Winter, Fall, Summer, Winter, Winter, Summer, Winter, S…",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#sec-univar-eda",
    "href": "03-eda.html#sec-univar-eda",
    "title": "3  Exploratory data analysis",
    "section": "\n3.4 Distribution of a single variable",
    "text": "3.4 Distribution of a single variable\nUnivariate exploratory data analysis is the exploration of individual variables. The goal is to better understand the variables in the data before exploring at relationships between variables. This is also where we begin to see how the observations in our sample data compare to the population of interest by identifying ways in which the sample data are representative of the population and ways it differs. This helps us understand the scope of inferential conclusions we can draw and potential limitations of the analysis. Univariate EDA is also where we can more clearly see outliers and missing data, and continue cleaning data, as needed.\n\n3.4.1 Categorical variable\nThe univariate distribution for a categorical variable includes the levels (or categories) of the variable and the number of observations or proportion of observations at each level. We examine distributions of categorical variables using visualizations and frequency tables.\nLet’s examine the distribution of month, the month a recipe was posted, and get some initial insight into the times of year people generally post recipes to the website.\nVisualize the distribution\nA bar chart  is a commonly used visualization for univariate categorical distributions. A bar chart has one bar for each level, such that the height of the bar represents the frequency, the number of observations at that level. Figure 3.2 is the bar chart for month. There is a bar for each month, and the height of the bar represents the number of recipes posted that month.\n\n\n\n\n\n\n\n\nFigure 3.2: Bar chart for the distribution of month\n\n\n\n\nAs we look at Figure 3.2, one thing that immediately stands out is the large number of recipes in November. We are unable to determine the reason for this large spike from the plot alone, but we may have some hypotheses about why this is the case based on our understanding the site’s users and our collaborators’ subject-matter expertise. One hypothesis is that there is a large uptick in recipes during the holiday season in the United States, Thanksgiving in particular, because it is a United States holiday in November that traditionally includes a large meal.\nThe bar chart in Figure 3.2 displays the frequencies, but perhaps we are more interested in visualizing the proportions, also called relative frequencies. In that case we can (1) remake the bar plot such that that height of the bars represent the proportions rather than frequencies (Figure 3.3 (a)), (2) make a pie chart, or (3) make a waffle chart.\n\n\n\n\n\n\n\n\n\n(a) Bar chart\n\n\n\n\n\n\n\n\n\n(b) Pie chart\n\n\n\n\n\n\n\n\n\n\n\n(c) Waffle chart\n\n\n\n\n\n\nFigure 3.3: Visualizations of distribution of month with proportions\n\n\nIn Figure 3.3 (b), the distribution of month is visualized using a pie chart. In a pie chart, each slice represents the proportion of observations at a given level. As we observed from the bar chart, the slice for Month 11 (November) is the largest, indicating the large proportion of recipes in the data were published that month.\nIn Figure 3.3 (c), the distribution of month is visualized using a waffle chart. In a waffle chart, the number of squares for each level represents the proportion of observations at that level. It is similar to a pie chart in that is shows how the parts (levels) make up the whole (variable). In fact, it is sometimes referred to as a “square pie chart”.  A nice feature of the waffle chart is that we can count the squares to approximate the proportion of observations at each level.\nIn general, we can use any of these charts to visualize a univariate distribution of a categorical variable. As we see in Figure 3.3, the pie chart and waffle chart can be challenging to read when there are many levels. It can be hard to compare the size of the slices in the pie chart, and distinguish between the many colors on both charts. Therefore, it is preferable to reserve these for categorical variables with fewer levels.\nSummarize the distribution\nIn addition to visualizations, we can make a frequency table, a table of the number and proportion of observations at each level.\n\n\n\nTable 3.3: Frequency table for month\n\n\n\n\nmonth\nn\nprop\n\n\n\n1\n230\n0.104\n\n\n2\n211\n0.095\n\n\n3\n155\n0.070\n\n\n4\n194\n0.087\n\n\n5\n108\n0.049\n\n\n6\n221\n0.100\n\n\n7\n188\n0.085\n\n\n8\n175\n0.079\n\n\n9\n107\n0.048\n\n\n10\n83\n0.037\n\n\n11\n394\n0.178\n\n\n12\n152\n0.069\n\n\n\n\n\n\n\n\nFrom Table 3.3, we not only see that Month 11 (November) is the most popular month for posting recipes, but we see more specifically that about 17.8% of the recipes in the data were posted that month. We can also more easily identify the least common month to post recipes, Month 10 (October). Only about 3.7% of the recipes in the data were posted that month.\nExample: Distribution of season\n\nLet’s use the visualizations and summaries introduced in this section to describe the distribution of the season in which a recipe was posted.\n\n\n\n\n\n\n\n\n\n(a) Bar chart\n\n\n\n\n\n\n\n\n\n(b) Pie chart\n\n\n\n\n\n\n\n\n\n\n\n(c) Waffle chart\n\n\n\n\n\n\nFigure 3.4: Distribution of season\n\n\n\n\n\nTable 3.4: Frequency table of season\n\n\n\n\nseason\nn\nprop\n\n\n\nWinter\n593\n0.267\n\n\nSpring\n457\n0.206\n\n\nSummer\n584\n0.263\n\n\nFall\n584\n0.263\n\n\n\n\n\n\n\n\n\nUse Figure 3.4 and Table 3.4 to describe the distribution of season.2\n\nBoth month and season provide information about when individuals post recipes to the site, so we wouldn’t necessarily need both variables in later stages of the analysis. An advantage to season is that it has fewer levels, so visualizations are easier to interpret, particularly the pie chart and waffle chart, compared to month (Figure 3.3). It will also be easier to interpret as we look at relationships between variables. An advantage to month is that we get the more specific detail, such as the large number of recipes posted in November. In practice, we could refer back to the analysis objective to determine which variable to use in later steps.\n\n3.4.2 Quantitative variable\nAs with categorical variables, we examine the univariate distributions of quantitative variables using visualizations and summary statistics. The description of the distribution of a quantitative variable includes the following components: shape, center, spread, and the presence of notable features such as outliers. Let’s examine the distribution of avg_rating, a recipe’s average user rating. The ratings range from 1 to 5, with 1 being the worst and 5 being the best.\nVisualize the distribution\nA histogram is commonly used to visualize the distribution of a quantitative variable. In a histogram, the values of the variable are divided into ranges of equal width (called bins), and there is one bar on the graph for each bin. The height of the bar represents the number of observations that have values within the bin. It is similar to a bar chart, but the bars represent a range of values instead of individual levels.\n\n\n\n\n\n\n\nFigure 3.5: Histogram of avg_rating\n\n\n\n\nFigure 3.5 is the histogram of avg_rating. We now clearly see that a vast majority of the recipes have average ratings between 4 and 5. There are very few recipes with ratings less that 3, and at least one outlier with a an average rating around 1.\n\nThe width of the bins on a histogram can make it easier or harder to see the features of the distribution. In general, the default bin width set by the statistical software is a good choice. In some cases, however, we may wish to change the width to illuminate more detail in the distribution (decrease bin width) or smooth out the distribution (increase bin width).\n\n\n\n\n\n\n\n\n\n(a) Default\n\n\n\n\n\n\n\n\n\n(b) Small\n\n\n\n\n\n\n\n\n\n(c) Large\n\n\n\n\n\n\nFigure 3.6: Different histogram bin widths\n\n\nFigure 3.6 shows examples of the default bin width, very small bin widths, and very large bin widths. Small bin widths adds more detail the histogram. This can make the histogram more challenging to read without adding much useful information for the analysis. In contrast, large bin widths can aggregate the data so much that we lose key information about the distribution.\nIn general, it is good practice to start with the default bins set by software and make small adjustments, if needed.\n\nFigure 3.7 shows two commonly used alternatives to histograms, density plots and boxplots. A density plot (Figure 3.7 (a)) is often thought of as a “smoothed out histogram”. Similar to a histogram, portions of the distribution with larger heights indicate the parts of the distribution where more observations occur. In general, we are not concerned with the exact values from the density plot, but rather we use the plot for a summary view of the distribution.\nA boxplot (Figure 3.7 (b)) is a visualization that highlights the quartiles and outliers of a distribution. The left line of the box is the first quartile, \\(Q_1\\), which marks the \\(25^{th}\\) percentile. The middle line in the box is the median, \\(Q_2\\), which marks the \\(50^{th}\\) percentile. The right line of the box is the third quartile, \\(Q_3\\), which marks the \\(75^{th}\\) percentile. The points on the graph represent outliers.\nThese visualizations are most useful for describing the shape of a distribution and for identifying outliers. Some features are more apparent on one type of visualization compared to others, so we use the goals of the EDA to help choose which visualization(s) to use.\n\n\n\n\n\n\n\n\n\n(a) Density plot\n\n\n\n\n\n\n\n\n\n(b) Boxplot\n\n\n\n\n\n\nFigure 3.7: Density plot and boxplot of avg_rating\n\n\nDescribe the distribution: Shape\nThe description of shape includes the skewness and number of modes.\n\n\n\n\n\n\n\n\n\n(a) Left-skewed\n\n\n\n\n\n\n\n\n\n(b) Right-skewed\n\n\n\n\n\n\n\n\n\n(c) Symmetric\n\n\n\n\n\n\nFigure 3.8: Skewness of distribution\n\n\nThe skewness is described as left-skewed, right-skewed, or symmetric. Figure 3.8 shows examples of a distribution with each type of skewness. If a distribution has a tail, a portion of the distribution that covers a wide range of values with very few observations, then the direction of the skewness is determined by the direction of the tail. Figure 3.8 (a) shows a left-skewed distribution (also called negatively-skewed). The majority of the observations take values at the higher end of the range, and the tail extends out to the lower values (the left side) of the distribution. An example of a left-skewed distribution is a typical distribution of online ratings for a product. Most online ratings tend to be very high, with few people giving low ratings, as we have observed with avg_rating.\nFigure 3.8 (b) is an example of a right-skewed distribution (also called positively-skewed). The majority of the observations take values at the lower end of the range, and the tail extends out to the higher values (the right side) of the distribution. An example of a right-skewed distribution is a distribution of annual income among adults in the United States. The income for most adults lie within a particular range, and a few adults have annual incomes far greater than the majority of the population (e.g., professional athletes, celebrities, etc.)\nFigure 3.8 (c) is an example of a symmetric distribution. If we divide the distribution down the middle, both sides of the distribution are approximately the same. Another way to think about it is if we fold the distribution along the center, the left side would fit almost perfectly on the right side. An example of a symmetric distribution is the distribution of shoe size among adults. A majority of individuals have a shoe size around the center of the distribution, and a small number of individuals have very small or very large shoe sizes.\n\n\n\n\n\n\n\n\n\n(a) Unimodal\n\n\n\n\n\n\n\n\n\n(b) Bimodal\n\n\n\n\n\n\n\n\n\n(c) Multimodal\n\n\n\n\n\n\nFigure 3.9: Modality of distribution\n\n\nIn addition to the skewness, the shape of the distribution includes the modality, number of peaks. Figure 3.9 shows examples of distributions with three different modalities: unimodal, bimodal, and multimodal. Figure 3.9 (a) is an example of a unimodal distribution, a distribution with a single peak. The distribution of annual incomes for adults in the United States is an example of a unimodal distribution.\nFigure 3.9 (b) is an example of a bimodal distribution, a distribution with two peaks. Bimodal distributions generally indicate there are at least two subgroups represented in the data that we may want to take into account in the analysis. An example of a bimodal distribution is a city’s average daily temperatures in a city across a year. If the city has four distinct seasons, there will typically be a peak around the mean temperature in the fall and winter (low temperatures) and a peak around the mean temperature for spring and summer (high temperatures).\nFigure 3.9 (c) is an example of a multimodal distribution, a distribution with three or more peaks. Similar to bimodal distributions, multimodal distributions generally indicate there are multiple subgroups in the data we want to take into account i the analysis. An example of a multimodal distribution is the distribution of heights for school-aged children across a large age range, about 5 - 18 years old. There will be a peak around the mean height for elementary school children (around ages 5 - 9), a peak around the mean height middle school children (around ages 10 - 14), and a peak around the mean height for high school children (around ages 14 - 18).\n\nTo determine the modality of a plot, think about using a pencil or finger to trace along the top of the distribution. The modality, then, is the number of peaks in the traced outline.\nAs we trace the distribution, we don’t need to hit every small dip in the distribution, because these small dips are often noise in the data. Rather, we are concerned about large peaks and valleys in the distribution.\n\n\nUse Figure 3.5 and Figure 3.7 to describe the shape of the distribution of avg_rating.3\n\nSummarize the distribution\nIn addition to visualizations, we compute summary statistics to more precisely describe aspects of the distribution. The summary statistics for avg_rating are in Table 3.5.\n\n\n\nTable 3.5: Summary statistics for avg_rating\n\n\n\n\nProp Complete\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\n\n\n0.956\n4.51\n0.401\n1\n4.3\n4.6\n4.8\n5\n\n\n\n\n\n\n\nWe will primarily use these summary statistics to describe the center and spread of the quantitative distribution. We have also computed the statistic Prop Complete that is the proportion of observations that have a reported value for the variable. If the proportion is less than 1, then there are observations with missing values for the variable. We need to address the missingness before moving forward with the analysis. Strategies for dealing with missing data are in Chapter 13.\nLet’s use the summary statistics to describe the center and spread of the distribution.\nDescribe the distribution: Center\nWe use the mean (also called average) or median (also called \\(Q_2\\), the \\(2^{nd}\\) quartile, \\(50^{th}\\) percentile) to describe the center of a distribution. To determine which measure is the better representation of the center, consider the shape of the distribution and whether there are outliers. If the distribution is approximately symmetric with no (or few) outliers, then the mean is the preferred measure of center. The mean is impacted by skewness and outliers, so the median is preferred if skewness or outliers are present.\n\nWe prefer the mean over the median to describe the center of the distribution when the criteria mentioned above are met. This is because the mean is computed using all the observations in the data set. In contrast, the median is determined based on the one or two observations in the middle to the distribution. In general, we prefer statistics that use all the data if they will be representative of the distribution and not skewed by a few extreme observations.\n\nFrom Table 3.5, the mean value of avg_rating in the data is about 4.509 and the median is 4.6. The distribution of avg_rating is left-skewed and has outliers (see Figure 3.5), so the median is the more representative measure of the center of the distribution.\nDescribe the distribution: Spread\nThe standard deviation, interquartile range (IQR), and range are measures used to describe the spread of a distribution. If shape of the distribution is approximately symmetric with no (or few) outliers, the standard deviation, a measure of the average distance between each observation and the mean, is the preferred measure of the spread. Because the mean is used to to compute the standard deviation, it is impacted by skewness and outliers. When the distribution is skewed or there are outliers, the interquartile range (IQR) is the more representative measure of spread. The IQR is the difference between the \\(75^{th}\\) and \\(25^{th}\\) percentiles, \\(Q_3 - Q_1\\).\nThe range \\((\\text{max} - \\text{min})\\) should be used with caution and not reported as the only measure of spread. Because it only takes into account the minimum and maximum values, it only gives describes the spread of the extreme ends of the distribution, which is not typically the spread of the majority of the data. Additionally, it is impacted by skewness and outliers, and thus can make the spread of a distribution appear larger than what is true for the vast majority of the data.\n\nTo describe the center and spread of a distribution, use\n\nthe mean and standard deviation if there is no extreme skewness or outliers, or\nthe median and IQR if skewness or outliers are present\n\n\nFrom Table 3.5, the standard deviation of avg_rating is 0.401, so the average rating for each recipe is about 0.401 points from the mean rating of 4.509, on average. The IQR is 0.5. This means the spread of the middle 50% of the distribution, \\(Q_3 - Q_1\\) is about 0.5 points. Lastly, the range, the distance between the minimum and maximum values, is 4 .\nBecause the distribution of avg_rating is skewed with outliers, the IQR 0.5 is the most representative measure of spread.\n\nDescribe the distribution: Outliers\nThe last part of describing the distribution of a quantitative variable is the presence of outliers. Outliers can be valid observations that are very different from the rest of the data (e.g,. a professional athlete’s annual salary in the distribution of annual salaries for 1000 randomly sampled adults in the United States). Sometimes, however, they indicate errors in the data (e.g., a person’s age is recorded as 150 years old).\nOutliers are most visible in histograms (Figure 3.5) and boxplots (Figure 3.7 (b)). It is more challenging to differentiate between outliers and skewness in a density plot. From the histogram, we visually assess outliers by looking for observations that are set apart on the graph, typically very high or very low. For example, in Figure 3.5, there appears to be at least a few outliers that have low average ratings around 1 and 2.\nOn a boxplot, the outliers are marked as points on the plot. An observation is considered an outlier if it is less than \\(Q_1 - 1.5 \\times IQR\\) or greater than \\(Q_3 + 1.5 \\times IQR\\). Based on the boxplot in Figure 3.7, observations with average ratings about 3.5 or less have been identified as outliers. Based on this plot, there are no outliers on the high end of the distribution of average rating.\nIf there are outliers due to data entry errors, we need to input the correct values, if possible, or remove the observations from the analysis data. If there are a lot of data entry errors for an individual variable, we may consider removing that variable from the analysis and thus retaining more observations for the analysis. If outliers are unusual yet valid observations, there are options on how to handle them based on the analysis goals and their impact on the regression analysis results. We discuss these options further in ?sec-handle-outliers.\n\n\nThe description of the distribution of a quantitative variable includes the following:\n\nShape: Skewness and modality of distribution\nCenter: Middle of the distribution (measured by mean or median)\nSpread: How far apart the observations are (measured by standard deviation or IQR)\nOutliers: Observations that are far from the rest of the data\n\n\nExample: Distribution of cook_time\n\nLet’s use the visualizations and summary statistics introduced in the previous section to describe the distribution of cook_time, the amount of time (in minutes) it takes to cook a dish.\n\n\n\n\n\n\n\nFigure 3.10: Distribution of cook_time\n\n\n\n\n\n\n\nTable 3.6: Summary statistics for cook_time\n\n\n\n\nProp Complete\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\n\n\n1\n41.8\n63.2\n0\n10\n25\n45\n600\n\n\n\n\n\n\n\n\nUse the visualizations in Figure 3.10 and summary statistics in Table 3.6 to describe the distribution of cook_time. Include the shape, center, spread, and potential outliers in the description.4\n\nWhen a distribution has extreme outliers, such as the distribution of cook_time, the outliers can compress the majority of the observations on the graph, making it difficult to get a detailed view of the majority of the distribution. In this case, we can also make a visualization without the outliers to get a better view.\nAbout 95% of the observations have cook times of 150 minutes or less, so we create a new histogram that only includes those observations.\n\n\n\n\n\n\n\n\n\n(a) All observations\n\n\n\n\n\n\n\n\n\n(b) Cook time 150 minutes or less\n\n\n\n\n\n\nFigure 3.11: Original versus zoomed-in distribution of cook_time\n\n\nIn Figure 3.11, we compare a visualization of the full distribution with one that only includes recipes with cook time 150 minutes or less. For example, in Figure 3.11 (b), we get a lot of detail about the distribution of recipes with cook time 50 minutes or less. In contrast, all these recipes are contained in the first three bars of the histogram in Figure 3.11 (a).",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#sec-bivar-eda",
    "href": "03-eda.html#sec-bivar-eda",
    "title": "3  Exploratory data analysis",
    "section": "\n3.5 Relationship between two variables",
    "text": "3.5 Relationship between two variables\nAfter we have explored the distributions of individual variables in the univariate EDA, we explore the relationships between variables. We begin with bivariate EDA, the exploration of the relationship between two variables. In Section 3.6, we extend this to the relationship between three or more variables. There are three types of bivariate relationships: relationship between two quantitative variables, relationship between one quantitative and one categorical variable, and the relationship between two categorical variables.\n\n3.5.1 Two quantitative variables\nSimilar to univariate EDA, we use visualizations and summary statistics to explore the relationship between two variables. The components to describe the relationship between two quantitative variables include the shape, direction, and potential outliers. We use a scatterplot to visualize the relationship and the correlation to quantify it.\nVisualize the relationship\nA scatterplot is a plot for two quantitative variables, such that one variable is on the x-axis (horizontal axis), one variable is on the y-axis (vertical axis), and each observation is represented by a point. Let’s take a look at the relationship between the number of calories per serving (calories) and the total grams of protein per serving (protein). \n\n\n\n\n\n\n\nFigure 3.12: Relationship between calories and protein\n\n\n\n\nFigure 3.12 is the scatterplot with calories on the x-axis and protein on the y-axis. From the visualization, we can see the shape and direction of the relationship, along with outliers. We also get an indication of the strength of the relationship that we will quantify using summary statistics.\n\nDescribe the relationship: Shape and direction\nIn bivariate EDA, the shape is a description of the overall trend of the points in the scatterplot. Common shapes we are linear, quadratic, and logarithmic. There may also be no clear trend. These are shown in Figure 3.13. Figure 3.13 shows examples of the commonly observed shapes. When we do linear regression, we generally assume that the relationship between the response variable and the predictor variable(s) is linear as in Figure 3.13 (a). Additionally, there are methods to account for quadratic (Figure 3.13 (b)) and logarithmic (Figure 3.13 (c)) relationships in the model. A scatterplot with no clear trend as in Figure 3.13 (d) indicates there is little to no relationship between the two variables.\n\n\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n(b) Quadratic\n\n\n\n\n\n\n\n\n\n\n\n(c) Logarithmic\n\n\n\n\n\n\n\n\n\n(d) No clear shape\n\n\n\n\n\n\nFigure 3.13: Shapes of bivariate relationships\n\n\nIf the shape of the relationship is monotonic, always increasing or always decreasing, then we describe the direction of the relationship in addition to the shape. Linear (Figure 3.13 (a)) and logarithmic (Figure 3.13 (c)) trends are examples of monotonic relationships. There are three potential directions: positive, negative, and no direction. These are illustrated in Figure 3.14.\n\n\n\n\n\n\n\n\n\n(a) Positive\n\n\n\n\n\n\n\n\n\n(b) Negative\n\n\n\n\n\n\n\n\n\n(c) No direction\n\n\n\n\n\n\nFigure 3.14: Direction of bivariate relationships\n\n\nThe direction is positive Figure 3.14 (a) if one variable tends to increase as the other increases. The direction is negative Figure 3.14 (b) if one variable tends to decrease as the other increases. Lastly, there no direction Figure 3.14 (c) if there is no clear pattern in how one variable changes as the other changes.\nFrom Figure 3.12, relationship between calories and protein is positive and linear. In general, recipes with more calories per serving tend to also have more grams of protein per serving.\nDescribe the relationship: Strength\nThe strength of the relationship between two variables is a measure how closely the observations follow the overall pattern or shape. Points that are tightly clustered together indicate a stronger relationship than points that are more dispersed. Figure 3.15 shows examples of linear relationships with different strengths.\n\n\n\n\n\n\n\n\n\n(a) Strong\n\n\n\n\n\n\n\n\n\n(b) Moderate\n\n\n\n\n\n\n\n\n\n(c) Weak\n\n\n\n\n\n\nFigure 3.15: Strength of bivariate relationships\n\n\nWhen the shape between two variables is linear, we use the correlation to quantify the strength of the relationship. The correlation, denoted \\(r\\), is a measure of the strength and direction of the linear relationship between two variables. The correlation ranges from -1 to 1, with \\(r \\approx -1\\) indicating a nearly perfect negative linear relationship, \\(r \\approx 1\\) indicating a nearly perfect positive relationship, and \\(r \\approx 0\\) indicating a very weak to no linear relationship.\nThe direction of the linear relationship between two variable is indicated by the sign of the correlation. The strength is indicated by the magnitude of the correlation, \\(|r|\\).\nLet’s use the correlation to describe the strength of the linear relationship between calories and protein. The correlation between the two variables is 0.705. Based on the scatterplot in Figure 3.12 and the correlation, there is a strong, positive linear relationship between the two variables.\n\nCorrelation measures the strength of the linear relationship, so values of correlation close to 0 do not necessarily mean there is no relationship between the two variables. In the graphs below, both relationships have correlation with magnitudes around \\(|r| = 0.12\\) (0.12 and -0.12 to be exact). From the scatterplots, however, we see a clear relationship between the two variables. The relationship is quadratic and not well described by a line, so the correlation is low.\nIt is important to use visualizations alongside summary statistics to provide additional context and reveal features that may be hidden by the summary statistic alone.\n\n\n\n\n\n\n\n\n\n(a) |r| = 0.12\n\n\n\n\n\n\n\n\n\n(b) |r| = -0.12\n\n\n\n\n\n\nFigure 3.16: Relationships with low correlation\n\n\n\nDescribe the relationship: Outliers and notable features\nWhen looking at the relationship between two variables, outliers are observations fall outside the general trend of the data. Outliers may have out-sized influence when building a regression model. Therefore, it is important to identify outliers in the EDA, so we can take them into account when evaluating the results from the regression analysis. We discuss how the impact of outliers on regression modeling in Section 6.4.5.\nThere are outliers in the relationship between calories and protein in Figure 3.12. In particular, there are two observations with over 1500 calories per serving and protein close to 0 grams. These are far outside the general trend of the data, as the amount of protein is lower than expected based on the other observations with high calories per serving.\nIn addition to identifying outliers, we want to make note of other interesting features in the relationship. These features are most often observed from visualizations. For example, the variability (spread) in the values of one variable may increase as the values of the other increases. We see this in the relationship between protein and calories in Figure 3.12, as the variability in protein (as seen by the vertical spread of the points) increases as the calories increase. Patterns like this are important to identify, because they directly relate to the assumptions we make when doing linear regression (Section 5.3). Recognizing these potential issues in the EDA can help us make decisions to address potential issues in the analysis.\n\nThe description of the relationship between two quantitative variables includes the following:\n\nShape: Overall pattern or trend in the data (e.g., linear, quadratic, etc.)\nDirection: For a monotonic relationship, description of whether variables move together (positive), move opposite of one another (negative), or have no clear direction (none)\nStrength: How clustered or spread out the observations are\nOutliers: Observations that do not follow general trend of the data\nInteresting features: Other interesting features, such as increased variability in one variable as the other increases\n\n\nExample: Relationship between prep_time and avg_rating\n\nIs the amount of time it takes to prepare a dish associated with users’ opinions about the recipe? To answer this, let’s look at the relationship between prep_time and avg_rating.\n\n\n\n\n\n\n\nFigure 3.17: Relationship between prep_time and avg_rating\n\n\n\n\nIn Figure 3.17, we see there are a few recipes with very long preparation times. Based on the descriptions in the data, these outlier recipes are those that require foods to marinate before cooking. They are making it difficult to see whether there is an association between the two variables for the vast majority of the data. The \\(95^{th}\\) percentile for prep_time is 45 minutes, so we will narrow the scope of data and evaluate the relationship for those recipes that have preparation time of 45 minutes or less.\n\n\n\n\n\n\n\nFigure 3.18: Relationship between prep_time and avg_rating for recipes with prep_time &lt;= 45\n\n\n\n\n\nFigure 3.18 is the updated scatterplot for recipes with preparation time 45 minutes or less. Based on Figure 3.18, does there appear to be a relationship between prep_time and avg_rating? Explain.5\n\nSomething that may stand out on this plot is that the data appear to be organized in columns. This is not an error in the data, but rather reveals something about the way prep_time appears on the website. The preparation times are recorded in 5-minute increments for recipes with preparation time of five minutes or greater. The exact time is recorded for the few recipes with preparation time less than five minutes.\nThe next thing that stands out on Figure 3.18 is the recipes with preparation times equal to 0. This seems unusual, as we would expect some preparation for any recipe. So what is happening here? The recipes with 0 minutes for the preparation times are those that show no preparation time on the website. In other words, these are examples of missing data. We don’t know why exactly these values are missing but some possible explanations are (1) the recipe author included the preparation time in the value for cook_time, (2) the recipe author just did not include preparation time in the materials posted on the site. In practice, we need to deal with these types of observations before moving too far into the analysis and modeling. \n\n3.5.2 Quantitative and categorical variables\nNext, we examine the relationship between a quantitative variable and a categorical variable. In general, we are interested in how the distribution of the quantitative variable differs based on levels of the categorical variable. Here we discuss three plots commonly used to visualize the relationship between a quantitative and categorical variable: side-by-side boxplots, ridgeline plots, and violin plots. These are all extensions of the plots for univariate quantitative distributions, so we can rely on the observations from Section 3.4.2 as we examine these plots.\nJohn Mitzewich, more commonly known as Chef John, is an American chef who posts online content about cooking and has posted a lot of recipes on allrecipes.com. He is the author of 130 recipes on the site; the second most common author (besides anonymous authors) has 22 recipes. The variable chef_john is an indicator of whether a recipe is posted by Chef John. Let’s look at the relationship between chef_john and avg_rating to see how users’ ratings of Chef John’s recipes compare to the ratings for other authors. Figure 3.19 shows three visualizations to explore the relationship between these two variables.\n\n\n\n\n\n\n\n\n\n(a) Boxplot\n\n\n\n\n\n\n\n\n\n(b) Ridgeline plot\n\n\n\n\n\n\n\n\n\n\n\n(c) Violin plot\n\n\n\n\n\n\nFigure 3.19: Relationship between chef_john and avg_rating\n\n\nThe side-by-side boxplot in Figure 3.19 (a) shows a boxplot of the distribution of avg_rating for each level of chef_john. When evaluating whether there appears to be a relationship between the two variables, we want to see if the boxplots are relatively similar for the recipes posted by Chef John (chef_john = 1) and those posted by all other authors (chef_john = 0). In this plot, the median for Chef John’s recipes is slightly higher than the median for all other posters, though there is overlap in the middle 50% of both distributions, as shown by the overlapping boxes. There is less variability in the average ratings among Chef John’s recipes compared to the variability in the average ratings for all others. We also note that there are more extreme low outliers in the distribution of recipes posted by all other authors.\nThe ridgeline plot in Figure 3.19 (b) shows the density plot of the distribution of avg_rating for each level of chef_john. These plots are useful for comparing the center and shape of the distribution at each level. They also show how the spreads of the distributions compare. As with the side-by-side boxplots, we see there is more spread (variability) in the average ratings for recipes posted by others compared to those posted by Chef John. The number of outliers is less apparent, however, compared to side-by-side boxplot.\nLastly, the violin plot in Figure 3.19 (c) is like a combination of the boxplot and ridgeline plot. The plots show the density of avg_rating for each level of chef_john with the median marked by a horizontal line. This makes it easier to compare the center of the distributions. We can also easily compare the spreads of the distributions. As with the ridgeline plots, it is less clear which observations are outliers.\nOne thing that is not clear from the visualizations is the number of observations in each subgroup. Knowing the number of observations in each subgroup provides useful context to better understand the differences in the distributions. Therefore, in addition to visualizations, we compute the number of observations and other summary statistics introduced in Section 3.4.2 to describe the distribution of the quantitative variable at each level of the categorical variable. The summary statistics for avg_rating for each level of chef_john is shown in Table 3.7.\n\n\n\nTable 3.7: Summary statistics of avg_rating for each level of chef_john\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchef_john\nn\nProp Complete\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\n\n\n\n0\n2088\n0.955\n4.50\n0.405\n1.0\n4.30\n4.6\n4.8\n5\n\n\n1\n130\n0.969\n4.74\n0.251\n3.5\n4.62\n4.8\n4.9\n5\n\n\n\n\n\n\n\n\nWhen evaluating a potential relationship, we are primarily concerned with how the centers of the distribution for the quantitative variable compare for each level of the categorical variable. If there are large differences between the centers, then there is indication of a potential relationship between the two variables. Otherwise, if the centers are equal or relatively close, there is little to no indication of a relationship between the variables. It is worth noting again that we are only making observations about the data, not drawing conclusions at this point.\nExample: Relationship between country and avg_rating\n\nHow do users rate recipes from different countries? Figure 3.20 shows the relationship between country and avg_rating.\n\n\n\n\n\n\n\nFigure 3.20: Relationship between country and avg_rating\n\n\n\n\n\nWrite two observations from Figure 3.20. Does there appear to be a relationship between country and avg_rating?6\n\n\n3.5.3 Two categorical variables\nThe last type of bivariate relationship is the relationship between two categorical variables. We will be particularly interested in these relationships in Chapter 11, as we fit models with categorical response variables. We use visualizations along with tables of frequencies and relative frequencies to examine these relationships. There are three commonly used visualizations for the relationship between two categorical variables: grouped bar plot, segmented bar plot, and mosaic plot.\nWhen looking at the relationship between a quantitative and categorical variable, we naturally looked at the distribution of the quantitative variable by levels of the categorical variable. When we have two categorical variables, the software does not have a natural way in which to arrange the data. Therefore, we rely on the analysis objective to determine how to most effectively visualize and summarize the relationship.\nIn Section 3.1 we introduced Chef John, the most prolific poster on allrecipes.com based on the observations in our data. We’d like to explore whether he tends to post recipes at similar times of the year compared to other authors. Therefore, we will explore the relationship between season and chef_john, by looking at the distribution of season for Chef John compared to the distribution for all other authors. \nFigure 3.21 shows three different visualizations of the relationship between chef_john and season.\n\n\n\n\n\n\n\n\n\n\n(a) Grouped bar chart\n\n\n\n\n\n\n\n\n\n(b) Segmented bar plot\n\n\n\n\n\n\n\n\n\n(c) Mosaic plot\n\n\n\n\n\n\nFigure 3.21: Relationship between chef_john and season\n\n\n\nThe grouped bar plot in Figure 3.21 (a) shows a bar chart of season based on the levels of chef_john. The height of the bars represent the number of observations that take a given level of season and given level of chef_john. As we evaluate whether there is a relationship between the two variables, we look to see whether the relative bar heights across seasons is the same for recipes posted by Chef John compared to the relative bar heights for other authors. The data give some indication of a relationship between the variables if the distribution of season differs based on values of chef_john.\nFigure 3.21 (a) shows a limitation of the grouped bar chart. Because the heights of the bars represent the number of observations, these plots can be difficult to interpret when the data have a large imbalance. That is the case here, as there are 130 recipes were posted by Chef John and 2088 were posted by all other authors.\nIn a segmented bar chart (also called stacked bar chart), the bars for one variable are filled in based on the distribution of another variable. In the segmented bar chart in Figure 3.21 (b), there is a bar for each level of chef_john, and each bar is filled in based on the distribution of season. Because we are now working with proportions, we can more easily make comparisons across groups even when the data are imbalanced. If the distributions are approximately the same within each bar, then it is a indication of no relationship between the two variables. Otherwise, differences in the distributions within the bars indicate a potential relationship between the variables.\nA mosaic plot  has some of the advantages of both grouped bar plots and segmented bar plots. In a mosaic plots, the bars for grouping variable are filled in based on the distribution of the other variable, and the width of the bars are based on the number of observations at each level of the grouping variable. Figure 3.21 (b) shows the mosaic plot of season versus chef_john. Now we can not only see the distribution of season for recipes posted by Chef John and for recipes posted by all other authors, but we also see there are far fewer observations that take values chef_john = 1 compared to chef_john = 0.\nFrom the plots in Figure 3.21, we see that a larger proportion of recipes were posted by Chef John in the fall and winter seasons, compared to all other authors. The other authors posted a higher proportion of recipes in the summer compared to Chef John. These figures indicate a potential relationship between whether the recipe was posted by Chef John and the season in which it was posted.\n\nExample: Relationship between country and season\n\nIs there a relationship between the country a recipe is from and the time of year it is posted? Figure 3.22 is a segmented bar chart showing the relationship between country and season.\n\n\n\n\n\n\n\nFigure 3.22: Segmented bar chart of country versus season\n\n\n\n\n\nWrite two observations from Figure 3.22. Does there appear to be a relationship between country and season?7",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#sec-multivar-eda",
    "href": "03-eda.html#sec-multivar-eda",
    "title": "3  Exploratory data analysis",
    "section": "\n3.6 Relationship between three or more variables",
    "text": "3.6 Relationship between three or more variables\nThe next step is multivariate EDA, exploring the relationship between three or more variables. We typically limit this to the exploration of three variables, because it can become challenging to interpret and glean meaningful insights from the relationship between a large number of variables. We often use multivariate EDA when we want to explore potential interaction terms for the regression model (Section 7.7). We generally rely on visualizations for multivariate EDA. We can start with one of the bivariate visualizations introduced in Section 3.5 and further explore them based on subgroups of a third variable.\nIn Section 3.5.2, we looked at the relationship between the average rating and whether a recipe was posted by Chef John or another author. Now let’s expand on that and consider whether this relationship differs between seasons. Figure 3.23 shows side-by-side boxplots of avg_rating versus chef_john faceted (split up) by season.\n\n\n\n\n\n\n\nFigure 3.23: Multivariate EDA of avg_rating versus chef_john faceted by season\n\n\n\n\nAs we look at the visualization, the primary question to ask is whether the relationship between avg_rating and chef_john looks similar or different across the levels of season. In other words, are the boxplots the same relative to one another for each level of season. Note that we are not necessarily interested in the absolute position of the boxplots (it’s OK if some seasons generally have higher or lower ratings compared to others) but rather we are interested in the relative positioning of the boxplots within a season.\nFrom Figure 3.23, the relationship between avg_rating and chef_john is approximately the same in each season. Within each season, the median avg_rating for recipes posted by Chef John is higher compared to those posted by other authors. Additionally, there is generally a lot of overlap in the boxes within each season. The exception is the summer, but we keep in mind the very small variability (and small sample size) of recipes posted by Chef John. Based on this EDA, the relationship between avg_rating and chef_john does not appear to differ by season.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#exploratory-data-analysis-in-r",
    "href": "03-eda.html#exploratory-data-analysis-in-r",
    "title": "3  Exploratory data analysis",
    "section": "\n3.7 Exploratory data analysis in R",
    "text": "3.7 Exploratory data analysis in R\nMuch of the exploratory data analysis in this chapter is done using functions from the dplyr and ggplot2 packages. We refer the reader to Chapter 2 for a more detailed introduction to those packages. Here we will focus on the new functions that were used in this chapter: skim() in Section 3.4.2, waffle() in Section 3.4.1, geom_ridges() in Section 3.5.2, and geom_mosaic() in Section 3.5.3.\n\n3.7.1 Data summaries\nThe skim() function in the skimr R package (Waring et al. 2022) provides a summary of all the columns in a data frame or tibble. This function is particularly useful for initially checking the data Section 3.3 and univariate data analysis (Section 3.4). The code below produces a summary for each column recipes. Different types of summary output is produced based on the column’s data format.\n\nrecipes |&gt; \n  skim()\n\n\nTable 3.8: skim() output for recipes\n\n\n\nData summary\n\n\nName\nrecipes\n\n\nNumber of rows\n2218\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n1\n\n\nfactor\n3\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1\n4\n87\n0\n2216\n0\n\n\ncountry\n0\n1\n5\n9\n0\n5\n0\n\n\nurl\n0\n1\n45\n120\n0\n2218\n0\n\n\nauthor\n0\n1\n1\n35\n0\n1635\n0\n\n\ningredients\n1\n1\n29\n1109\n0\n2217\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ndate_published\n0\n1\n2009-02-09\n2025-07-29\n2024-07-14\n751\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nmonth\n0\n1\nFALSE\n12\n11: 394, 1: 230, 6: 221, 2: 211\n\n\nchef_john\n0\n1\nFALSE\n2\n0: 2088, 1: 130\n\n\nseason\n0\n1\nFALSE\n4\nWin: 593, Sum: 584, Fal: 584, Spr: 457\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ncalories\n32\n0.99\n358.41\n240.04\n3\n190.0\n319.5\n477.0\n2266\n▇▃▁▁▁\n\n\nfat\n55\n0.98\n18.76\n16.96\n0\n7.0\n15.0\n26.0\n225\n▇▁▁▁▁\n\n\ncarbs\n35\n0.98\n31.96\n26.06\n1\n13.0\n26.0\n45.0\n264\n▇▂▁▁▁\n\n\nprotein\n39\n0.98\n16.61\n16.30\n0\n4.0\n11.0\n25.0\n159\n▇▁▁▁▁\n\n\navg_rating\n97\n0.96\n4.51\n0.40\n1\n4.3\n4.6\n4.8\n5\n▁▁▁▂▇\n\n\ntotal_ratings\n97\n0.96\n85.25\n148.24\n1\n6.0\n24.0\n87.0\n997\n▇▁▁▁▁\n\n\nreviews\n108\n0.95\n76.93\n142.07\n1\n6.0\n21.0\n74.0\n975\n▇▁▁▁▁\n\n\nprep_time\n0\n1.00\n21.50\n60.72\n0\n10.0\n15.0\n25.0\n1800\n▇▁▁▁▁\n\n\ncook_time\n0\n1.00\n41.75\n63.18\n0\n10.0\n25.0\n45.0\n600\n▇▁▁▁▁\n\n\ntotal_time\n0\n1.00\n170.98\n641.73\n0\n35.0\n60.0\n120.0\n14440\n▇▁▁▁▁\n\n\nservings\n2\n1.00\n10.48\n13.42\n1\n4.0\n8.0\n12.0\n240\n▇▁▁▁▁\n\n\nyear\n0\n1.00\n2023.32\n1.75\n2009\n2022.0\n2024.0\n2025.0\n2025\n▁▁▁▁▇\n\n\ndays_on_site\n0\n1.00\n643.15\n637.64\n3\n207.0\n382.5\n1002.8\n6017\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nIf we are only interested in the summary for particular variables, we specify them in the skim() function. The code below produces the summaries for avg_rating and season.\n\nrecipes |&gt;\n  skim(avg_rating, season)\n\n\nTable 3.9: skim() output for avg_rating and season\n\n\n\nData summary\n\n\nName\nrecipes\n\n\nNumber of rows\n2218\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nseason\n0\n1\nFALSE\n4\nWin: 593, Sum: 584, Fal: 584, Spr: 457\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\navg_rating\n97\n0.96\n4.51\n0.4\n1\n4.3\n4.6\n4.8\n5\n▁▁▁▂▇\n\n\n\n\n\n\n\nThe output from skim() is a tibble, so we can apply the usual dplyr functions to the results. For example, we may not be interested in all the summary output, so we can use select() to choose certain columns. The code below shows the mean and standard deviation for avg_rating. The column names from the skim() output differ slightly from what is displayed, so use glimpse() or names() to see the underlying column names.\n\nrecipes |&gt; \n  skim(avg_rating) |&gt;\n  select(numeric.mean, numeric.sd)\n\n# A tibble: 1 × 2\n  numeric.mean numeric.sd\n         &lt;dbl&gt;      &lt;dbl&gt;\n1         4.51      0.401\n\n\n\n3.7.2 Waffle charts\nWe introduced waffle charts in Section 3.4.1 to explore the univariate distribution of a categorical variable. Waffle charts are produced using geom_waffle() in the waffle R package (Rudis and Gandy 2023). The functions in this package are developed to work within the ggplot2 framework, so we can use all the ggplot functions from Section 2.6.2 to customize the charts.\nThe code to create a waffle chart for the season is below. We begin by using count() to compute the number of observations at each level. Inside geom_waffle(), the argument flip = TRUE arranges the level of season by rows, rather than the default arrangement by columns. The argument make_proportional = TRUE produces a chart such that the number of squares is approximately equal to the proportion of observations at each level, rather than the raw number of observations. Lastly, theme_enhance_waffle() removes unnecessary axis labels.\n\nrecipes |&gt;\n  count(season) |&gt;\n  ggplot(aes(fill = season, values = n)) +\n  geom_waffle(flip = TRUE, make_proportional = TRUE) +\n  theme_enhance_waffle() +\n  scale_fill_manual(values = sunset4)\n\n\n\n\n\n\n\n\n3.7.3 Ridgeline plots\nIn Section 3.5.2, we introduced ridgeline plots to explore the relationship between a quantitative and categorical variable. We use geom_density_ridges() in the ggridges R package (Wilke 2025) to create the plots. Similar to ggwaffle, the functions in this package are developed to work within the ggplot2 framework, so we can utilize the functions from Section 2.6.2 to customize the plots.\nThe code for the ridgeline plot of the relationship between avg_rating and chef_john is below.\n\nggplot(data = recipes, aes(x = avg_rating, y = chef_john)) + \n  geom_density_ridges() \n\n\n\n\n\n\n\nWe customize the ridgeline plot by filling in the color of the densities based on chef_john, updating the labels, and applying a new theme to the visualization.\n\nggplot(data = recipes, aes(x = avg_rating, y = chef_john, fill = chef_john)) + \n  geom_density_ridges() +\n  labs(x = \"Average rating\", \n       y = \"Chef John\", \n       fill = \"Chef John\") + \n  theme_bw() +\n  scale_fill_manual(values = sunset2)\n\n\n\n\n\n\n\n\n3.7.4 Mosaic plots\nWe introduced the mosaic plot for examining the relationship between two categorical variables in Section 3.5.3. Mosaic plots are created using geom_mosaic() in the ggmosaic R package (Jeppson, Hofmann, and Cook 2021).\nBelow is a mosaic plot of chef_john versus season. The aes() function to define the aesthetics must be an argument of geom_mosaic(). Additionally, the x aesthetic is defined as the product of the two categorical variables of interest.\n\nggplot(data = recipes) +\n  geom_mosaic(aes(x = product(season, chef_john), fill = season))\n\n\n\n\n\n\n\nSimilar to the packages for waffle plots and ridgeline plots, we can customize mosaic plots using the functions in Section 2.6.2. In the code below, we update the colors using a palette from the viridis R package (Garnier et al. 2024).\n\nlibrary(viridis)\n\nggplot(data = recipes) +\n  geom_mosaic(aes(x = product(season, chef_john), fill = season)) +\n  scale_fill_viridis_d()",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#summary",
    "href": "03-eda.html#summary",
    "title": "3  Exploratory data analysis",
    "section": "\n3.8 Summary",
    "text": "3.8 Summary\n\nExploratory data analysis workflow:\n\nClean data\nExplore individual variables\nExplore relationships between two variables\nExplore relationships between three or more variables\n\n\nIn this chapter we introduced the exploratory data analysis, the step in the data science workflow when we begin to clean and explore the data. This is an important step in a regression analysis, because having a clear understanding of the data helps inform the decisions we make throughout the analysis process. It is also the point at which we may identify errors or missing values in the data. We conduct EDA by using visualizations and summary statistics to get an overview of the data, explore the distributions of individual variables, explore the relationships between two variables, and explore the relationships between three or more variables.\nThe remainder of the text focuses on regression analysis, methods for modeling the relationship between a response variable and one or more predictor variables. Given the importance of EDA in every analysis, each chapter will begin with a short exploration of key variables and relationships in the analysis. The EDA in these sections are is meant to provide context about the data, equipping us to more fully interpret and draw conclusions from the regression analysis results. Because the focus of each chapter is the regression analysis method, the EDA will be brief and focused, similar to what might be presented in a final presentation or report. In practice, the EDA is more in-depth, particularly when working with data sets that have a large number of variables.\nWe begin with simple linear regression in Chapter 4, using regression analysis to model the relationship between the response variable and one predictor variable. This will provide the foundation for the more complex models introduced in Chapter 7.\n\n\n\n\nAllrecipes. 2025. “About Us.” https://www.allrecipes.com/about-us-6648102.\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2024. viridis(Lite) - Colorblind-Friendly Color Maps for r. https://doi.org/10.5281/zenodo.4679423.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. “Ggmosaic: Mosaic Plots in the ’Ggplot2’ Framework.” https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nMubia, Brian. 2025a. “tastyR: Recipe Data from ’Allrecipes.com’.” https://doi.org/10.32614/CRAN.package.tastyR.\n\n\n———. 2025b. “I Scraped 14K Recipes, so You Won’t Have To.” August 1, 2025. https://www.brians.works/i-scraped-14k-recipes-so-you-wont-have-to/.\n\n\nNational Centers for Environmental Information (NCEI). 2025. “Meteorological Versus Astronomical Seasons.” https://www.ncei.noaa.gov/news/meteorological-versus-astronomical-seasons.\n\n\nRudis, Bob, and Dave Gandy. 2023. “Waffle: Create Waffle Chart Visualizations.” https://github.com/hrbrmstr/waffle.\n\n\nTukey, John Wilder et al. 1977. Exploratory Data Analysis. Vol. 2. Springer.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. “Skimr: Compact and Flexible Summaries of Data.” https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse” 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilke, Claus O. 2025. “Ggridges: Ridgeline Plots in ’Ggplot2’.” https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "03-eda.html#footnotes",
    "href": "03-eda.html#footnotes",
    "title": "3  Exploratory data analysis",
    "section": "",
    "text": "Month is a categorical variable, because each number represents a specific month. The more reliable identifier variable is url. There is one recipe per page, and multiple pages cannot have the same URL. Therefore, the URL uniquely identifies the recipe. Though unlikely, it is possible for multiple recipes to have the same name. This actually occurs for two names in the data “Cajun Chicken and Sausage Gumbo” and “Chicharrones de Pollo”.↩︎\nIn general, there number of recipes posted is approximately equal across the winter, summer, and fall seasons. The smallest proportion of recipes are posted during the spring season.↩︎\nThe shape of the distribution of avg_rating is skewed left and unimodal.↩︎\nThe distribution of cook_time is unimodal and right-skewed. The center is best represented by the median, of 25 minutes. The spread is best represented by the IQR, of 35 minutes (45 - 10). There are some clear outliers with cook times of about 350 minutes and greater (the max is 600).↩︎\nThere does not appear to be a relationship between the average rating and the preparation time. There is not a clear trend on the plot.↩︎\nExample observations: (1) There is the least variability in the avg_rating for French recipes. (2) The median avg_rating seems to be similar for all countries.\nThere does not appear to be a relationship between country and avg_rating. The median avg_ratingis similar across countries, and there is a lot of overlap in the distributions of avg_rating↩︎\nExample observations: (1) A larger proportion of Filipino and French recipes are posted in the fall compared to recipes from other countries. (2) A much smaller proportion of Filipino recipes are posted in the spring compared to other countries.\nThere does appear to be a relationship between season and country . There are differences in the distribution of season across countries.↩︎",
    "crumbs": [
      "Part 1: Getting started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "04-slr.html",
    "href": "04-slr.html",
    "title": "4  Simple linear regression",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#learning-goals",
    "href": "04-slr.html#learning-goals",
    "title": "4  Simple linear regression",
    "section": "",
    "text": "Use exploratory data analysis to assess whether a simple linear regression is an appropriate model to describe the relationship between two variables\nEstimate the slope and intercept for a simple linear regression model\nInterpret the slope and intercept in the context of the data\nUse the model to compute predictions and residuals\nEvaluate model performance using RMSE and \\(R^2\\)\n\nConduct simple linear regression using R",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-intro",
    "href": "04-slr.html#sec-slr-intro",
    "title": "4  Simple linear regression",
    "section": "\n4.1 Introduction: Movie ratings",
    "text": "4.1 Introduction: Movie ratings\nReviews from movie critics can be helpful when determining whether a movie is high quality and well-made; however, it can sometimes be challenging to determine whether regular audience members will like a movie based on critics reviews. We would like a way to better understand the relationship between what movie critics and regular movie goers think about a movie, and ultimately predict how an audience will rate a movie based on its score from movie critics.\nTo do so, we will analyze data that contains the critics scores and audience scores for 146 movies released in 2014 and 2015. The scores are for every movie released in these years that have “a rating on Rotten Tomatoes, a RT User rating, a Metacritic score, a Metacritic User score, an IMDb score, and at least 30 fan reviews on Fandango” (Albert Y. Kim, Ismay, and Chunn 2018a). The analysis in this chapter focuses on scores from Rotten Tomatoes, a website for information and ratings on movies and television shows. The data were originally analyzed in the article “Be Suspicious of Online Movie Ratings, Especially Fandango’s” (Hickey 2015) on the former data journalism website FiveThirtyEight. The data are available in movie_scores.csv. The data set was adapted from the fandago data frame in the fivethirtyeight R package (Albert Y. Kim, Ismay, and Chunn 2018b).\nWe will focus on two variables for this analysis:\n\ncritics_score: The percentage of critics who have a favorable review of the movie. This is known as the “Tomatometer” score on the Rotten Tomatoes website. The possible values are 0 - 100.\naudience_score: The percentage of users (regular movie-goers) on Rotten Tomatoes who have a favorable review of the movie. The possible values are 0 - 100.\n\n\nOur goal is to use simple linear regression to model the relationship between the critics score and audience score. We want to use the model to\n\ndescribe how the audience score is expected to change as the critics score changes.\npredict the audience score for a movie based on its critics score.\n\n\nRecall from Section 1.1.1, the response variable is the outcome of interest, meaning the variable we are interested in predicting and understanding its variability. It is also known as the outcome or dependent variable and is represented as \\(Y\\). The predictor variable(s) is the variable (or variables) used to understand variability in the response. It is also known as the explanatory or independent variable and represented as \\(X\\). The observed values of the response and predictor are represented as \\(y_i\\) and \\(x_i\\), respectively.\n\nWhat is the response variable for the movie scores analysis? What is the predictor variable?1",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-eda",
    "href": "04-slr.html#sec-slr-eda",
    "title": "4  Simple linear regression",
    "section": "\n4.2 Exploratory data analysis",
    "text": "4.2 Exploratory data analysis\nRecall from Chapter 3 that we begin analysis with exploratory data analysis (EDA) to better understand the data, the distributions of key variables, and relationships in the data before fitting the regression model. The exploratory data analysis here focuses on the two variables that will be in the regression model, critics and audience. In practice, however, we may want to explore other variables in the data set (for example, year in this analysis) to provide additional context later on as we interpret results from the regression model. We begin with univariate EDA (Section 3.4), exploring one variable at a time, then we’ll conduct bivariate EDA (Section 3.5) to look at the relationship between the critics scores and audience scores.\n\n4.2.1 Univariate EDA\nThe univariate distributions of critics_score and audience_score are visualized in Figure 4.1 and summarized in Table 4.1.\n\n\n\n\n\n\n\n\n\n(a) Critics score\n\n\n\n\n\n\n\n\n\n(b) Adience score\n\n\n\n\n\nFigure 4.1: Univariate distributions of critics_score and audience_score\n\n\n\n\n\n\nTable 4.1: Summary statistics for critics_score and audience_score\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\ncritics_score\n60.8\n30.2\n5\n31.2\n63.5\n89\n100\n0\n\n\naudience_score\n63.9\n20.0\n20\n50.0\n66.5\n81\n94\n0\n\n\n\n\n\n\n\n\nThe distribution of critics_score is left-skewed, meaning the movies in the data set are generally more favorably reviewed by critics (more observations with higher critics scores). Given the apparent skewness, the center is best described by the median score of 63.5 points. The interquartile range (IQR), the spread of the middle 50% of the distribution, is 57.8 points \\((Q_3 - Q_1 = 89 - 31.2)\\), so there is a lot of variability in the critics scores for the movies in the data. There are no apparent outliers, but we observe from the raw data that there are two notable observations of movies that have perfect critics scores of 100. There are no missing values of critics score.\n\nUse the histogram in Figure 4.1 (b) and summary statistics in Table 4.1 to describe the distribution of the response variable audience_score.2\n\n\n4.2.2 Bivariate EDA\nNow let’s look at the relationship between critics_score and audience_score. From Section 3.5.1, we use visualizations and summary statistics to examine the relationship between two quantitative variables. A scatterplot of the the audience score versus critics score is shown in Figure 4.2. The predictor variable is on the \\(x\\)-axis (horizontal axis), and the response variable is on the \\(y\\)-axis (vertical axis).\n\n\n\n\n\n\n\nFigure 4.2: Scatterplot of critics_score and audience_score\n\n\n\n\nThere is a positive, linear relationship between the critics scores and audience scores for the movies in our data. The correlation between these two variables is 0.78, indicating the relationship is strong. Therefore, we can generally expect the audience score to be higher for movies with higher critics scores. There are no apparent outliers, but there does appear to be more variability in the audience score for movies with lower critics scores than for those with higher critics scores.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-fit-regression",
    "href": "04-slr.html#sec-fit-regression",
    "title": "4  Simple linear regression",
    "section": "\n4.3 Linear regression",
    "text": "4.3 Linear regression\nIn Section 4.2, we used visualizations and summary statistics to describe the relationship between two variables. The exploratory data analysis, however, does not tell us what the audience score is predicted to be for a given value of the critics score or how much the audience score is expected to change as the critics score changes. Therefore, we will fit a linear regression model to quantify the relationship between the two variables. Recall the general form of the linear regression model in Equation 1.3. More specifically, when we have one predictor variable, we will fit a model of the form\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{4.1}\\]\nEquation 4.1, called a simple linear regression (SLR) model, is the equation to model the relationship between one quantitative response variable and one predictor variable. For now we will focus on models with one quantitative predictor variable. In later chapters, we will introduce models with two or more predictors (Chapter 7), categorical predictors (Section 7.4.2), and models with a categorical response variable (Chapter 11).\nWe are generally interested in using regression models for two types of tasks:\n\n\nPrediction: Finding the expected value of the response variable for given values of the predictor variable(s).\n\nInference: Drawing conclusions about the relationship between the response and predictor variables.\n\n\nSuppose we fit a simple linear regression line to summarize the relationship between critics_score and audience_scores for movies.\n\nWhat is an example of a prediction question that can be answered using a simple linear regression model?\nWhat is an example of an inference question that can be answered using a simple linear regression model?3\n\n\n\n\n4.3.1 Statistical (theoretical) model\nWe expand on the concepts introduced in Section 1.1.1 for the simple linear regression model. Suppose there is a response variable \\(Y\\) and a predictor variable \\(X\\). The values of the response variable \\(Y\\) can be generated in the following way:\n\\[\nY = \\text{Model} + \\text{Error}\n\\tag{4.2}\\]\nMore specifically, we define the model as a function of the predictor \\(X\\), \\(\\text{Model} = f(X)\\), and error \\(\\epsilon\\), such that\n\\[\nY = f(X) + \\epsilon\n\\tag{4.3}\\]\nThe function \\(f(X)\\) that describes the relationship between the response and predictor variables is the regression model. This is the model we will fit in later sections using equations and software. The error, \\(\\epsilon\\), is how much the actual value of the response \\(Y\\) deviates from the value produced by the regression model, \\(f(X)\\). There is some randomness in \\(\\epsilon\\), because not all observations with the same value of \\(X\\) have the same value of \\(Y\\). For example, not all movies with a critics score of 70 have the same audience score.\nEquation 4.3 is the general form of the equation to generate values of \\(Y\\) given values of \\(X\\). In the context of simple linear regression, the function \\(f(X)\\) in Equation 4.3 is\n\\[\nf(X) = \\mu_{Y|X} = \\beta_0 + \\beta_1X\n\\tag{4.4}\\]\nwhere \\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) at a particular value of \\(X\\), and \\(\\beta_0\\) and \\(\\beta_1\\) are the model coefficients. The error terms \\(\\epsilon\\) from Equation 4.3 are normally distributed with a mean of 0 and variance \\(\\sigma_{\\epsilon}^2\\), represented as \\(N(0, \\sigma^2_{\\epsilon})\\) (more on this in Section 5.3. The specification of the simple linear regression model written in terms of individual observations \\((x_i, y_i)\\) is\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\hspace{7mm} \\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\n\\tag{4.5}\\]\nsuch that \\(y_i\\) is the response for the \\(i^{th}\\) observation, \\(x_i\\) is he predictor for the \\(i^{th}\\) observation, and \\(\\epsilon_i\\) is the error for the \\(i^{th}\\) observation. Equation 4.5 is the statistical model, also called the data-generating model or population-level model. It is the theoretical form of the model that describes exactly how to generate the values of the response \\(Y\\) given values of the predictor in the population. The model coefficients are the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\). The\\(\\sigma^2_{\\epsilon}\\) is called the standard error. In practice we don’t know the exact values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma_{\\epsilon}^2\\), so our goal is to use sample data to estimate these values. We will focus on the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this chapter. We discuss estimating \\(\\sigma^2_{\\epsilon}\\) in Section 5.8.2.\n\nIn simple linear regression, we use sample data to estimate a model to understand trends in the population.\nWhat is the population in the movie scores analysis? What is the sample?4 Recall the definition of population and sample in Section 1.1.\n\n\n4.3.2 Evaluating whether SLR is appropriate\nBefore doing any more calculations, we need to determine if the simple linear regression model is a reasonable choice to summarize the relationship between the response variable and predictor variable based on what we know about the data and what we’ve observed from the exploratory data analysis. Determining this early on can help prevent going in a wrong analysis direction if a linear regression model is obviously not a good choice for the data.\nWe can ask the following questions to evaluate whether simple linear regression is appropriate:\n\nWill a linear regression model be practically useful? Does quantifying and interpreting the relationship between the variables make sense in this scenario?\nIs the shape of the relationship reasonably linear?\nDo the observations in the data represent the population of interest, or are there biases in the data that could limit conclusions drawn from the analysis? \n\n\nMathematical equations or statistical software can be used to fit a linear regression model between any two quantitative variables. Therefore it is upon the judgment of the data scientist to determine if it is reasonable to proceed with a linear regression model or if doing so might result in misleading conclusions about the data. If the answer is “no” to any of the questions above, consider if a different analysis technique is better for the data, or proceed with caution if using regression. If we proceed with regression, be transparent about some of the limitations of the conclusions.\nFrom Section 4.1, the goal of this analysis is understand the relationship between the critics scores and audience score for movies on Rotten Tomatoes. Therefore, there is a practical use for fitting the regression model. We observed from Figure 4.2 that the relationship between the two variables is approximately linear, so it could reasonably be summarized a model of the form of Equation 4.5. Lastly, the data set includes all movies in 2014 and 2015 that has a sufficient number of ratings on popular movie ratings websites, so we can reasonably conclude the sample is representative of the population of movies on Rotten Tomatoes. Therefore, we are comfortable drawing conclusions about the population based on the analysis of our sample data. \nThe form of the simple linear regression model for the movie scores data is\n\\[\n\\text{audience\\_score} = \\beta_0 + \\beta_1~\\text{critics\\_score} + \\epsilon, \\hspace{5mm}\\epsilon  \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{4.6}\\]\nNow that we have the form of the model, let’s discuss how to estimate and interpret the model coefficients, the slope \\(\\beta_1\\) and the intercept \\(\\beta_0\\). We will estimate \\(\\sigma^2_{\\epsilon}\\) in Chapter 5.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-estimation",
    "href": "04-slr.html#sec-slr-estimation",
    "title": "4  Simple linear regression",
    "section": "\n4.4 Estimating the model coefficients",
    "text": "4.4 Estimating the model coefficients\nIdeally, we would have data from the entire population of movies rated on Rotten Tomatoes in order to calculate the exact values for \\(\\beta_0\\) and \\(\\beta_1\\). In reality we don’t have access to the data from the entire population, but we can use the sample to obtain the estimated regression equation in Equation 4.7\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\n\\tag{4.7}\\]\nwhere \\(\\hat{\\beta}_0\\) is the estimated intercept, \\(\\hat{\\beta}_1\\) is the estimated slope, and \\(\\hat{y}_i\\) is the predicted (estimated) response.\nSpecifically for the movie scores analysis, the estimated regression equation is\n\\[\n\\widehat{\\text{audience\\_score}}_i = 32.316 + 0.519~\\text{critics\\_score}_i\n\\tag{4.8}\\]\nIn this equation 32.316 is \\(\\hat{\\beta}_0\\), the estimated value for the intercept, 0.519 is \\(\\hat{\\beta}_1\\), the estimated value for the slope \\(\\beta_1\\), and \\(\\widehat{\\text{audience\\_score}}_i\\) is the expected audience score when the critics score is equal to \\(\\text{critics\\_score}_i\\). Notice that Equation 4.7 and Equation 4.8 do not have have error term, \\(\\epsilon\\). The output from the regression equation is \\(\\hat{f(X)} = \\hat{\\mu}_{Y|X}\\), the expected mean value of the response given a value of the predictor. Therefore, when we discuss the values of the response estimated using simple linear regression, what we are really talking about is what the value of the response variable is expected to be, on average, for a given value of the predictor variable.\nFrom Figure 4.2, we know that the value of the response is not necessarily the same for all observations with the same value of the predictor. For example, we wouldn’t expect (nor do we observe) the same audience score for every movie with a critics score of 70. We know there are other factors other than the critics score that are related to how an audience reacts to a movie. Our analysis, however, only takes into account the critics score, so we do not capture these additional factors in our regression equation Equation 4.8. This is where the error terms come back in.\nOnce we computed estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the regression equation, we can calculate how far the predicted values of the response produced by the regression equation differ from the actual values of the response variable observed in the data. This difference is called the residual, denoted \\(e_i\\). \n\nEquation 4.9 shows the equation of the residual for the \\(i^{th}\\) observation.\n\\[\ne_i = \\text{observed}_i - \\text{predicted}_i =  y_i - \\hat{y}_i\n\\tag{4.9}\\]\nIn the case of the movie scores data, the residual is the difference between the actual audience score and the audience score predicted by Equation 4.8. For example, the 2015 movie Avengers: Age of Ultron received a critics score of \\(y_i = 74\\). Therefore, using Equation 4.8, the estimated (predicted) audience score is.\n\\[\n\\hat{y}_i = 32.316 + 0.519 \\times 74 = 70.722.\n\\]\nThe observed audience score is 86, so the residual is\n\\[\ne_i = y_i - \\hat{y}_i = 86 - 70.722 = 15.278\n\\]\n\nWould you rather see a movie that has a positive or negative residual? Explain your response.5\n\n\n4.4.1 Least squares regression\nThere are many possible regression lines (infinitely many, in fact) that we could use to summarize the relationship between critics_score and audience_scores. We see some fo the potential lines represented in Figure 4.3. So how did we determine the line that “best” fits the data is the one described by Equation 4.8? We’ll use the residuals to help us answer this question.\n\n\n\n\n\n\n\nFigure 4.3: Potential regression lines for the relationship between critics_score and audience_score\n\n\n\n\nThe residuals, represented by the vertical dotted lines in Figure 4.4, are a measure of the “error”, the difference between the observed value of the response and the value predicted from a regression model. The line that “best” fits the data is the one that generally results in the smallest overall error. One way to find the line with the smallest overall error is to add up all the residuals for each possible line in Figure 4.3 and choose the one that has the smallest sum. Notice, however, that for lines that seem to closely align with the trend of the data, there is approximately equal distribution of points above and below the line. Thus as we’re trying to compare lines that pretty closely fit the data, we’d expect the residuals to add up to a value very close to zero. This would make it difficult, then, to determine a best fit line.\n\n\n\n\n\n\n\nFigure 4.4: Regression line of the relationship between critics_score and audience_score with residuals\n\n\n\n\nInstead of using the sum of the residuals, we will instead consider the sum of the squared residuals in Equation 4.10\n\\[\n\\sum_{i=1}^n e_i^2 = e_1^2 + e_2^2 + \\dots + e_n^2\n\\tag{4.10}\\]\nwhere \\(n\\) is the number of observations in the data. The line that “best” fits the data, then, is the line that minimizes Equation 4.10. This is called the least squares regression model.\n\n\nLet’s expand Equation 4.10. Recall that \\(e_i\\), the residual of the \\(i^{th}\\) observation, is \\(y_i - \\hat{y}_i\\) where \\(\\hat{y}_i\\) is the estimated response. Then,\n\\[\n\\begin{aligned}\ne_i &= y_i - \\hat{y}_i \\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\\end{aligned}\n\\tag{4.11}\\]\nThus, putting Equation 4.11 into Equation 4.10, we have\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n e_i^2 &= e_1^2 + e_2^2 + \\dots + e_n^2 \\\\\n&= [y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_1)]^2 + [y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_2)]^2 + & \\\\\n& \\dots + [y_n - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_n)]^2\n\\end{aligned}\n\\tag{4.12}\\]\nUsing calculus, the \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize Equation 4.12 are\n\\[\n\\hat{\\beta}_1 = r\\frac{s_Y}{s_X} \\hspace{10mm} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\tag{4.13}\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the mean values of the predictor and response variables, respectively, \\(s_X\\) and \\(s_Y\\) are the standard deviations of the predictor and response variables, respectively, and \\(r\\) is the correlation between the response and predictor variables. See Appendix A.1 for the full details of the derivation from Equation 4.12 to Equation 4.13.\nEquation 4.14 show the calculations of slope and intercept for the movie scores model based on the summary statistics in Table 4.1. Note that the small differences in the values compared to Equation 4.8 are due to rounding (versus coefficients computed by software).\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= 0.78 \\times \\frac{20.0}{30.2} = 0.517 \\\\ \\hat{\\beta}_0 &=  63.9 - 0.517 \\times  60.8 = 32.467\n\\end{aligned}\n\\tag{4.14}\\]\n\nBelow are a few properties of least-squares regression models.\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r\\frac{s_Y}{s_X}\\)\nThe sum of the residuals is zero: \\(\\sum_{i=1}^n e_i = 0\\)\nThe residuals and values of the predictor are uncorrelated",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-interpret-coef",
    "href": "04-slr.html#sec-slr-interpret-coef",
    "title": "4  Simple linear regression",
    "section": "\n4.5 Interpreting the model coefficients",
    "text": "4.5 Interpreting the model coefficients\nThe slope \\(\\hat{\\beta}_1\\) is the estimated change in the response for each unit increase in the predictor variable. What do we mean by “estimated change”? Recall that the output from the regression equation is \\({\\mu}_{Y|X}\\) the estimated mean of the response \\(Y\\) for a given value of the predictor \\(X\\). Thus, the slope or the “steepness” of the regression line, is a measure of how much the response variable is expected to change, on average, for each unit increase of the predictor.\nIt is good practice to write the interpretation of the slope in the context of the data, so that it can be more easily understood by others reading the analysis results. “In the context of the data” means that the interpretation includes\n\nmeaningful descriptions of the variables, if the variable names would be unclear to an outside reader\nunits for each variable\nan indication of the population for which the model applies.\n\nThe slope in Equation 4.8 of 0.519 is interpreted as the following:\n\nFor each additional point in the critics score, the audience score for movies on Rotten Tomatoes is expected to increase by 0.519 points, on average.\n\nThe intercept is the estimated value of the response variable when the predictor variable equals zero \\((x_i = 0)\\). On a scatterplot of the response and predictor variable, this is the point where the regression line crosses the \\(y\\)-axis. Similar to the slope, the “estimated value” is more specifically the estimated mean value of the response variable when the predictor equals 0 ( \\(\\hat{\\mu}_{Y|X = 0})\\).\nThe intercept in Equation 4.8 of 32.316 is interpreted as the following:\n\nThe expected audience score for movies on Rotten Tomatoes with a critics score of 0 is 32.316 points.\n\nWe always need to estimate the intercept in ?eq-regressio to get the line that best fit using least squares regression. The intercept, however, does not always have a meaningful interpretation. We ask the following questions to determine if the intercept has a meaningful interpretation:\n\nIs it plausible for the predictor variable to take values at or near zero?\nAre there observations in the data with values of the predictor at or near zero?\n\nIf the answer to either question is no, then it is not meaningful, and potentially misleading, to interpret the intercept.\n\nIs the interpretation of the intercept in Equation 4.8 meaningful? Briefly explain.6\n\n\nAvoid using causal language and making declarative statements (e.g., “The audience score for a movie with a critics score of 0 points will be 32.316 points.”) when interpreting the slope and intercept. Remember the slope and intercept are estimates describing what is expectedin the relationship between the response and predictor to be based on the sample data and linear regression model. They do not tell us exactly what will happen in the data.\n\nThere is an area of statistics called causal inference about model that can be used to make causal statements from observational (non-experimental) data. See Section 13.4 for a brief introduction to causal inference.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-predict",
    "href": "04-slr.html#sec-slr-predict",
    "title": "4  Simple linear regression",
    "section": "\n4.6 Prediction",
    "text": "4.6 Prediction\nOne of the primary purposes of a regression model is to use for prediction. When a regression model is used for prediction, the estimated value of the response variable is computed based on a given value of the predictor. We’ve seen this in earlier sections when calculating the residuals. Let’s take a look at the model predictions for two movies released in 2023.\nThe movie Barbie was released in theaters on July 21, 2023. This movie was widely praised by critics, and it has a critics score of 88 at the time the data were obtained. Based on Equation 4.8, the predicted audience score is\n\\[\n\\begin{aligned}\n\\widehat{\\text{audience\\_scores}} &= 32.316 + 0.519 \\times 88 \\\\\n&= \\textbf{77.988} \\\\\n\\end{aligned}\n\\]\nFrom the snapshot of the Barbie Rotten Tomatoes page (Figure 4.5), we see the actual audience score is 837. Therefore, the model under predicted the audience score by about 5 points (83 - 77.988). Perhaps this isn’t surprising given this film’s massive box office success! \n\n\n\n\n\nFigure 4.5: Source: https://www.rottentomatoes.com/m/barbie (accessed August 29, 2023)\n\n\n\nThe movie Asteroid City was released in theaters on June 23, 2023. The critics score for this movie was 758.\n\nWhat is the predicted audience score?\nThe actual audience score is 62. Did the model over or under predict? What is the residual? 9\n\n\n\nThe regression model is most reliable when predicting the response for values of the predictor within the range of the sample data used to fit the regression model. Using the model to predict for values far outside this range is called extrapolation. The sample data provide information about the relationship between the response and predictor variables for values within the range of the predictor in the data. We can not safely assume that the linear relationship quantified by our model is the same for values of the predictor far outside of this range. Therefore, extrapolation often results in unreliable predictions that could be misleading if the linear relationship does not hold outside the range of the sample data.\n\nOnly use the regression model to compute predictions for values of the predictor that are within (or very close) to the range of values in the sample data used to fit the model. Extrapolation, using a model to compute predictions for value so the predictor far outside the range in the data, can result in unreliable predictions.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-model-assessment",
    "href": "04-slr.html#sec-model-assessment",
    "title": "4  Simple linear regression",
    "section": "\n4.7 Model evaluation",
    "text": "4.7 Model evaluation\nWe have shown how a simple linear regression model can be used to describe the relationship between a response and predictor variable and to predict new values of the response. Now we will look at two statistics that will help us evaluate how well the model fits the data and how well it explains variability in the response.\n\n4.7.1 Root Mean Square Error\nThe Root Mean Square Error (RMSE), shown in Equation 4.15, is a measure of the average difference between the observed and predicted values of the response variable.\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}}\n\\tag{4.15}\\]\nThis measure is especially useful if prediction is the primary modeling objective. The RMSE takes values from 0 to \\(\\infty\\) (infinity) and has the same units as the response variable.\n\nDo higher or lower values of RMSE indicate a better model fit?10\n\nThere is no universal threshold of RMSE to determine whether the model is a good fit. In fact, the RMSE is often most useful when comparing the performance of multiple models. Take the following into account when using RMSE to evaluate model fit.\n\nWhat is the range \\((\\text{max} - \\text{min})\\) of the response variable in the data? How does the RMSE compare to the range? For example, \\(RMSE = 10\\) indicates very good model performance if the response variable ranges from 10000 to 20000, but very poor model performance if the response variable ranges from 0 to 20.\nWhat is a reasonable error threshold based on the subject matter and analysis objectives? We may be willing to use a model with higher RMSE for a low-stakes analysis objective (for example, the model is used to inform the choices of movie-goers) than a high-stakes objective (the model is used to inform how a movie studio’s multi-million dollar marketing budget will be allocated).\n\n\nThe RMSE for the movie scores model is 12.452. The range for the audience score is -. What is your evaluation of the model fit based on RMSE? Explain your response.11\n\n\n4.7.2 Analysis of variance and \\(R^2\\)\n\nThe coefficient of determination, \\(R^2\\), the percentage of variability in the response variable that is explained by the predictor variable. In terms of the movie scores data, it is the percentage of variability in the audience score that is accounted for by changes in the critics score. Before talking more about how \\(R^2\\) is used for model evaluation, let’s discuss how this percentage is calculated.\nThere is variability in the response variable, as we see in the exploratory data analysis in Figure 4.1 and Table 4.1. Analysis of Variance (ANOVA), shown in Equation 4.16, is the process of partitioning the various sources of variability.\n\\[\n\\text{Total variability} = \\text{Explained variability} + \\text{Unexplained variability}\n\\tag{4.16}\\]\nFrom Equation 4.16, the variability in the response variable is from two sources:\n\nExplained variability (Model): This is the variability in the response variable that can be explained by the model. In the case of simple linear regression, it is the variability in the response variable that can be explained by the predictor variable. In the movie scores analysis, this is the variability in audience_score that is explained by the critics_score.\nUnexplained variability (Residuals): This is the variability in the response variable that is left unexplained after the model is fit. This can be understood by assessing the variability in the residuals. In the movie scores analysis, this is the variability due to the factors other than critics score.\n\nThe variability in the response variable and the contribution from each source is quantified using sum of squares. In general, the sum of squares (SS) is a measure of how far the observations are from a given point, for example the mean. Using sum of squares, we can quantify the components of Equation 4.16.\nLet \\(SST\\) = Sum of Squares Total, \\(SSM\\) = Sum of Squares Model, and \\(SSR\\) = Sum of Squares Residuals. Then,\n\\[\n\\begin{aligned}\nSST &= SSM + SSR \\\\[10pt]\n\\sum_{i=1}^n (y_i - \\bar{y})^2 &= \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\n\\end{aligned}\n\\tag{4.17}\\]\n\nSum of Squares Total (SST) \\(= \\sum_{i=1}^n(y_i - \\bar{y})^2\\), is the total variability, an overall measure of how far the observed values of the response variable are from the mean value of the response \\(\\bar{y}\\). The formula for SST may look familiar, as it is \\((n-1)s_y^2\\) , which equals\\((n-1)\\) times the variance of \\(y\\). SST can be partitioned into two pieces, Sum of Squares Model (SSM) and Sum of Squares Residuals (SSR).\nSum of Squares Model (SSM) \\(= \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\\), is the explained variability, an overall measure of how much the predicted value of the response variable (the expected mean value of the response given the predictor) differs from the overall mean value of the response. This indicates how much the observed response’s deviation from the mean is accounted for by knowing the value of the predictor.\nLastly, the Sum of Squares Residual (SSR) \\(= \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\), is the unexplained variability, an overall measure of how much the observed values of the response differ from the predicted values. This is the same sum of squared residuals used to estimate the least-squares regression model in Section 4.4.1.\nWe use the sum of squares to calculate the coefficient of determination \\(R^2\\)\n\\[\nR^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\n\\tag{4.18}\\]\n\nEquation 10.1, shows that \\(R^2\\) is a the proportion of variability in the response (SST) that is explained by the model (SSM). Note that \\(R^2\\) is calculated as proportion between 0 and 1, but is reported as a percentage between 0% and 100%.\nThe \\(R^2\\) for the model in Equation 4.8 is 0.611. It is interpreted as the following:\n\nAbout 61.1% of the variability in the audience score for movies on Rotten Tomatoes can be explained by the model (critics score).\n\n\nDo higher or lower values of \\(R^2\\) indicate a better model fit?12\n\nSimilar to RMSE, there is no universal threshold for what makes a “good” \\(R^2\\) value. When using \\(R^2\\) to determine if the model is a good fit, take into account what might be a reasonable to expect given the subject matter.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#sec-slr-R",
    "href": "04-slr.html#sec-slr-R",
    "title": "4  Simple linear regression",
    "section": "\n4.8 Simple linear regression in R",
    "text": "4.8 Simple linear regression in R\n\n4.8.1 Fitting the least-squares model\nWe fit linear regression models using the lm function, which is part of the stats package (2024) built into R. We then use the tidy function from the broom package (Robinson, Hayes, and Couch 2023) to display the results in a tidy data format (Section 2.3.1). The code to find the linear regression model using the movie_scores data with audience_score as the response and critics_score as the predictor (Equation 4.8) is below.\n\nlm(audience_score ~ critics_score, data = movie_scores)\n\n\nCall:\nlm(formula = audience_score ~ critics_score, data = movie_scores)\n\nCoefficients:\n  (Intercept)  critics_score  \n       32.316          0.519  \n\n\nNext, we want to display the model results in a tidy format. We build upon the code above by saving the model in an object called movie_fit and displaying the object. We will also use movie_fit to calculate predictions.\n\nmovie_fit &lt;- lm(audience_score ~ critics_score, data = movie_scores) \n\ntidy(movie_fit) \n\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     32.3      2.34        13.8 4.03e-28\n2 critics_score    0.519    0.0345      15.0 2.70e-31\n\n\nNotice the resulting the model is the same as Equation 4.8, which we calculated based on Equation 4.13. We will discuss the other columns in the output in Chapter 5.\nWe can also use kable() from the knitr package (Xie 2024) to display the tidy results in an neatly formatted table and control the number of digits in the output.\n\ntidy(movie_fit) |&gt;\n  kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n32.316\n2.343\n13.8\n0\n\n\ncritics_score\n0.519\n0.035\n15.0\n0\n\n\n\n\n\n\n4.8.2 Prediction\nBelow is the code to predict the audience score for Barbie as shown earlier in the section. We create a tibble that contains the critics score for Barbie, then use predict() and the model object to compute the prediction.\n\nbarbie_movie &lt;- tibble(critics_score = 88) \npredict(movie_fit, barbie_movie)\n\n 1 \n78 \n\n\nWe can also produce predictions for multiple movies by putting multiple values of the predictor in the tibble. In the code below we produce predictions for Barbie and Asteroid City. We begin by storing the critics scores for both movies in a tibble. Then we use predict(), as before.\n\nnew_movies &lt;- tibble(critics_score = c(88, 75)) \npredict(movie_fit, new_movies) \n\n   1    2 \n78.0 71.2 \n\n\n\n4.8.3 \\(R^2\\) and RMSE\nThe glance() function in the broom package produces model summary statistics, including \\(R^2\\).\n\nglance(movie_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.611         0.608  12.5      226. 2.70e-31     1  -575. 1157. 1166.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe code below will only return \\(R^2\\) from the output of glance().\n\nglance(movie_fit)$r.squared\n\n[1] 0.611\n\n\nRMSE is computed using the rmse() function from the yardstick package (Kuhn, Vaughan, and Hvitfeldt 2025). First, we use augment() from the broom package to compute the predicted value for each observation in the data set. These values are stored if the column .fitted. We may notice that many other columns are produced by augment() as well; these are discussed in Chapter 6. We input the augmented data into rmse().\n\nmovies_augment &lt;- augment(movie_fit) \n\nrmse(movies_augment, truth = audience_score, estimate = .fitted) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        12.5",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#summary",
    "href": "04-slr.html#summary",
    "title": "4  Simple linear regression",
    "section": "\n4.9 Summary",
    "text": "4.9 Summary\nIn this chapter, we introduced simple linear regression. We showed how to use exploratory data analysis to evaluate whether linear regression is appropriate to model the relationship between two variables. Next, we computed the slope and intercept (the model coefficients) and interpreted these values in in the context of the data. We used the model to compute predictions and evaluated the model performance using \\(R^2\\) and RMSE. We finished the chapter by conducting simple linear regression in R.\nThis chapter has helped set the foundation for all the regression methods presented throughout the remainder of the text. In Chapter 5, we’ll use the simple linear regression model to draw conclusions about the relationship between the response and predictor variables.\n\n\n\n\n\n\n\n\n\n\n\n\nHickey, Walt. 2015. “Be Suspicious of Online Movie Ratings, Especially Fandango’s.” FiveThirtyEight, Available at: Http://Fivethirtyeight. Com/Features/Fandango-Movies-Ratings.\n\n\nKim, Albert Y., Chester Ismay, and Jennifer Chunn. 2018a. “The Fivethirtyeight r Package: ’Tame Data’ Principles for Introductory Statistics and Data Science Courses” 11. https://escholarship.org/uc/item/0rx1231m.\n\n\nKim, Albert Y, Chester Ismay, and Jennifer Chunn. 2018b. “The Fivethirtyeight r Package:‘tame Data’principles for Introductory Statistics and Data Science Courses.” Technology Innovations in Statistics Education 11 (1).\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2025. “Yardstick: Tidy Characterizations of Model Performance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” https://www.R-project.org/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. “Broom: Convert Statistical Objects into Tidy Tibbles.” https://CRAN.R-project.org/package=broom.\n\n\nXie, Yihui. 2024. “Knitr: A General-Purpose Package for Dynamic Report Generation in r.” https://yihui.org/knitr/.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "04-slr.html#footnotes",
    "href": "04-slr.html#footnotes",
    "title": "4  Simple linear regression",
    "section": "",
    "text": "The response variable is audience, the audience score. The predictor variable is critics, the critics score.↩︎\nThe distribution of audience_score is unimodal and left-skewed. The median score is 66.5 and the IQR is 31 (81 - 50). We note that the center is higher and there is less variability in the middle 50% of the distribution compared to critics_score .↩︎\nExample prediction question: What do we expect the audience score to be for movies with a critics score of 75?Example inference question Is the critics score a useful predictor of the audience score?↩︎\nThe population is all movies on the Rotten Tomatoes website. The sample is the set of 146 movies in the data set.↩︎\nExample answer: I would rather see a movie with a positive residual, because that means the audience actually rated the movie more favorably than what was expected based on the model.↩︎\nThe interpretation of the intercept is meaningful, because it is plausible for a movie to have a critics score of 0 and there are observations with scores around 5, which is near 0 on the 0 - 100 point scale.↩︎\nSource: https://www.rottentomatoes.com/m/barbie Accessed on August 29, 2023.↩︎\nSource: https://www.rottentomatoes.com/m/asteroid_city Accessed on August 29, 2023.↩︎\nThe predicted audience score is 32.316 + 0.519 * 75 = 71.241. The model over predicted. The residual is 62 - 71.241 = -9.241.↩︎\nLower values indicate a better fit, with 0 indicating the predictor variable perfectly predicts the response.↩︎\nExample answer: An error of 12.452 is about a 17% error based on the range of the audience scores. Because the audience scores range 0 to 100, this error seems relatively large.↩︎\nHigher values of \\(R^2\\) indicate a better model fit, as it means more of the variability in the response is being explained by the model.↩︎",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html",
    "href": "05-slr-inference.html",
    "title": "5  Inference for simple linear regression",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#learning-goals",
    "href": "05-slr-inference.html#learning-goals",
    "title": "5  Inference for simple linear regression",
    "section": "",
    "text": "Explain how statistical inference is used to draw conclusions about a population model coefficient\nConstruct confidence intervals using bootstrap simulation\nConduct hypothesis tests using permutation\nExplain how the Central Limit Theorem is applied to inference for the model coefficient\nConduct statistical inference using mathematical models based on the Central Limit Theorem\nInterpret results from statistical inference in the context of the data\nExplain the connection between hypothesis tests and confidence intervals",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-slr-inf-data",
    "href": "05-slr-inference.html#sec-slr-inf-data",
    "title": "5  Inference for simple linear regression",
    "section": "5.1 Introduction: Access to playgrounds",
    "text": "5.1 Introduction: Access to playgrounds\nThe Trust for Public Land is a non-profit organization that advocates for equitable access to outdoor spaces in cities across the United States. In the 2021 report Parks and an Equitable Recovery, the organization stated that “parks are not just a nicety—they are a necessity” (The Trust for Public Land 2021). The report details the many health, social, and environmental benefits of having ample access to public outdoor space in cities, along with the various factors that impede the access to parks and other outdoor space for some residents.\nOne type of outdoor space the authors study in the report is playgrounds. The report describes playgrounds as outdoor spaces that “bring children and adults together” (The Trust for Public Land 2021, 13) and a place that was important for distributing “fresh food and prepared meals to those in need, particularly school-aged children” (The Trust for Public Land 2021, 9) during the global COVID-19 pandemic.\n\nGiven the impact of playgrounds for both children and adults in a community, we want to understand factors associated with variability in the access to playgrounds. In particular, we want to (1) investigate whether local government spending is useful in understanding variability in playground access, and if so, (2) quantify the true relationship between local government spending and playground access.\n\nThe data includes information on 97 of the most populated cities in the United States (US) in the year 2020. The data were originally collected by the Trust for Public Land and was a featured as part of the TidyTuesday weekly data visualization challenge in June 2021 (Community 2024). The data are in parks.csv. The analysis in this chapter focuses on two variables:\n\n\n\nper_capita_expend: Total amount the city government spent per resident in 2020 in US dollars (USD). This is a measure of how much a city invests in services and facilities for its residents. We refer to it as a city’s “per capita expenditure”.\nplaygrounds : Number of playgrounds per 10,000 residents in 2020\n\n\nWhich of the following do you think best describes the relationship between per_capita_expend and playgrounds?1\n\nThe relationship is positive.\nThe relationship is negative.\nThere is no relationship.\n\n\n\n5.1.1 Exploratory data analysis\nThe visualizations and summary statistics for univariate and bivariate exploratory data analysis are in Figure 5.1 and Table 5.1.\n\n\n\n\n\n\n\n\n\n\n\n(a) Playgrounds per 10,000 residents\n\n\n\n\n\n\n\n\n\n\n\n(b) Per capita expenditure in USD\n\n\n\n\n\n\n\nFigure 5.1: Univariate exploratory data analysis of playgrounds and per_capita_expend\n\n\n\n\n\n\n\nTable 5.1: Summary statistics of playgrounds and per_capita_spend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\n\nplaygrounds\n2.8\n1.1\n1\n1.9\n2.6\n3.6\n7\n0\n\n\nper_capita_expend\n113.0\n72.0\n15\n65.0\n89.0\n142.0\n399\n0\n\n\n\n\n\n\n\n\nThe distribution of playgrounds, the number of playgrounds per 10,000 residents (the response variable), is unimodal and right-skewed. The center of the distribution is the median of about 2.6 playgrounds per 10,000 residents, and the the spread of the middle 50% of the distribution (the IQR) is 1.7. There appear to be two potential outlying cities with more than 6 playgrounds per 10,000 residents, indicating high playground access relative to the other cities in the data set.\nThe distribution of per_capita_expend, a city’s expenditure per resident (the predictor variable), is also unimodal and right-skewed. The center of the distribution is around 89 dollars per resident, and the middle 50% of the distribution has a spread of about 77 dollars per resident. Similar to the response variable, there are some potential outliers. There are 5 cities that invests more than 300 dollars per resident.\n\n\n\n\n\n\n\n\nFigure 5.2: Bivariate exploratory data analysis of playgrounds versus per_capita_spend\n\n\n\n\n\nFrom Figure 5.2 there appears to be a positive relationship between a city’s per capita expenditure and the number of playgrounds per 10,000 residents. The correlation is 0.206, indicating the relationship between playground access and city expenditure is not strong.This is partially influenced by the outlying observations in that have relatively low values of per capita expenditure but high numbers of playgrounds per 10,000 residents.\n\nLinear regression model\nTo better explore this relationship, we fit a simple linear regression model of the form\n\\[\n\\text{playgrounds} = \\beta_0 + \\beta_1~\\text{per\\_capita\\_expend} + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{5.1}\\]\nThe output of the fitted regression model is Table 5.2.\n\n\n\n\nTable 5.2: Linear regression model per_capita_expend and playgrounds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.4184\n0.2144\n11.28\n0.0000\n\n\nper_capita_expend\n0.0033\n0.0016\n2.06\n0.0424\n\n\n\n\n\n\n\n\n\\[\n\\widehat{\\text{playgrounds}} = 2.418 + 0.003 \\times \\text{per\\_capita\\_expend}\n\\tag{5.2}\\]\n\n\nInterpret the slope in the context of the data.\nDoes the intercept have a meaningful interpretation? 2\n\n\nFrom the sample of 97 cities in 2020, the estimated slope of 0.003. This estimated slope is likely close to but not the exact value of the true population slope we would obtain using data from every city in the United States. Based on the equation alone, we are also not sure if this slope indicates an actual meaningful relationship between the two variables, or if the slope is due to random variability in the data. We will use statistical inference methods to help answer these questions and use the model to draw conclusions about the relationship between per_capita_expend and playgrounds beyond these 97 cities.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-mlr-inf-objective",
    "href": "05-slr-inference.html#sec-mlr-inf-objective",
    "title": "5  Inference for simple linear regression",
    "section": "5.2 Objectives of statistical inference",
    "text": "5.2 Objectives of statistical inference\n\nBased on the regression output in Table 5.2, for each additional dollar in per capita expenditure, we expect there to be 0.003 more playgrounds per 10,000 residents, on average.\n\nThe estimate 0.003 is the “best guess” of the relationship between per capita expenditure and the number of playgrounds per 10,000 residents; however, this is likely not the exact value of the relationship in the population of all US cities. We can use statistical inference, the process of drawing conclusions about the population based on the analysis of the sample data. More specifically, we will use statistical inference to draw conclusions about the population-level slope, \\(\\beta_1\\).\nThere are two types of statistical inference procedures:\n\nHypothesis tests: Test a specific claim about the population-level slope\nConfidence intervals: A range of values that the population-level slope may reasonably take\n\nThis chapter focuses on statistical inference for the slope \\(\\beta_1\\), but the concepts introduced here can be applied to inference on the population-level intercept \\(\\beta_0\\) and other population-level parameters.\nAs we’ll see throughout the chapter, a key component of statistical inference is quantifying the sampling variability, sample-to-sample variability in the statistic that is the “best guest” estimate for the parameter. For example, when we conduct statistical inference on the slope of per capita expenditure \\(\\beta_1\\), we need to quantify the sampling the variability of the statistic \\(\\hat{\\beta}_1\\), the estimated (sample) slope. This is the amount of variability in \\(\\hat{\\beta}_1\\) that is expected if we repeated the following process many times: (1) collect a new sample that is the same size as our sample data ( 97 in this analysis), and (2) use the new sample to fit a model using per_capita_expend to predict playgrounds to obtain an estimate of the slope. The idea is that \\(\\hat{\\beta}_1\\) would not be the same for each new sample, so we need a way to quantify the variability in these estimated slopes to understand this natural variation. The \\(\\hat{\\beta}_1\\) values from the new samples make up the sampling distribution.\nWhile the process described above would be an approach for constructing the sampling distribution, it is not feasible to collect a lot of new samples in practice. Instead, there are two approaches for obtaining the sampling distribution, in order to quantify the variability in the estimated slopes and conduct statistical inference.\n\nSimulation-based methods: Quantify the sampling variability by generating a sampling distribution directly from the data\nTheory-based methods: Quantify the sampling variability using mathematical models based on the Central Limit Theorem\n\nSection 5.4 and Section 5.6 introduce statistical inference using simulation-based methods, and Section 5.8 introduces inference using theory-based methods. Before we get into those details, however, let’s introduce more of the foundational ideas underlying simple linear regression and how they relate to statistical inference.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-slr-foundation",
    "href": "05-slr-inference.html#sec-slr-foundation",
    "title": "5  Inference for simple linear regression",
    "section": "5.3 Foundations of simple linear regression",
    "text": "5.3 Foundations of simple linear regression\n\nIn Section 4.3.1, we introduced the statistical model for simple linear regression \\[\nY = \\beta_0 + \\beta_1 X + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{5.3}\\]\nsuch that \\(Y\\) is the response variable, \\(X\\) is the predictor variable, and \\(\\epsilon\\) is the error term. Equation 5.3 can be rewritten in terms of the distribution of the response variable \\(Y\\) given the predictor \\(X\\). It is represented as \\(Y|X\\) ( “\\(Y\\) given \\(X\\)”)\n\\[Y|X \\sim N(\\beta_0 + \\beta_1X , \\sigma^2_\\epsilon) \\tag{5.4}\\]\nEquation 5.4 is the assumed distribution of the response variable conditional on the predictor variable under the simple linear regression model. Therefore, we conduct simple linear regression assuming Equation 5.4 is true.  Based on the equation  we specify the assumptions that are made when we do simple linear regression. More specifically, the following assumptions are made based on Equation 5.4:\n\n\n\nThe distribution of the response \\(Y\\) is normal for a given value of the predictor \\(X\\).\nThe expected value (mean) of \\(Y|X\\) is \\(\\beta_0 + \\beta_1 X\\). There is a linear relationship between the response and predictor variable.\nThe variance \\(Y|X\\) is \\(\\sigma^2_{\\epsilon}\\). This variance is equal for all values of \\(X\\) and thus does not depend on \\(X\\).\nThe error terms for each observation, \\(\\epsilon\\) in Equation 5.3, are independent. This also means the values of the response variable, and observations more generally, are independent.\n\n\nWhenever we fit linear regression models and conduct inference on the slope, we do so under the assumption that some or all of these four statements hold. In Chapter 6, we will discuss how to check if these assumptions hold in a given analysis. As we might expect, these assumptions do not always perfectly hold in practice, so we will also discuss circumstances in which an assumption is necessary versus when an assumption can be relaxed. For the remainder of this chapter, however, we will proceed as if all four assumptions hold.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-bootstrap-ci",
    "href": "05-slr-inference.html#sec-bootstrap-ci",
    "title": "5  Inference for simple linear regression",
    "section": "5.4 Bootstrap confidence intervals",
    "text": "5.4 Bootstrap confidence intervals\nWe’ll begin by looking at simulation-based methods for statistical inference: bootstrap confidence intervals and permutation tests (Section 5.9.2). In these procedures, we use the sample data to construct a simulated sampling distribution to quantify the sample-to-sample variability in \\(\\hat{\\beta}_1\\). Let’s start with the simulation-based approach to construct confidence intervals.\nA confidence interval is a range of values the population-level slope \\(\\beta_1\\) may reasonably take. Though we have \\(\\hat{\\beta}_1\\), the best guess for the population slope (called a point estimate) , we are more likely to capture the value of the true population slope by computing a range of plausible values than by solely relying on a single estimate. We get this range by constructing \\(C\\%\\) confidence intervals, where \\(C\\%\\) is how confident we are the interval contains \\(\\beta_1\\) based on the statistical methods.\nIn order to obtain this range of values we must understand the sampling variability of the statistic. Suppose we repeatedly take samples of size \\(n\\) (the same size as the sample data) and fit regression models to compute \\(\\hat{\\beta}_1\\), the estimated slope. Recall that the sampling variability is the variability in these estimated slopes. In practice, it is generally not feasible to collect multiple samples from the population, so we use our sample data to simulate the process of obtaining new samples. We generate these new samples by bootstrapping, a simulation process in which we generate a sample of size \\(n\\) by sampling with replacement from the current data. \nWe then fit the regression model and compute \\(\\hat{\\beta}_1\\) for each bootstrap sample. These \\(\\hat{\\beta}_1\\) estimated from the bootstrap samples make up the bootstrap distribution, i.e., the simulated sampling distribution. The variability in this distribution is the sampling variability we need to construct the confidence intervals.\n\nWhy do we sample with replacement when generating a bootstrap sample? How would a bootstrap sample compare to the original sample data if sampling is done without replacement?3\n\n\n\n5.4.1 Constructing a bootstrap confidence interval for \\(\\beta_1\\)\n\nA bootstrap confidence interval for the population slope, \\(\\beta_1\\), is constructed using the following steps:\n\nGenerate \\(n_{iter}\\) bootstrap samples, where \\(n_{iter}\\) is the number of iterations. We typically want to use at least 1000 iterations in order to construct a sampling distribution that is close to the theoretical distribution of \\(\\hat{\\beta}_1\\) defined in Section 5.8. \nFit the linear regression model to each of the \\(n_{iter}\\) bootstrap samples to obtain \\(n_{iter}\\) values of \\(\\hat{\\beta}_1\\), the estimated slope. There will also be \\(n_{iter}\\) values of the estimated intercept, \\(\\hat{\\beta}_0\\), but we will ignore those for now because we are not focusing on inference for the intercept.\nCollect the \\(n_{iter}\\) values of \\(\\hat{\\beta}_1\\) from the previous step to obtain the bootstrapped sampling distribution. It is an approximation of the sampling distribution of \\(\\hat{\\beta}_1\\), and thus provides information about sample-to-sample variability of \\(\\hat{\\beta}_1\\).\nUse the distribution from the previous step to calculate the \\(C\\%\\) confidence interval. The lower and upper bounds of the interval are the points in the distribution that mark the middle \\(C\\%\\) of the distribution.\n\nUsing these four steps, let’s construct the 95% confidence for the population slope \\(\\beta_1\\) of the relationship between per_capita_spend and playgrounds.\n\nGenerate 1000 bootstrap samples (97 observations in each sample) by sampling with replacement from the current sample data of 97 observations. The first 10 observations from the first bootstrapped sample are shown in Table 5.3.\n\n\n\n\n\nTable 5.3: First 10 rows of the first bootstrap sample. The replicate column identifies the bootstrap sample.\n\n\n\n\n\n\nreplicate\nplaygrounds\nper_capita_expend\n\n\n\n\n1\n2.1\n320\n\n\n1\n1.8\n65\n\n\n1\n2.2\n67\n\n\n1\n1.0\n33\n\n\n1\n2.6\n42\n\n\n1\n2.2\n149\n\n\n1\n3.3\n73\n\n\n1\n2.2\n35\n\n\n1\n1.8\n89\n\n\n1\n1.3\n65\n\n\n\n\n\n\n\n\n\nWhy are there 97 observations in each bootstrap sample?4\n\n\nNext, we fit a linear model of the form in Equation 5.1 to each of the 1000 bootstrap samples. The estimated slopes and intercepts for the first three bootstrap samples are shown in Table 5.4.\n\n\n\n\nTable 5.4: Estimated slope and intercept for the first three bootstrap samples.\n\n\n\n\n\n\nreplicate\nterm\nestimate\n\n\n\n\n1\nintercept\n2.383\n\n\n1\nper_capita_expend\n0.005\n\n\n2\nintercept\n2.545\n\n\n2\nper_capita_expend\n0.002\n\n\n3\nintercept\n2.386\n\n\n3\nper_capita_expend\n0.005\n\n\n\n\n\n\n\n\nWe are focused on inference for \\(\\beta_1\\), the slope of per_capita_expend, so we collect estimated slopes of per_capita_expend to make the bootstrap distribution. This is the approximation of the sampling distribution of \\(\\hat{\\beta}_1\\). A histogram and summary statistics for this distribution are shown in Figure 5.3 and Table 5.5, respectively.\n\n\n\n\n\n\n\n\nFigure 5.3: Bootstrap distribution of the slope of `per_capita_expend\n\n\n\n\n\n\n\n\n\nTable 5.5: Summary statistics for bootstrap distribution of the slope per_capita_expend\n\n\n\n\n\n\nMin\nQ1\nMedian\nQ3\nMax\nMean\nStd.Dev.\n\n\n\n\n-0.001\n0.002\n0.003\n0.002\n0.01\n0.004\n0.002\n\n\n\n\n\n\n\n\n\n\nHow many values of \\(\\hat{\\beta}_1\\) make up the bootstrap sampling distribution shown in Figure 5.3 and summarized in Table 5.5 ?5\n\n\nAs the final step, we use the bootstrap distribution to calculate the lower and upper bounds of the 95% confidence interval. These bounds are calculated as the points that mark off the middle 95% of the distribution. These are the points that at the \\(2.5^{th}\\) and \\(97.5^{th}\\) percentiles, as shown by the vertical lines in Figure 5.4.\n\n\n\n\n\n\n\n\n\nFigure 5.4: 95% bootstrap confidence interval for the slope of per_capita_spend\n\n\n\n\n\n\n\n\n\nTable 5.6: 95% confidence interval for the slope\n\n\n\n\n\n\nLower bound (2.5th percentile)\nUpper bound (97.5th percentile)\n\n\n\n\n0.001\n0.007\n\n\n\n\n\n\n\n\nThe 95% bootstrapped confidence interval for \\(\\beta_1\\), the slope of per_capita_expend is 0.001 to 0.007.\n\nThe points at what percentiles in the bootstrap distribution mark the lower and upper bounds for a\n\n90% confidence interval?\n98% confidence interval?6\n\n\n\n\n5.4.2 Interpreting the interval\nThe general interpretation of the 95% confidence interval for \\(\\beta_1\\), the slope of per_capita_expend is\n\nWe are 95% confident that the interval 0.001 to 0.007 contains the population slope for per capita expenditure in the model of the relationship between a city’s per capita expenditure and number of playgrounds per 10,000 residents.\n\nThough this interpretation indicates the range of values that may reasonably contain the true population slope for per_capita_expend, it still requires the reader to further interpret what it means about the relationship between per_capita_expend and playgrounds. It is more informative to interpret the confidence interval in a way that also utilizes the interpretation of the slope from Section 5.1.1 , so the reader more clearly understands what the confidence interval is conveying. Thus, a more complete and informative interpretation of the confidence interval is as follows:\n\nWe are 95% confident that for each additional dollar a city spends per resident, there are between 0.001 to 0.007 more playgrounds per 10,000 residents, on average.\n\nThis interpretation not only indicates the range of values as before, but it also clearly describes what this range means in terms of the average change in playgrounds per 10,000 residents as a city’s per capita expenditure increases.\n\n\n5.4.3 What does “confidence” mean?\nThe beginning of the interpretation for a confidence interval is “We are \\(C\\%\\) confident…”. What does “\\(C\\%\\) confident” mean? The notion of “confidence” refers to the statistical process used construct the confidence interval. This means if we replicate the process thousands of times - obtain a sample of 97 cities, construct a bootstrap distribution for \\(\\hat{\\beta}_1\\), and calculate the bounds that mark the middle \\(C\\%\\) of the distribution, the intervals defined by the upper and lower bounds would contain the value of \\(\\beta_1\\), the true population slope, \\(C\\%\\) of the time.\nIn reality we don’t know the value of the population slope (if we did, we wouldn’t need statistical inference!), so we can’t definitively conclude if the interval constructed in Section 5.4.1 is one of the \\(C\\%\\) that contains the population slope or not. Though we aren’t certain that our interval contains the population slope, we can conclude with some level of confidence, \\(C\\%\\) confidence to be exact, that we think it does based on the process.\n\n\n\nThus far, we have used a confidence interval to produce a plausible range of values for the population slope. We can also test specific claims about the population slope using another inferential procedure called hypothesis testing.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-hypothesis-test-overview",
    "href": "05-slr-inference.html#sec-hypothesis-test-overview",
    "title": "5  Inference for simple linear regression",
    "section": "5.5 Hypothesis tests",
    "text": "5.5 Hypothesis tests\nHypothesis tests are used to evaluate a claim about about a population parameter. The claim could be based on previous research, an idea a research or business team wants to explore, or a general statement about the parameter. We will again focus on the population slope \\(\\beta_1\\). Before getting into the details of simulation-based hypothesis tests, we’ll describe the steps for a hypothesis test based on a commonly used analogy, the general procedure of a court trial in the United States (US) judicial system. \n\n\n5.5.1 Define the hypotheses\nThe first step of any hypothesis test (or trial) is to define the hypotheses that will be evaluated. These hypotheses are called the null and alternative. The null hypothesis \\((H_0)\\) is the baseline condition typically indicating no relationship between the response and predictor, and the alternative hypothesis \\((H_a)\\) is defined by the claim being tested. Typically, the claim is that there is some relationship between the two variables.\nIn the US judicial system, a defendant is deemed innocent unless proven otherwise. Therefore, the null and alternative hypotheses are \\(H_0\\): the defendant is not guilty, and \\(H_a\\): the defendant is guilty. We say that a person is “innocent until proven guilty beyond a reasonable doubt.” Therefore, the trial proceeds assuming the null hypothesis of innocence is true and the objective is to evaluate the strength of evidence against this hypothesis. The same is true for hypothesis testing in statistics. The test is conducted under the assumption that the null hypothesis, \\(H_0\\), is true, and we use statistical methods to evaluate the strength of the evidence against \\(H_0\\).\n\n\n\n5.5.2 Evaluate the evidence\nThe primary component of trial (or hypothesis test) is a presenting and evaluating the evidence. In a trial, this is the point when the evidence is presented and it is evaluated under the assumption the null hypothesis (defendant is not guilty) is true. Thus, the lens in which the evidence is being evaluated is “given the defendant is not guilty, how likely is it that this evidence would exist?”\nFor example, suppose an individual is on trial for a robbery at a jewelry store. The null hypothesis is that they are not guilty and did not rob the jewelry store. The alternative hypothesis is they are guilty and did rob the jewelry store. If there is evidence that the person was in a different city during the time of the jewelry store robbery, the evidence would be more in support of the null hypothesis of innocence. It seems plausible the individual could have been in a different city at the time of the robbery if the null hypothesis is true. Alternatively, if some of the missing jewelry was found in the individual’s car, the evidence would seem to be strongly in support of the alternative hypothesis. If the null hypothesis is true, it does not seem likely that the individual would have the missing jewelry in their car.\nIn hypothesis testing, the “evidence” being assessed is the analysis of the sample data. Thus we are considering the question “given the null hypothesis is true, how likely is it to observe the results seen in the sample data?” We will introduce approaches to address this question using simulation-based methods in Section 5.6 and theory-based methods in Section 5.8.3.\n\n\n5.5.3 Make a conclusion\nThere are two typical conclusions in a trial in the US judicial system - the defendant is guilty or not guilty based on the evidence. The criteria to conclude the alternative that a defendant is guilty is that the strength of evidence must be “beyond reasonable doubt”. If there is sufficiently strong evidence against the null hypothesis of not guilty, then the conclusion is the alternative hypothesis that the defendant is guilty. Otherwise, the conclusion is that the defendant is not guilty, indicating the evidence against the null was not strong enough to otherwise refute it. Note that this is the not the same as “accepting” the null hypothesis but rather indicating that there wasn’t enough evidence to suggest otherwise.  \nSimilarly in hypothesis testing, we will use a predetermined threshold to assess if the evidence against the null hypothesis is strong enough to reject the null hypothesis and conclude the alternative, or if there is not enough evidence “beyond a reasonable doubt” to draw a conclusion other than the assumed null hypothesis.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-slr-testing-sim",
    "href": "05-slr-inference.html#sec-slr-testing-sim",
    "title": "5  Inference for simple linear regression",
    "section": "5.6 Permutation tests",
    "text": "5.6 Permutation tests\nNow that we have explained the general process of hypothesis testing, let’s take a look at hypothesis testing using a simulation-based approach, called a permutation test.\nThe four steps of permutation test for a slope \\(\\beta_1\\) are\n\nState the null and alternative hypotheses.\nGenerate the null distribution.\nCalculate the p-value.\nDraw a conclusion.\n\nThese steps are described in detail in the context of the hypothesis test for the slope in Equation 5.1.\n\n5.6.1 State the hypotheses\nAs defined in Section 5.5 the null hypothesis (\\(H_0\\)) is the baseline condition, and the alternative hypothesis (\\(H_a\\)) is defined by the claim being tested. Recall from Section 5.1 that one objective for the analysis in this chapter is to investigate whether per capita expenditure is useful in understanding variability in playground access in US cities. In terms of the linear regression model, the claim being tested is whether there is a linear relationship between per_capita_expend and playgrounds. The null hypothesis is the baseline condition of there being no linear relationship between the two variables. We use this claim to define the alternative hypothesis.\n\nNull hypothesis: There is no linear relationship between playgrounds per 10,000 residents and per capita expenditure. The slope of per_capita_expend is equal to 0. \\((H_0: \\beta_1 = 0)\\)\nAlternative hypothesis: There is a linear relationship between playgrounds per 10,000 residents and per capita expenditure. The coefficient of per_capita_expend is not equal to 0. \\((H_a: \\beta_1 \\neq 0)\\)\n\nNote that we have not hypothesized whether the slope is positive or negative.\n\n\nThe hypotheses are defined specifically in terms of the linear relationship between the two variables, because we are ultimately drawing conclusions about the slope \\(\\beta_1\\).\n\nMathematical statement of hypotheses for \\(\\beta_1\\)\nSuppose there is a response variable \\(Y\\) and a predictor variable \\(X\\) such that\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n\\]\nThe hypotheses for testing whether there is a linear relationship between \\(X\\) and \\(Y\\) in the population are\n\\[\n\\begin{aligned}\n&H_0: \\beta_1 = 0\\\\\n&H_a: \\beta_1 \\neq 0\n\\end{aligned}\n\\tag{5.5}\\]\n\n\nOne vs. two-sided hypotheses\nThe alternative hypothesis defined in Equation 5.5 is “not equal to 0”. This is the alternative hypothesis corresponding to a two-sided hypothesis test, because it includes the scenarios in which \\(\\beta_1\\) is less than or greater than 0. There are two other options for defining the alternative hypothesis, “\\(\\beta_1\\) is less than 0” (\\(H_a: \\beta_1 &lt; 0\\)) and “\\(\\beta_1\\) is greater than 0” (\\(H_a: \\beta_1 &gt; 0\\)). These are one-sided hypothesis tests, as they only consider the alternative scenario in which \\(\\beta_1\\) is either less than or greater than 0, respectively.\nA one-sided hypothesis test imposes some information about the direction of the parameter, that is positive (\\(&gt; 0\\)) or negative ( \\(&lt; 0\\)). Given this additional information imposed by the direction of the alternative hypothesis, it requires less evidence to reject the null hypothesis in favor of the alternative. Therefore, it is best to use a one-sided hypothesis only if (1) there is some indication from previous knowledge or research that the relationship between the response variable and the predictor variable is in a particular direction, or (2) only one direction of the relationship between the response and predictor variables is relevant in practice. Outside of these two scenarios, it is not advisable to use the one-sided hypothesis, as there could appear to be a statistically significant relationship between the two variables merely by chance of how the hypotheses were constructed.\nBecause a two-sided hypothesis test makes no assumption about the direction of the relationship between the response variable and predictor variable. It is a good starting point for drawing conclusions about the relationship between the two variables. From the two-sided hypothesis, we will conclude whether there is or is not sufficient statistical evidence of a linear relationship between the response and predictor. With this conclusion, we cannot determine if the relationship between the variables is positive or negative without additional analysis. We use a confidence interval (Section 5.4) to make specific conclusions about the direction and magnitude of the relationship.\n\n\n\n5.6.2 Simulate the null distribution\nRecall that hypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true. Based on the hypotheses defined in Section 5.6.1, a hypothesis test for the slope is conducted under the assumption \\(\\beta_1 = 0\\) , that there is no linear relationship between the response and predictor variables.\nTo assess the evidence, we will use a simulation-based method to approximate the sampling distribution of the estimated slope \\(\\hat{\\beta}_1\\) under the assumption that \\(H_0: \\beta_1 = 0\\) is true. This distribution, called the null distribution, allows us to understand the sample-to-sample variability under the scenario in which the true population slope equals 0. The variability in the simulated null distribution will be the same (or very similar since we are working with simulated data) as the variability in the bootstrap distribution, but the difference between the two distributions is the location of the center. The center for the bootstrap distribution in Section 5.4 is close to the value of \\(\\hat{\\beta}_1\\) estimated from the data. The center of the null distribution, however, is the null hypothesized value of 0. Therefore, to construct the null distribution for hypothesis testing, we will use a different simulation method, called permutation sampling.\nIn permutation sampling the values of the predictor variable are randomly shuffled and paired with values of the response, thus generating a new sample of the same size as the original data. The process of randomly pairing the values of the response and the predictor variables simulates the null hypothesized condition that there is no linear relationship between the two variables.\nThe steps for simulating the null distribution using permutation sampling are the following:\n\nGenerate \\(n_{iter}\\) permutation samples, where \\(n_{iter}\\) is the number of iterations. We ideally use at least 1,000 iterations in order to construct a distribution that is close to the theoretical null distribution defined in Section 5.8.\nFit the linear regression model to each of the \\(n_{iter}\\) permutation samples to obtain \\(n_{iter}\\) values of \\(\\hat{\\beta}_1\\), the estimated slope. There will also be \\(n_{iter}\\) values of \\(\\hat{\\beta}_0\\) the estimated intercepts; we will ignore those for now because we are focused on inference for the slope.\nCollect the \\(n_{iter}\\) values of \\(\\hat{\\beta}_1\\) from the previous step to make the simulated null distribution. This is an approximation of the distribution of \\(\\hat{\\beta}_1\\) values if we were to repeatedly take samples the same size as the original data and fit the linear regression model to each sample, under the assumption that the null hypothesis is true.\n\nLet’s simulate the null distribution to test the hypotheses in Equation 5.5 for the parks data.\n\nFirst we generate 1,000 permutation samples, such that in each sample, we permute the values of per_capita_expend, randomly pairing each to a value of playgrounds. This is to simulate the scenario in which there is no linear relationship between per_capita_expend and playgrounds. The first 10 rows of the first permutation sample are in Table 5.7.\n\n\n\n\n\nTable 5.7: First 10 rows of the first permutation sample. The replicate column identifies the permutation sample.\n\n\n\n\n\n\nreplicate\nplaygrounds\nper_capita_expend\n\n\n\n\n1\n2.1\n319\n\n\n1\n1.8\n307\n\n\n1\n2.2\n219\n\n\n1\n1.0\n301\n\n\n1\n2.6\n190\n\n\n1\n2.2\n250\n\n\n1\n3.3\n215\n\n\n1\n1.8\n399\n\n\n1\n1.8\n162\n\n\n1\n1.9\n179\n\n\n\n\n\n\n\n\n\nNext, we fit a linear regression model to each of the 1000 permutation samples. This gives us 1000 estimates of the slope and intercept. The slopes estimated from the first 10 permutation samples are shown in Table 5.8.\n\n\n\n\n\nTable 5.8: Estimated slopes from first 10 permutation samples.\n\n\n\n\n\n\nreplicate\nterm\nestimate\n\n\n\n\n1\nper_capita_expend\n0.000\n\n\n2\nper_capita_expend\n0.001\n\n\n3\nper_capita_expend\n0.001\n\n\n4\nper_capita_expend\n-0.002\n\n\n5\nper_capita_expend\n-0.002\n\n\n6\nper_capita_expend\n0.004\n\n\n7\nper_capita_expend\n-0.002\n\n\n8\nper_capita_expend\n-0.002\n\n\n9\nper_capita_expend\n-0.001\n\n\n10\nper_capita_expend\n0.001\n\n\n\n\n\n\n\n\n\nNext, we collect the estimated slopes from the previous step to construct the simulated null distribution. We will use this distribution to assess the strength of the evidence from the original sample data against the null hypothesis.\n\n\n\n\n\n\n\n\nFigure 5.5: Simulated null distribution to test the slope of per_capita_expend\n\n\n\n\n\n\n\n\n\n\nTable 5.9: Summary statistics of the simulated null distribution to test the slope of per_capita_expend\n\n\n\n\n\n\nMin\nQ1\nMedian\nQ3\nMax\nMean\nStd.Dev.\n\n\n\n\n-0.005\n-0.001\n0\n-0.001\n0.006\n0\n0.002\n\n\n\n\n\n\n\n\nNote that the distribution visualized in Figure 5.5 and summarized in Table 5.9 is approximately unimodal, symmetric, and looks similar to the normal distribution. As the number of iterations (permutation samples) increases, the simulated null distribution will be closer and closer to a normal distribution. Additionally, the center of the distribution is approximately 0, the null hypothesized value. The standard deviation of this distribution 0.002 is an estimate of the standard error of \\(\\hat{\\beta}_1\\), the sample-to-sample variability in the estimates of \\(\\hat{\\beta}_1\\) when taking random samples of size 97, the same size as the original data.\n\nHow does the estimated variability in the simulated null distribution in Table 5.9 compare to the variability in the bootstrapped distribution in Table 5.5? Is this what you expected? Why or why not?7\n\n\n\n\n\n5.6.3 Calculate p-value\nThe null distribution helps us understand the values \\(\\hat{\\beta}_1\\), the slope of per_capita_expend, is expected to take if we repeatedly take random samples and fit a linear regression model, assuming the null hypothesis \\(\\beta_1 = 0\\) is true. To evaluate the strength of evidence against the null hypothesis, we will compare the estimated slope in Table 5.2, \\(\\hat{\\beta}_1 =\\) 0.003 (the evidence) to what we would expect \\(\\hat{\\beta}_1\\) to be based on the null distribution.\nThis comparison is quantified using a p-value. The p-value is the probability of observing estimated slopes at least as extreme as the value estimated from the sample data, given the null hypothesis is true. In the context of the parks data, the p-value is the probability of observing values of the slope that are at least as extreme as \\(\\hat{\\beta}_1 =\\) 0.003 in the null distribution.\nIn the context of statistical inference, the phrase “more extreme” means the area between the estimated value ( \\(\\hat{\\beta}_1\\) in our case), and the outer tail(s) of the simulated null distribution. The alternative hypothesis determines which tail(s) to include when calculating the p-value.\n\nIf \\(H_a: \\beta_1 &gt; 0\\), the p-value is the probability of obtaining a value in the null distribution that is greater than or equal to \\(\\hat{\\beta}_1\\).\nIf \\(H_a: \\beta_1 &lt; 0\\), the p-value is the probability of obtaining a value in the null distribution that is less than or equal to \\(\\hat{\\beta}_1\\).\nIf \\(H_a: \\beta_1 \\neq 0\\), the p-value is the probability of obtaining a value in the null distribution whose absolute value is greater than or equal to \\(\\hat{\\beta}_1\\) . This includes values that are greater than or equal to \\(|\\hat{\\beta}_1|\\) or less than or equal to \\(-|\\hat{\\beta}_1|\\).\n\n\nRecall from Section 5.6.1 that we are testing a two-sided alternative hypothesis. Therefore, we will calculate the p-value corresponding to the alternative hypothesis \\(H_a: \\beta_1 \\neq 0\\). As illustrated in Figure 5.6, this p-value is the probability of observing the slope that is 0.003 or more extreme, given the null hypothesis is true. In this case, it is the probability of observing a value in the null distribution that is greater than or equal to |0.003| or a value that is less than or equal to -|0.003|.\n\nThe p-value for this hypothesis test is 0.046 and is shown by the dark shaded area in Figure 5.6.\n\n\n\n\n\n\n\n\n\nFigure 5.6: Simulated null distribution with p-value represented by the shaded area. The shaded area are values less than -0.003 and greater than 0.003.\n\n\n\n\n\n\nUse the definition of the p-value at the beginning of this section to interpret the p-value of 0.046 in the context of the data.8\n\n\n\n\n5.6.4 Draw conclusion\nWe ultimately want to evaluate the strength of evidence against the null hypothesis. The p-value is a measure of the strength of that evidence and is used to draw one of the following conclusions:\n\nIf the p-value is “sufficiently small”, there is strong evidence against the null hypothesis. We reject the null hypothesis, \\(H_0\\), and conclude the alternative \\(H_a\\).\nIf the p-value is not “sufficiently small”, there is not strong enough evidence against the null hypothesis. We fail to reject the null hypothesis, \\(H_0\\), and stay with the null hypothesis.\n\n\nWe use a predetermined decision-making threshold called an \\(\\boldsymbol{\\alpha}\\)-level to determine if a p-value is sufficiently small enough to reject the null hypothesis.\n\nIf \\(\\text{p-value} &lt; \\alpha\\), then reject \\(H_0\\)\nIf \\(\\text{p-value} \\geq \\alpha\\), then fail to reject \\(H_0\\).\n\nA commonly used threshold is \\(\\alpha = 0.05\\). If stronger evidence is required to reject the null hypothesis, then a lower threshold can be used to make a conclusion. If such strong evidence is not required (this may be the case in analyses with very small sample sizes), then a threshold can be used. It is general convention to use a threshold 0.1 or less, so any p-value 0.1 is considered large enough to fail to reject the null hypothesis.\nBack to the parks analysis. We will use the common threshold of \\(\\alpha = 0.05\\).\n\nThe p-value calculated in the previous section is 0.046. Therefore, we reject the null hypothesis \\(H_0\\). The data provide sufficient evidence of a linear relationship between the amount a city spends per resident and the number of playgrounds per 10,000 residents.\n\n\n\n5.6.5 Type I and Type II error\nRegardless of the conclusion that is drawn (reject or fail to reject the null hypothesis), we have not determined that the null or alternative hypothesis are definitive truth. We have just concluded that the evidence (the data) has provided more evidence in favor of one conclusion versus the other. As with any statistical procedure, there is the possibility of making an error, more specifically a Type I or Type II error. Because we don’t know the value of the population slope, we will not know for certain whether we have made an error; however, understanding the potential errors that can be made can help inform the decision-making threshold \\(\\alpha\\) and make a more informed assessment about the implication of these results in practice.\nTable 5.10 shows how Type I and Type II errors correspond to the (unknown) truth and the conclusion drawn from the hypothesis test.\n\n\n\n\nTable 5.10: Type I and Type II error\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\n\n\n\n\n\n\\(H_0\\) true\n\\(H_a\\) true\n\n\nHypothesis test decision\nFail to reject \\(H_0\\)\nCorrect decision\nType II error\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect decision\n\n\n\n\n\n\nA Type I error has occurred if the null hypothesis is actually true, but the p-value is small enough to reject the null hypothesis. The probability of making this type of error is the decision-making threshold \\(\\alpha\\). This is because \\(P(\\text{reject }H_0 | H_0 \\text{ true}) = \\alpha\\), meaning the probability of rejecting the null hypothesis given the null is true is \\(\\alpha\\).\nA Type II error has occurred if the alternative hypothesis is actually true, but we fail to reject the null hypothesis, because the p-value is large. Computing the probability of making this type of error is less straightforward. It is calculated as \\(1 - Power\\) , where the \\(Power = P(\\text{reject }H_0 | H_a \\text{ true})\\). \nIn the context of the parks data, a Type I error is concluding that there is a linear relationship between per capita expenditure and playgrounds per 10,000 residents in the model, when there actually isn’t one in the population. A Type II error is concluding there is no linear relationship between per capita expenditure and playgrounds per 10,000 residents when in fact there is.\n\nGiven the conclusion in Section 5.6.4, is it possible we’ve made a Type I or Type II error?9",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-slr-inf-relationship-ci-test",
    "href": "05-slr-inference.html#sec-slr-inf-relationship-ci-test",
    "title": "5  Inference for simple linear regression",
    "section": "5.7 Relationship between confidence intervals and hypothesis tests",
    "text": "5.7 Relationship between confidence intervals and hypothesis tests\nAt this point, we might wonder whether there is any connection between the confidence intervals and hypothesis tests. Spoiler alert: there is!\nTesting a claim with the two-sided alternative \\(H_a: \\beta_1 \\neq 0\\) and decision-making threshold \\(\\alpha\\) is equivalent to using the \\(C\\%\\) confidence interval to evaluate the claim, where \\(C = (1 - \\alpha)\\times100\\). This means we can also use confidence intervals to evaluate two-sided hypotheses. When using a confidence interval to draw conclusions about a claim, we use the following guide:\n\nIf the null hypothesized value ( \\(0\\) based on the tests defined in Section 5.6.1 ) is within the range of the confidence interval, fail to reject \\(H_0\\) at the \\(\\alpha\\)-level.\nIf the null hypothesized value is not within the range of the confidence interval, reject \\(H_0\\) at the \\(\\alpha\\)-level.\n\nThis illustrates the power of confidence intervals; they can not only be used to draw a conclusion about a claim (reject or fail to reject \\(H_0\\)), but they also give the range values that the population slope may take. Thus, it is good practice to always report the confidence interval, because the confidence interval provides more detail about a population slope \\(\\beta_1\\) beyond the reject/fail to reject conclusion of the hypothesis test.\n\nWhen we reject a null hypothesis, we conclude that there is a statistically significant linear relationship  between the response and predictor variables. Concluding there is statistically significant relationship between the response and predictor, however, does not necessarily mean that the relationship is practically significant. The practical significance, how meaningful the results are in the real world, is determined by the magnitude of the estimated slope of the predictor on the response and what an effect of that magnitude means in the context of the data and analysis question.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#sec-slr-inf-clt",
    "href": "05-slr-inference.html#sec-slr-inf-clt",
    "title": "5  Inference for simple linear regression",
    "section": "5.8 Theory-based inference",
    "text": "5.8 Theory-based inference\n\n\nThus far we have approached inference using simulation-based methods (bootstrapping and permutation) to generate sampling distributions and null distributions. When certain conditions are met, however, we can use theoretical results about the sampling distribution to understand the variability in \\(\\hat{\\beta}_1\\). In this section, we present that theory, then use it to conduct statistical inference for the population slope. Notice as we go through this section is that the inferential procedures and conclusions are very similar as before. The primary difference is in how we understand the sampling variability in \\(\\hat{\\beta}_1\\) and obtain the null distribution.\n\n5.8.1 Central Limit Theorem\n\nThe Central Limit Theorem (CLT) is a foundational theorem in statistics about the distribution of a statistic and the associated mathematical properties of that distribution. For the purposes of this text, we will focus on what the Central Limit Theorem says the distribution of an estimated slope \\(\\hat{\\beta}_1\\), but note that this theorem applies to statistics other than the slope. We will also focus on the results of the theorem and less so on derivations or advanced mathematical details of the Central Limit Theorem.\nBy the Central Limit Theorem, we know under certain conditions (more on these conditions in the Chapter 6) \\[\\hat{\\beta}_1 \\sim N(\\beta_1, SE_{\\hat{\\beta}_1}) \\tag{5.6}\\]\nEquation 5.6 means that by the Central Limit Theorem, we know that the sampling distribution of \\(\\hat{\\beta}_1\\) is (1) normal, (2) with a expected value at the true slope \\(\\beta_1\\), and (3) a standard error of \\(SE_{\\hat{\\beta}_1}\\)10. The center of this distribution \\(\\beta_1\\) is unknown; the purpose of statistical inference is to draw conclusions about \\(\\beta_1\\). The standard error of this distribution \\(SE_{\\hat{\\beta}_1}\\) is\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_{\\epsilon}\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\tag{5.7}\\]\nwhere \\(n\\) is the number of observations, \\(s_X^2\\) is the variance of the predictor variable \\(X\\), and \\(\\hat{\\sigma}_{\\epsilon}\\) is the regression standard error, the variability of the observations around the regression line. The regression error is introduced in more detail in Section 5.8.2. The details of how this formula is derived is beyond the scope of this text, but let’s take a moment to think about what we know about the sampling variability of \\(\\hat{\\beta}_1\\) from Equation 5.7.\nThe regression standard error \\(\\hat{\\sigma}_\\epsilon\\) is in the numerator, indicating one would expect more variability in \\(\\hat{\\beta}_1\\) when there is more variability in the data about the regression line. The variability in the predictor variable \\(s_X^2\\) is in the denominator, indicating we expect more variability in \\(\\hat{\\beta}_1\\) when there is less variability in the values of the predictor. Therefore, \\(SE_{\\hat{\\beta}_1}\\) is the balance between the variability about the regression line and the variability in the predictor variable itself.\nAs you will see in the following sections, we will use this estimate of the sampling variability in the estimated slope to draw conclusions about the true relationship between the response and predictor variables based on hypothesis testing and confidence intervals.\n\n\n5.8.2 Estimating \\(\\sigma_{\\epsilon}\\)\nAs discussed in Section 4.3.1, there are three parameters that need to be estimated for simple linear regression \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma_{\\epsilon}\\). Section 4.4 introduced least squares regression, a method for deriving \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) the estimates for intercept and slope, respectively. Now, we turn to the estimation of \\(\\sigma_\\epsilon\\), also known as the regression standard error.\n\nBy obtaining the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we have the equation of the regression line and therefore can estimate the expected value \\(Y\\) given a value of the predictor \\(X\\). We can then use the residuals (\\(e_i = y_i - \\hat{y}_i\\)) to estimate the variability of the distribution of the response variable about the regression line (see Section 5.3). The regression standard error is the estimate of this variability as it is the estimated standard deviation of the distribution of the errors (Equation 5.3). The equation for the regression standard error is shown in Equation 5.8 11.\n\\[\n\\hat{\\sigma}_{\\epsilon} = \\sqrt{\\frac{\\sum_{i = 1}^ne_i^2}{n - 2}} = \\sqrt{\\frac{\\sum_{i = 1}^n(y_i - \\hat{y}_i)^2}{n - 2}}\n\\tag{5.8}\\]\n\nYou may have noticed that the denominator in Equation 5.8 is \\(n - 2\\) not \\(n\\) or \\(n-1\\). The value \\(n - 2\\) is called the degrees of freedom (df). The degrees of freedom are how many observations are available to understand variability about the regression line. We need at a minimum two observations to estimate the equation for the simple linear regression line, i.e., it takes at a minimum two points to make a line. The remaining \\(n-2\\) observations help us understand variability about the line. We will talk more about how we use degrees of freedom as we define the distribution used to compute confidence intervals and conduct hypothesis tests. \n\nRecall that the standard deviation is the average distance between each observation and the mean of the distribution. Similarly, the regression standard error can be thought of as the average distance the observed value the response is from the regression line. The regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is used to quantify the sampling variability in the estimated slope \\(\\hat{\\beta}_1\\) (Equation 5.7), so we will use this value as we conduct inference on the slope.\n\n\n5.8.3 Hypothesis test for the slope\nThe overall goals of hypothesis tests for a population slope are the same when using theory-based methods as previously described in Section 5.5. We define a null and alternative hypothesis, conduct testing assuming the null hypothesis is true, and draw a conclusion based on an evaluation of the strength of evidence against the null hypothesis. The main difference from the simulation-based approach in Section 5.6 is in how we quantify the variability in \\(\\hat{\\beta}_1\\) and thus obtain the null distribution. In Section 5.6.2 we used permutation sampling to generate the null distribution. We’ll see that by the Central Limit Theorem we have results that specify exactly how the null distribution is defined.\nThe steps for conducting a hypothesis test based on the Central Limit Theorem are the following:\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate a p-value.\nDraw a conclusion.\n\nAs in Section 5.6 the goal is to use hypothesis testing to determine whether there is evidence of a statistically significant linear relationship between a response and predictor variable, corresponding to the two-sided alternative hypothesis of “not equal to 0”. Therefore, the null and alternative hypotheses are the same as defined in Equation 5.5 \\[\n\\begin{aligned}\nH_0: \\beta_1 = 0 \\\\\nH_a: \\beta_1 \\neq 0 \\\\\n\\end{aligned}\n\\]\nThe next step is to calculate a test statistic. Similar to a \\(z\\)-score in the Normal distribution, the test statistic tells us how far the observed slope is from the hypothesized center of the distribution. The general form of a test statistic is\n\\[\nT = \\frac{\\text{estimate } - \\text{ hypothesized}}{\\text{standard error}}\n\\]\nMore specifically, in the hypothesis test for \\(\\beta_1\\), the test statistic is\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}}\n\\tag{5.9}\\]\nTo calculate the test statistic, the estimated slope is shifted by the mean, and then rescaled by the standard error. Let’s consider what we learn from the test statistic. Recall that by the Central Limit Theorem, the distribution of \\(\\hat{\\beta}_1\\) is \\(N(\\beta_1, SE_{\\hat{\\beta}_1})\\). Because we conduct hypothesis testing under the assumption the null hypothesis is true, we are assuming that the mean of this distribution of \\(\\beta_1= 0\\).\nSince the hypothesized mean is \\(\\beta_1 = 0\\), we shift by 0 and rescale by \\(SE_{\\hat{\\beta}_1}\\), defined in Equation 5.7. Thus, the test statistic is the number of standard errors the estimated slope is from the hypothesized mean of the sampling distribution. The magnitude of the test statistic \\(|T|\\) provides a measure of how far the observed slope is from the center of the distribution, and the sign of \\(T\\) indicates whether the observed slope is above (positive sign) or below (negative sign) the hypothesized mean of 0.\n\nConsider the magnitude of the test statistic, \\(|T|\\). Do you think test statistics with small magnitude provide evidence in support or against the null hypothesis? What about test statistics with large magnitude?12\n\nNext, we use the test statistic to calculate a p-value and we will ultimately use the p-value to draw a conclusion about the strength of the evidence against the null hypothesis, as before. The test statistic, \\(T\\), follows a \\(t\\) distribution with \\(n -2\\) degrees of freedom, denoted as \\(T \\sim t_{n-2}\\) . Similar to the simulated null distribution in Section 5.6.2, we use this \\(t_{n-2}\\) distribution to evaluate how far the estimated slope is from what we would expect given the null hypothesis is true.\nThough the sampling distribution of \\(\\hat{\\beta}_1\\) is normal by the Central Limit Theorem (Equation 5.6), the test statistic follows a \\(t\\) distribution. The \\(t\\) distribution is used, because the value \\(SE_{\\hat{\\beta}_1}\\) in the test statistic is calculated using the regression standard error, \\(\\hat{\\sigma}_\\epsilon\\) (see Equation 5.7) We know the estimates \\(\\hat{\\sigma}_\\epsilon\\) and \\(SE_{\\hat{\\beta}_1 }\\) are likely not equal to the true population values, so we need a distribution that allows for a bit more variability when calculating the p-value. The \\(t\\) distribution better accounts for this extra variability compared to the standard normal distribution.\n\nFigure 5.7 shows the standard normal distribution \\(N(0,1)\\) and the \\(t\\) distribution for different degrees of freedom. The \\(t\\) distribution is very similar to the standard normal distribution: they are both centered at 0 and have a shape that is unimodal and symmetric. In other words, they both look like “bell curves” centered at 0. The difference is that the \\(t\\) distribution allows for more variability than what is expected in the standard normal distribution. This is also referred to as having “heavier tails”. The \\(t\\) distribution has more area under the curve in the tails (or more extreme values) of the distribution, meaning that more extreme values are more likely under the \\(t\\) distribution than under the \\(N(0,1)\\) distribution. This is most clearly seen by th comparing the height of tails for \\(t_2\\) and \\(N(0,1)\\). As the degrees of freedom increase, the \\(t\\) distribution becomes closer to the the standard normal distribution.\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Standard normal vs. t distributions\n\n\n\n\n\n\nAs described in Section 5.6.3, because the alternative hypothesis is “not equal to”, the p-value is calculated on both the high and low extremes of the distribution as shown in Equation 5.10.\n\\[\n\\begin{aligned}\n&\\text{p-value} = Pr(|t| &gt; |T|) = Pr(t &lt; -|T| \\text{ or } t &gt; |T|) \\\\[5pt]\n&\\text{where } t \\sim t_{n-2}\n\\end{aligned}\n\\tag{5.10}\\]\nWe compare the p-value to a decision-making threshold \\(\\alpha\\) to draw final conclusions. If \\(\\text{p-value} &lt; \\alpha\\) , we reject the null hypothesis and conclude the alternative. Otherwise, we fail to reject the null hypothesis. See Section 5.6.4 for more detail about using the \\(\\alpha\\)-level and p-value to draw conclusions.\nNow let’s apply this process to test whether there is evidence of a linear relationship between per capita expenditure and the number of playgrounds per 10,000 residents. As before, the null and and alternative hypotheses are\n\\[\n\\begin{aligned}\n&H_0: \\beta_{1} = 0 \\\\\n&H_a: \\beta_{1} \\neq 0\n\\end{aligned}\n\\]\nwhere \\(\\beta_1\\) is the true slope between per_capita_expend and playgrounds. The observed slope from Table 5.2, we know the observed slope \\(\\hat{\\beta}_{1}\\) is 0.003 and the estimated standard error \\(SE_{\\hat{\\beta}_{1}}\\) is 0.002. The test statistic is\n\\[\nT = \\frac{0.0033 - 0}{0.0016} = 2.063\n\\]\nThis test statistic means that assuming the true slope of per_capita_expend in this model is 0 and thus the mean of the distribution of \\(\\hat{\\beta}_{1}\\) is 0, the observed slope of 0.003 is 2.063 standard errors above this hypothesized mean. It’s difficult to determine whether or not this is really “far enough” away from the center of the distribution, but we can calculate a p-value to determine the probability of observing a slope at least this far given the null hypothesis is true.\nGiven there are \\(n\\) = 97 observations, the test statistic follows a \\(t\\) distribution with \\(97 - 2 =\\) 95 degrees of freedom. The p-value, is \\(Pr(t &lt; - |2.063| \\text{ or } t &gt; |2.063|) =\\) 0.042.\nUsing a decision-making threshold \\(\\alpha = 0.05\\), the p-value \\(0.042\\) is sufficiently small, so we reject the null hypothesis. The data provide sufficient evidence that the coefficient of per_capita_expend is not 0 in this model and that there is a statistically significant linear relationship between a city’s per capita expenditure and playgrounds per 10,000 residents in US cities.\nNote that this conclusion is the same as in Section 5.6.4 using a simulation-based approach (even with small differences in the p-value). This is what we would expect, given these are the two different approaches for conducting the same inferential process. We are also conducting the tests under the same assumptions that the null hypothesis is true. The difference is in the methods available to quantify \\(SE_{\\hat{\\beta}_1}\\), simulation-based versus theory-based.\n\n\n5.8.4 Confidence interval\nAs with simulation-based inference, a confidence interval calculated based on the results from the Central Limit Theorem is an estimated range of the values that \\(\\beta_1\\) can reasonably take. The purpose, interpretation, and conclusions drawn from confidence intervals are the same as described before in Section 5.4. What differs, however, is how the interval is calculated. In simulation-based inference, we used bootstrapping to construct a sampling distribution to understand sample-to-sample variability in \\(\\beta_1\\). By the Central Limit Theorem, we know exactly how to quantify the sample-to-sample variability in \\(\\hat{\\beta}_1\\) using theoretical results.\nThe equation for a \\(C\\%\\) confidence interval for \\(\\beta_1\\) is\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\tag{5.11}\\]\nwhere \\(t^* \\sim t_{n - 2}\\).\nIn Section 5.8.1, we discussed \\(\\hat{\\beta}_1\\) and its standard error \\(SE_{\\hat{\\beta}_1}\\). Now we’ll focus on \\(t^*\\), known as the critical value.\nThe critical value is the point on the \\(t_{n-2}\\) distribution such that the probability of being between \\(-t^*\\) and \\(t^*\\) is \\(C\\%\\). Thinking about this visually, this is the point such that the \\(C\\%\\) of the area under the curve is between \\(-t^*\\) and \\(t^*\\). Note that we are still using a \\(t\\) distribution with \\(n - 2\\) degrees of freedom, the same distribution used to calculate the p-value in the hypothesis tests. The critical value can be calculated from modern statistical software or using online apps (more on this in Section 5.9.3).\n\nLet’s calculate the 95% confidence interval for the slope of per_capita_spend. There are 97 observations, so we use the \\(t\\) distribution with 95 degrees of freedom. The critical value on the \\(t_{95}\\) distribution is 1.985. Plugging these values into Equation 5.11, the 95% confidence interval is\n\\[\n\\begin{aligned}\n0.0033 \\pm 1.985 \\times 0.0016 \\\\\n0.0033 \\pm 0.0032 \\\\\n[0.0001, 0.0065]\n\\end{aligned}\n\\tag{5.12}\\]\nThe interpretation is the same as before: We are 95% confident that the interval 0.0001 to 0.0065 contains the true slope for per_capita_expend. This means we are 95% confident that for each additional dollar increase in per capita expenditure, there are 0.0001 to 0.0065 more playgrounds per 10,000 residents, on average.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#inference-in-r",
    "href": "05-slr-inference.html#inference-in-r",
    "title": "5  Inference for simple linear regression",
    "section": "5.9 Inference in R",
    "text": "5.9 Inference in R\n\n5.9.1 Bootstrap confidence intervals in R\nThe bootstrap distribution and confidence interval are computed using the infer package (Couch et al. 2021). Because bootstrapping is a random sampling process, the code begins with set.seed() to ensure the results are reproducible. Any integer value can go inside the set.seed function.\n\n1set.seed(12345)\n\n2niter = 1000\n\n3boot_dist &lt;- parks |&gt;\n4  specify(playgrounds ~ per_capita_expend) |&gt;\n5  generate(reps = niter, type = \"bootstrap\") |&gt;\n6  fit()\n\n\n1\n\nSet a seed to make the results reproducible.\n\n2\n\nDefine the number of bootstrap samples (iterations). Bootstrapping can be computing intensive when using large data sets and a large number of iterations. We recommend using a small number of iterations ( 10 - 100) when testing code, then increasing the iterations once the code is finalized.\n\n3\n\nSpecify the data set and save the bootstrap distribution in the object boot_dist.\n\n4\n\nSpecify the response and predictor variable.\n\n5\n\nSpecify the type of simulation (“bootstrap”) and the number of iterations.\n\n6\n\nFor each bootstrap sample, fit the linear regression model.\n\n\n\n\nWe can use ggplot to make a histogram of the bootstrap distribution.\n\nboot_dist |&gt;\n  filter(term == \"per_capita_expend\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nFinally, we can compute the lower and upper bounds for the confidence interval using the quantile function. Note that the code includes ungroup() , so that the data are not grouped by replicate.\n\nboot_dist |&gt; \n  ungroup() |&gt;\n  filter(term == \"per_capita_expend\") |&gt;\n  summarise(lb = quantile(estimate, 0.025),\n            ub = quantile(estimate, 0.975))\n\n# A tibble: 1 × 2\n        lb      ub\n     &lt;dbl&gt;   &lt;dbl&gt;\n1 0.000735 0.00711\n\n\n\n\n5.9.2 Permutation tests in R\nThe null distribution and p-value for the permutation test are computed using the infer package (Couch et al. 2021). Much of the code to generate the null distribution is similar to the code for the bootstrap distribution. Because permutation sampling is a random process, the code starts with set.seed() to ensure the results are reproducible.\n\n1set.seed(12345)\n\n2niter = 1000\n\n3null_dist &lt;- parks |&gt;\n4  specify(playgrounds ~ per_capita_expend) |&gt;\n5   hypothesize(null = \"independence\") |&gt;\n6  generate(reps = niter, type = \"permute\") |&gt;\n  fit()\n\n\n1\n\nSet a seed to make the results reproducible.\n\n2\n\nDefine the number of bootstrap samples (iterations). Permutation sampling can be computing intensive when using large data sets and a large number of iterations. We recommend using a small number of iterations ( 10 - 100) when testing code, then increasing the iterations once the code is finalized.\n\n3\n\nSpecify the data set and save the null distribution in the object null_dist.\n\n4\n\nSpecify the response and predictor variable.\n\n5\n\nSpecify the null hypothesis of “independence”, corresponding to no linear relationship between the response and predictor variables.\n\n6\n\nSpecify the type of simulation (“permute”) and the number of iterations.\n\n\n\n\nWe can use ggplot to make a histogram of the null distribution.\n\nnull_dist |&gt;\n  filter(term == \"per_capita_expend\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nFinally, we compute the the p-value using the get_p_value().\n\n# get estimated slope \nestimated_slope &lt;- parks |&gt;\n  specify(playgrounds ~ per_capita_expend)  |&gt; \n  fit()\n\n# compute p-value \nget_p_value(null_dist, estimated_slope, direction = \"both\") |&gt;\n  filter(term == \"per_capita_expend\")\n\n# A tibble: 1 × 2\n  term              p_value\n  &lt;chr&gt;               &lt;dbl&gt;\n1 per_capita_expend   0.054\n\n\n\n\n\n5.9.3 Theory-based inference in R\nThe output from lm() contains the statistics discussed in this section to conduct theory-based inference. The p-value in the output corresponds to the two-sided alternative hypothesis \\(H_a: \\beta_1 \\neq 0\\).\nThe confidence interval does not display by default, but can be added using the conf.int argument in the tidy function. The default confidence level is 95%; it can be adjusted using the conf.level argument in tidy().\n\nparks_fit &lt;- lm(playgrounds ~ per_capita_expend, data = parks)\n\ntidy(parks_fit, conf.int = TRUE, conf.level= 0.95)\n\n# A tibble: 2 × 7\n  term              estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        2.42      0.214       11.3  3.11e-19 1.99       2.84   \n2 per_capita_expend  0.00330   0.00160      2.06 4.24e- 2 0.000115   0.00648\n\n\nIn practice, we use the output from tidy() to get the confidence interval. To compute the confidence interval directly from the formula in Equation 5.11, we get \\(\\hat{\\beta}_1\\) from the estimate column of the tidy() output and \\(SE_{\\hat{\\beta}_1}\\) from the std.error column. The critical value is computed using the qt function. the first argument is the cumulative probability (the percentile associated with the upper bound of the interval), and the degrees of freedom go in the second argument. For example, the critical value for the 95% confidence interval for the parks data used in Equation 5.12 is\n\n\n[1] 1.99",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#summary",
    "href": "05-slr-inference.html#summary",
    "title": "5  Inference for simple linear regression",
    "section": "5.10 Summary",
    "text": "5.10 Summary\nIn this chapter we introduced two approaches for conducting statistical inference to draw conclusions about a population slope, simulation-based methods and theory-based methods. The standard error, test statistic, p-value and confidence interval we calculated using the mathematical models from the Central Limit Theorem align with what it seen from the output produced by statistical software in Table 5.2. Modern statistical software will produce these values for you, so in practice you will not typically derive these values “manually” as we did in this chapter. As the data scientist your role will be to interpret the output and use it to draw conclusions. It’s still valuable, however, to have an understanding of where these values come from in order to interpret and apply them accurately. As more software has embedded artificial intelligence features, understanding how the values are computed also helps us check if the software’s output makes sense given the data, analysis objective, and methods.\nWhich of these two methods is preferred to use in practice? In the next chapter, we will discuss the model assumptions from Section 5.3 and the conditions we use to evaluate whether the assumptions hold for our data. We will use these conditions in conjunction with other statistical and practical considerations to determine when we might prefer simulation-based methods or theory-based methods for inference.\n\n\n\n\n\n\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “Infer: An r Package for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nThe Trust for Public Land. 2021. “Parks and an Equitable Recovery.” Online.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "05-slr-inference.html#footnotes",
    "href": "05-slr-inference.html#footnotes",
    "title": "5  Inference for simple linear regression",
    "section": "",
    "text": "Example: I think the relationship is positive. I predict that if the city spends more per resident, some of the funding is used for facilities like playgrounds.↩︎\nSlope: For each additional dollar a city spends per resident, is expected to be 0.003 more playgrounds per 10,000 residents, on average.\nIntercept: We would not expect a city to invest $0 on services and facilities for its residents, so the interpretation of the intercept is not meaningful in practice.↩︎\nWe sample with replacement so that we get a new sample each time we bootstrap. If we sampled without replacement, we would always end up with a bootstrap sample is exactly the same as the original sample.↩︎\nEach bootstrap sample is the same size as our current sample data. In this case, the sample data we’re analyzing has 97 observations.↩︎\nThere are 1000 values, the number of iterations, in the bootstrapped sampling distribution.↩︎\nThe points at the \\(5^{th}\\) and \\(95^{th}\\) percentiles make the bounds for the 95% confidence interval. The points at the \\(1^{st}\\) and \\(99^{th}\\) percentiles mark the lower and upper bounds for a 98% confidence interval.↩︎\nThe variability is approximately equal in both distributions. This is expected, because the distributions will have the same variability but different centers.↩︎\nGiven there is no linear relationship between spending per resident and playgrounds per 10,000 residents ( \\(H_0\\) is true), the probability of observing a slope of 0.003 or more extreme in a random sample of 97 cities is 0.046.↩︎\nIt is possible we have made a Type I error, because we concluded to reject the null hypothesis.↩︎\nStandard error is the term used for the standard deviation of a sampling distribution.↩︎\nNote its similarities to the general equation for sample standard deviation, \\(s = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2}{n-1}}\\)↩︎\nTest statistics with small magnitude provide evidence in support of the null hypothesis, as they are close to the hypothesized value. Conversely test statistics with large magnitude provide evidence against the null hypothesis, as they are very far away from the hypothesized value.↩︎",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for simple linear regression</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html",
    "href": "06-slr-conditions.html",
    "title": "6  Model conditions and diagnostics",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#learning-goals",
    "href": "06-slr-conditions.html#learning-goals",
    "title": "6  Model conditions and diagnostics",
    "section": "",
    "text": "Describe how model conditions are used to check the assumptions for linear regression\nUse the data to check model conditions and diagnostics\nIdentify strategies to handle violations in the conditions\nIdentifying influential observations using Cook’s distance\nUse leverage and standardized residuals to understand potential outliers\nIdentify strategies for dealing with outliers and influential points\nCheck model conditions and diagnostics using R",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-coffee-intro",
    "href": "06-slr-conditions.html#sec-coffee-intro",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.1 Introduction: Coffee grades",
    "text": "6.1 Introduction: Coffee grades\nWhat makes a delicious cup of coffee? Is there a relationship between a coffee’s aroma and its flavor? We will consider these questions by analyzing data from the Coffee Quality Database (https://database.coffeeinstitute.org/). The data set used in this analysis was curated by James LeDoux and was featured as part of the TidyTuesday weekly data visualization challenge in July 2020 (Community 2024). It contains a variety of features and quality ratings for over 1,000 coffees scraped from the Coffee Quality Database in 2018.The data are in coffee-grades.csv. We will use the following variables in this chapter:\n\naroma : Aroma grade (0: worst aroma - 10: best aroma)\nflavor: Flavor grade (0: worst flavor - 10: best flavor)\n\n\nThe goal of the analysis is to use the aroma grade to understand variability in the flavor grade, and assess model conditions and diagnostics to evaluate whether regression analysis is suitable for drawing reliable conclusions from the data.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of flavor\n\n\n\n\n\n\n\n\n\n(b) Distribution of aroma\n\n\n\n\n\n\n\n\n\n\n\n(c) flavor versus aroma\n\n\n\n\n\n\nFigure 6.1: Univariate and bivariate exploratory data analysis for coffee data\n\n\n\n\n\n\nTable 6.1: Summary statistics for aroma and flavor\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\naroma\n7.6\n0.3\n5.1\n7.4\n7.6\n7.8\n8.8\n0\n\n\nflavor\n7.5\n0.3\n6.1\n7.3\n7.6\n7.8\n8.8\n0\n\n\n\n\n\n\n\n\nFrom the exploratory data analysis in Figure 6.1, we see preliminary indication of a relationship between the aroma and flavor grades. There is an outlier in Figure 6.1 (c) that has a very low aroma grade but a flavor grade close to the average.\n\n\n\nTable 6.2: Linear regression model of the aroma versus flavor with 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n1.47\n0.151\n9.75\n0\n1.173\n1.764\n\n\naroma\n0.80\n0.020\n40.25\n0\n0.761\n0.839\n\n\n\n\n\n\n\n\nWe use the model in Table 6.2 to describe the relationship between the aroma and flavor of coffee. For each additional point in the aroma grade, the flavor grade is expected to increase by 0.8 points, on average.\n\n\nUse Table 6.2 to conduct a hypothesis test to determine if the data provide evidence of a linear relationship between aroma and flavor.\nIs the confidence interval consistent with the conclusion from the test?1\n\n\n\nFrom the interval in Table 6.2, we are 95% confident that for each additional point in the aroma grade, the flavor grade increases by 0.761 to 0.839 points, on average. In summary, coffees that smell better are expected to taste better!\nThe inferential methods introduced in Chapter 5 used to make conclusions about the relationship between the response and predictor variables rely on a set of underlying assumptions about the data. Though we are unable to check if these underlying conditions hold for the entire population, we will see in Section 6.2 how we are able to check a set of model conditions to evaluate whether these assumptions reasonably hold in the sample data.\nAs seen in Figure 6.1 (c), there is an outlying observation that is outside the general trend of the data, given its aroma grade is much lower than the others but its flavor grade is around the average. This is a valid data point and not the result of data entry error; therefore, we’d like to evaluate whether this outlier has a noticeable impact on the estimated regression model and inferential conclusions. In Section 6.4, we will introduce a set of model diagnostics to assess the potential impact of this observation on the results, and the impact of outliers more generally.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-model-conditions",
    "href": "06-slr-conditions.html#sec-model-conditions",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.2 Model conditions",
    "text": "6.2 Model conditions\n\n6.2.1 Assumptions and the LINE conditions\nSection 5.3 introduced the assumptions underlying the population when doing simple linear regression. These assumptions are important, as the reliability of the interpretations and inferential conclusions from the regression analysis rely on being consistent with the assumed framework. There are various analysis tasks we can do with a simple linear regression model: describe the relationship between a response and predictor variable, predict the response given values of the predictor, and draw conclusions about a population slope using inference. As we’ll see in this chapter, we need some or all of these assumptions to hold depending on the analysis task.\nRecall the assumptions from Section 5.3:\n\nThe distribution of the response \\(Y\\) is normal for a given value of the predictor \\(X\\).\nThe expected value (mean) of \\(Y|X\\) is \\(\\beta_0 + \\beta_1 X\\). There is a linear relationship between the response and predictor variable.\nThe variance \\(Y|X\\) is \\(\\sigma^2_{\\epsilon}\\). This variance is equal for all values of \\(X\\) and thus does not depend on \\(X\\).\nThe error terms for each observation, \\(\\epsilon\\) in Equation 5.3, are independent. This also means the values of the response variable, and observations more generally, are independent.\n\nThe assumptions are based on population-level data, yet we work with sample data in practice. Therefore, we have a set of model conditions we can check using the data at hand to assess these assumptions.\nAfter fitting a simple linear regression model, we check four conditions to evaluate whether the data are consistent with the underlying assumptions. They are commonly referred to using the mnemonic LINE. These four conditions are the following:\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\n\nIndependence: The residuals are independent of one another. \n\n\nNormality: The distribution of the residuals is approximately normal.\n\nEqual variance: The spread of the residuals is approximately equal for all predicted values.\n\nAfter fitting the regression model, we calculate the residuals for each observation, \\(e_i = y_i - \\hat{y}_i\\), and use the residuals to check the conditions. We introduce the four model conditions in order based on the LINE acronym; however, the conditions may be checked in any order.\n\n6.2.2 Linearity\nThe linearity condition states that there is a linear relationship between the response and predictor variable. Though we don’t expect the relationship between the two variables to be perfectly linear, it should be linear enough that it would be reasonable to summarize the relationship between the two variables using the linear regression model.\nRecall from Section 6.1 that we used a scatterplot and the correlation coefficient to describe the shape and other features of the relationship between the aroma (predictor) and flavor (response). This exploratory data analysis provides an initial assessment of whether the linear regression model seems like a reasonable fit for the data. For example, if we saw a clear curve in the scatterplot or calculated a correlation coefficient close to 0, we might conclude that the proposed linear regression model would not sufficiently capture the relationship between the two variables and thus rethink the modeling strategy. While the exploratory data analysis can help provide initial assessment and help us thoughtfully consider our modeling approach, it should not be solely relied upon as the final evaluation of whether the proposed model sufficiently captures the relationship between the two variables. We analyze the residuals for that assessment.\n\nTo check linearity, we make a scatterplot of the residuals versus the predicted values as shown in Figure 6.2. We have included a horizontal line at \\(\\text{residuals} = 0\\) to more easily see when the model over or under predicts.\n\n\n\n\n\n\n\nFigure 6.2: Residuals versus predicted values from the model of flavor and aroma\n\n\n\n\nThis scatterplot looks very different than any of the scatterplots we’ve seen thus far in the exploratory data analysis sections in the text. Let’s break down what this plot is showing, then use it to make an assessment about the linearity condition.\nWhen we fit a linear regression model, we are using the predictor to explain some of the variability in the response variable. Thus the residuals represent the remaining variability in the response variable that is not explained by the regression model (see Section 4.7.2 for more detail). If the relationship between the response and predictor is linear, then we would expect the linear regression model has sufficiently captured the systematic variability that can be explained by the predictor, leaving the residuals to capture any random variability we generally expect when analyzing sample data. Keeping this in mind, we conclude that the linearity condition is satisfied if the residuals are randomly scattered around 0. We can check this by asking the following:\n\nBased on the plot, can you generally guess with some certainty whether the residual would be positive or negative for a given predicted value or range of predicted values?\n\nIf the answer is no, then the linearity condition is satisfied.\nIf the answer is yes, then the current linear model does not adequately capture the relationship between the response and predictor variables. A model that incorporates information from more predictors or a different modeling approach is needed (we’ll discuss some of these in Chapter 7 and @@sec-ch-transformations).\n\n\nFigure 6.3 shows an example of a residual plot for simulated data in which the linearity condition is not met. There is a parabolic pattern in the plot, so the residuals are not randomly scattered around 0.\n\n\n\n\n\n\n\nFigure 6.3: Example of violation in linearity condition. The residuals are not randomly scattered around 0. Based on the plot, we can generally guess with some certainty whether the residual will be positive or negative for a given predicted value or range of predicted values.\n\n\n\n\n\nNow let’s use the plot of residuals versus predicted values in Figure 6.2 to assess the linearity condition for the model in Table 6.2. Based on this plot, we conclude the linearity condition is satisfied. We cannot determine with great certainty whether the residual will be positive or negative for any predicted value or range of predicted values, as the points are randomly scattered around the horizontal line at \\(y = 0\\). Thus the linear model adequately captures the relationship between aroma and flavor grades, and the residuals capture the random variability in flavor grade due to sources other than the aroma.\n\nWe made the assessment about linearity considering the range of fitted values that represents the bulk of the data. Because there is only one outlying observation with a fitted value less than 6, we are unable to determine what the random variability of the residuals would look like for fitted values in this range. Thus the presence of a few extreme outliers. Outliers are discussed in more detail in Section 6.4.\n\n\n6.2.3 Independence\nThe next condition in LINE is independence, that the residuals are independent of each other and that the sign of one residual does not inform the sign of other residuals. This condition is primarily checked based on the given information about the subject matter and the data collection process. This condition can be more challenging to evaluate, especially if we do not have much information about the data. If the condition is violated, there are two general scenarios in which the violation occurred. If neither of these apply to the data, then it is usually reasonable to conclude that the independence condition is satisfied.\nThe first scenario in which independence is often violated is due to a serial effect.  This occurs when data have a chronological order or are collected over time, and there is some pattern in the residuals when examining them in chronological order. If the data have a natural chronological order or were collected over time, make a scatterplot of residuals versus time order. Similar to checking the linearity condition, seeing no discernible pattern in the plot indicates the independence condition is satisfied. In this case, it means that knowing something about the order in which an observation was collected does not give information about whether the model tends to over or under predict. Otherwise, the independence condition is violated.\n\nThe second common violation of independence is due to a cluster effect.  This is when there are subgroups present in the data that are not accounted for by the model. To check this condition, modify the plot of the residuals versus predicted by using color, shape, faceting, or visual features to differentiate the observations by the subgroup. Similar to checking the linearity condition, we expect the residuals to be randomly scattered above and below \\(y = 0\\) for each subgroup. If this is not the case, there is indication that subgroup should be accounted for in the model. We will talk more about strategies for dealing with violations in the model conditions in Section 6.3 and how to include categorical predictors in the model to account for subgroups in Chapter 7.\nBased on the description of the coffee data, we conclude that the independence condition is satisfied. We do not have evidence of a serial or cluster effect in the data description.\n\n\n6.2.4 Normality\nThe next condition is normality, that the residuals are normally distributed. Though not explicitly part of the condition, we also know that the mean of the residuals, and thus the center of the distribution, is approximately 0. This condition arises from the assumption from Section 5.3 that the distribution of the response variable, and thus the residuals, at each value of the predictor variable is normal. In practice, it would be nearly impossible to check the distribution of the response or residuals for each possible value of the predictor, so we will look at the overall distribution of the residuals to check the normality condition.\n\n\n\n\n\n\n\nFigure 6.4: The distribution of the residuals for model of flavor and aroma with normal density curve\n\n\n\n\nFigure 6.4 shows the distribution of the residuals along with the theoretical curve for a normal distribution that is centered at 0 the mean of the residuals, and a standard deviation of 0.229, the standard deviation of the residuals. Similar to the other conditions, we are looking for obvious departures from normality when assessing this condition. This might include strong skewness, multiple modes, large outliers, or other major departures from normality.\nFrom Figure 6.4, we see the distribution of the residuals is approximately normal, as the shape of the histogram is unimodal and symmetric, the general shape of the normal curve. Therefore, we conclude that the normality condition is satisfied.\n\n\n\n6.2.5 Equal variance\nThe last condition is equal variance (also called constant variance), that the variability of the residuals is approximately equal for each predicted value. This condition stems from the assumption in Section 5.3 that \\(\\sigma_{\\epsilon}^2\\) is equal for all values of the predictor, and thus for all predicted values.\nTo check this condition, we will go back to the plot of the residuals versus predicted values from Figure 6.2. We look to see if the vertical spread of the residuals is approximately equal as we move along the \\(x\\)-axis. Since we are working with sample data, we don’t expect the vertical spread of the residuals to be exactly equal as we move along the \\(x\\)-axis (for example, there may be outliers). Therefore, similar to previous conditions, we are looking for obvious departures from equal variance to conclude that the condition is not met. Figure 6.5 shows an example using simulated data of plot of residuals versus fitted when the equal variance condition is not satisfied. There is a distinguishable “fan-shape” showing that the spread of the residuals increases as the predicted value increases. Additionally, keep in mind that we are examining the vertical spread for the majority of the data, not including outliers in the assessments as shown by the dotted lines in Figure 6.6 to serve as a guide as we assess equal variance.\n\n\n\n\n\n\n\nFigure 6.5: Example of residuals from simulated data that violate the equal variance condition. The vertical spread of the residuals increases as the predicted value increases.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: Plot of the residuals versus predicted values. The vertical spread is approximately for each predicted value, as shown by the residuals generally falling between the two dotted lines when moving along the \\(x\\)-axis.\n\n\n\n\nFrom Figure 6.6, the vertical spread of the residuals is approximately equal, as the vast majority of the residuals are between the two dotted lines as we move along the \\(x\\)-axis. Therefore, the equal variance condition is satisfied.\n\n\nChecking model conditions\n\nLinearity: Plot residuals versus predicted values. Look for points to be randomly scattered around 0 on the \\(y\\)-axis \\((\\text{residuals} = 0)\\) .\nIndependence: Use the description of the data and data collection process to assess if the observations can reasonably be treated as independent. If data are collected over time, examine plot of residuals versus time to assess potential serial effect. If unaccounted for subgroups are represented in the data, examine plot of residuals versus predicted for each subgroup.\nNormality: Plot the distribution of the residuals. Look for a distribution that is approximately unimodal and symmetric.\nEqual variance: Plot the residuals versus predicted values. Look for approximately equal vertical spread as we move along the \\(x\\)-axis.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-conditions-not-satisfied",
    "href": "06-slr-conditions.html#sec-conditions-not-satisfied",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.3 Strategies when conditions aren’t satisfied",
    "text": "6.3 Strategies when conditions aren’t satisfied\nIf all the model conditions are satisfied, we can move on to the next step in the analysis; however, that is sometimes not the case in practice. The good news is we there are a variety of methods to address violations in the model conditions. Additionally, many of the analysis tasks we do are robust to violations in some of the model conditions, meaning we can still obtain reliable analysis results, even if some conditions are not satisfied.\nFor each model condition we will discuss the analysis tasks for which satisfying the model condition is very important and some approaches we can take to address the violations in the condition.\n\n6.3.1 Linearity\nThe linearity condition is the most important condition, because the regression analysis relies on the assumption that the linear regression model adequately summarizes the relationship between the response and predictor variable (see Section 4.3.2). Therefore, the linearity condition must be satisfied in order for any interpretations and conclusions from the regression analysis to be useful and reliable. This means it is necessary for fitting the linear model, interpreting the model coefficients, drawing conclusions inference conducted using simulation-based or theory-based methods.\nIf the linearity condition is not satisfied, we can ask ourselves a few questions:\n\nIs the violation in the linearity condition due to the fact that there is, in fact, no evidence of a meaningful relationship between the response and predictor variable? In this case, reconsider if a linear regression model is the most useful way to understand these data.\nIs the violation in the linearity condition because there is evidence of a meaningful non-linear relationship between the response and predictor variable? If this is the case, there are a few options to work with this non-linear relationship.\n\nAdd a higher-order term, for example \\(x^2\\), in the model to capture the non-linearity of the relationship. We discuss higher-order model terms in more detail in Chapter 7. \n\nApply a transformation on the response and/or predictor variable, so there is a linear relationship between the transformed variables. Then proceed with simple linear regression using the transformed variable(s). We discuss variable transformations in Chapter 9.\n\n\n\n6.3.2 Independence\nThe independence condition states that the residuals are independent of one another. This assurance is most important when estimating variability about the line or variability in the estimated regression coefficients when doing statistical inference. If the independence condition is not satisfied and some residuals are correlated with one another, then our procedures may underestimate the true variability about the regression line and ultimately the variability in the estimated regression coefficients.  This is due to the fact that each observation is not fully contributing new information, because there is some correlation between two or more observations. Thus the effective sample size, or how many observations independently provide information, is smaller than the true sample size \\(n\\). This could result in misleading inferential conclusions, such as concluding there is a linear relationship between the response and predictor variable, when in fact there is no such a relationship.\nIf the independence condition is not met, more advanced statistical methods would be required to properly deal with the correlated residuals. These methods are beyond the scope of this book, but you can learn more about these methods called multilevel models or hierarchical models in books such as Roback and Legler (2021).\nIf we observe some pattern in the residuals based on subgroup, then there are two approaches to address it. The first is creating a separate model for each subgroup. The primary disadvantage to this approach is that we may have a different estimate for the slope and intercept for each subgroup, and thus it may be more difficult to draw overall conclusions about the relationship between the response and predictor variables. The second option is to fit a model that includes the subgroup as a predictor. This will allow us to account for differences by subgroup while maintaining a single model that makes it easier to draw overall conclusions. This type of model is called a multiple linear regression model, because it includes more than one predictor variable. We will introduce these models more in detail in Chapter 7.\nIf the violations in the independence condition are not addressed, we must use caution when drawing conclusions from the model or calculating predictions given the systematic under or over prediction by subgroup. In this case, we should consider if the model can be used to produce the type of valuable inferential insights we intend.\n\n\n\n6.3.3 Normality\nBy the Central Limit Theorem, we know the distribution of the estimate slope (the statistic of interest) is normal when the sample size is sufficiently large (see Section 5.8 for more details), regardless of the distribution of the original data. Therefore, when the sample size is “large enough”, we can be confident that the distribution of the estimated slope \\(\\hat{\\beta}_1\\) is normal, even if the residuals (and thus the response variable) do not follow a normal distribution.\nA common threshold for a “large enough” sample size is 30. This means if the sample has at least 30 observations (which is often the case for modern data sets!), we can use all the methods we’ve discussed to fit a simple linear regression model and draw conclusions from inference, even if the residuals are not normally distributed. Thus the inferential methods based on the Central Limit Theorem are considered robust to violations in the normality assumption, because we can feel confident about the conclusions from the inferential procedures even if the normality condition is not satisfied (given \\(n &gt; 30\\)). One note of caution is that if the sample size is very close to 30, then the inferential methods based on the Central Limit Theorem are not robust to major departures form normality such as extreme skewness or outliers.\nRecall from Chapter 5 the objective of simulation-based inference is to use the data to generate the sample distribution. Therefore, we do not make any assumptions about the sampling distribution of the estimated slopes. Because there are no assumptions about the sampling distribution, the normality condition is not required for simulation-based inferential procedures.\n\nThirty should not be treated as an absolute threshold. In other words, we should not treat data with a sample size of 29 materially different from data with a sample size of 31. If the sample size is small:\n\nThe distribution of the residuals should not have large departures from normality, in order to have reliable conclusions from the CLT-based inferential results.\nYou can conduct simulation-based inference, because these methods do not rely any assumptions about the sampling distribution of \\(\\hat{\\beta}_1\\), and thus do not rely on assumptions about the distribution of the residuals.\n\n\n\n6.3.4 Equal variance\nThe equal variance condition is as assessment of whether the distribution of the residuals is approximately equal for each value of the predictor variable. This is equivalent to the distribution of the response about the line for each predictor variable. The regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is the estimated value of this variability about the line, and it is the assumed to be the same for all values of the predictor (see Section 5.3 for more detail). When conducting inference based on the Central Limit Theorem, this regression standard error is used to calculate \\(SE_{\\hat{\\beta}_1}\\), the estimated standard error for the estimated slopes. Thus similar to violations in the independence condition, if the variability about the regression line is not equal for all values of the predictor, the regression standard error will not be accurate and thus the standard error of the slope, our estimate of the sampling variability in this statistic will be inaccurate as well. The calculation of the confidence interval and test statistic for hypothesis testing both use \\(SE_{\\hat{\\beta}_1}\\), so these values will likely be inaccurate as well. Therefore, we should approach conclusions from the theory-based inferential procedures with caution if the equal variance condition is not met.\nSimilar to the normality condition, the equal variance condition is not required for simulation-based inference, because \\(SE_{\\hat{\\beta}_1}\\) is estimated from the sampling distribution simulated from the data.\nThere are approaches for handling violations in the equal variance condition. These approaches typically involve performing a transformation on the response variable. We will discuss these further in @@sec-ch-transformations.\n\n6.3.5 Recap\nTable 6.3 provides a summary of the model conditions and whether they must be satisfied to reliably conduct each modeling task.\n\n\nTable 6.3: Summary of necessary conditions for various analysis tasks on the slope\n\n\n\n\n\n\n\n\n\nModel condition\nInterpret and predict\nSimulation-based inference\nTheory-based inference\n\n\n\nLinearity\nImportant\nImportant\nImportant\n\n\nIndependence\nImportant\nImportant\nImportant\n\n\nNormality\nNot needed\nNot needed\n\nNot needed if \\(n &gt; 30\\)\nImportant if \\(n &lt; 30\\)\n\n\n\nEqual variance\nNot needed\nNot needed\nImportant\n\n\n\n\n\n\nSo far we have discussed the model conditions to assess whether the assumptions for simple linear regression reasonably hold true for our data. We have not, however, dealt with the presence of outliers in the data and the potential impact (or “influence”) they may have on the regression model. In the next section, we discuss measures we can use to identify and assess the potential impact of such outlying observations.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-model-diagnostics",
    "href": "06-slr-conditions.html#sec-model-diagnostics",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.4 Model diagnostics",
    "text": "6.4 Model diagnostics\nOne challenge that may arise when doing regression analysis (or really any type of data analysis) is how to handle outliers in the data. Because we are focused on the linear relationship between two (and eventually three or more) variables, there are multiple ways in which the outliers may show up in our analysis:\n\nOutlier in the value of the predictor variable\nOutlier in the value of the response variable\nOutlier outside the general pattern of the relationship between the response and predictor variables\n\nA single observation could fall into multiple categories. For example look at the outlier identified in Figure 6.1. This observation is around the center of the distribution for the response variable flavor but has a low outlying value in terms of the predictor aroma. Therefore, it is an outlier as described by categories (1) and (3) above. There is a set of model diagnostics to assess if there are observations in the data that fall into one or more of these outlier scenarios, and more importantly, if any such outlying observations have an out-sized impact on the regression model.\n\n6.4.1 Influential points\nAn observation is considered an influential point if the estimated regression coefficients noticeably differ when the point is included in the data used to fit the model versus when it is not. Figure 6.7 illustrates the potential impact an influential point can have on the estimated regression line. Additionally, influential points can affect \\(SE_{\\hat{\\beta}_1}\\), estimated standard error of the slope, which can result in unreliable inferential conclusions as described in the previous section.\n\n\n\n\n\n\n\n\n\n(a) With influential point\n\n\n\n\n\n\n\n\n\n(b) Without influential point\n\n\n\n\n\n\nFigure 6.7: Simulated data showing the effect of an infulential point representd by the red triangle. The slope of the line changes when the influential point is removed from the data set.\n\n\nSometimes potential influential points can be identified in a scatterplot of the response versus predictor variable in the exploratory data analysis. In the EDA, we have called out these points as being outliers, as they are typically outside of the general trend of the relationship between the two variables. Identifying these points from the EDA can become more difficult, however, when there are multiple predictor variables (see Chapter 7). Therefore, we will use a set of model diagnostics to identify observations that are outliers and perhaps more importantly, those that are influential.\nThere are three diagnostic measures to identify outliers and influential points:\n\n\nLeverage: Identify observations that are outliers in the values of the predictor\n\nStandardized residuals: Identify observations that are outliers in the value of the response\n\nCook’s distance: Identify observations that are influential (a combination of leverage and standardized residuals)\n\nUltimately, we are most interested in examining Cook’s distance, because it provides the indication about each observation’s impact on the regression analysis. Cook’s distance takes into account information gleaned from leverage and standardized residuals, so we will introduce these diagnostics first.\n\n6.4.2 Leverage\nThe leverage of the \\(i^{th}\\) observation is a measure of the distance between its value of the predictor, \\(x_i\\), and the average value of the predictor across all \\(n\\) observations, \\(\\bar{x} = \\frac{1}{n}\\sum_{j=1}^n x_j\\).\nWhen doing simple linear regression, an observation’s leverage, denoted \\(h_i\\), is calculated using Equation 6.1.\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n(x_j - \\bar{x})^2}\n\\tag{6.1}\\]\n\nThe values of the leverage are between \\(\\frac{1}{n}\\) and 1 for each observation in the data set, and the average value of leverage across all \\(n\\) observations is \\(\\frac{(p+1)}{n}\\), where \\(p\\) is the number of predictors in the model. In simple linear regression \\(p = 1\\), thus the average leverage is \\(\\frac{2}{n}\\).\nAn observation is considered to have large leverage if its leverage is greater than \\(\\frac{2(p+1)}{n}\\) , i.e., greater than \\(\\frac{2 \\times 2}{n}\\) for simple linear regression. This means that an observation is considered to have large leverage if its value of leverage is greater than twice the average value of leverage for the observations in the data set.\n\n\nLeverage only depends on the values of the predictor variable(s). It does not depend on values of the response.\n\nLet’s take a look at the leverage for the observations in the coffee ratings data set. There are 1338 observations in the data and one predictor variable, so the threshold for identifying observations with large leverage is \\(\\frac{2 \\times 2}{1338} =\\) 0.003.\nFigure 6.8 is a visualization of the leverage for each observation in the data set. The red, horizontal dotted line marks the threshold for identifying points as having large leverage. There are 114 observations that have large leverage.\n\n\n\n\n\n\n\nFigure 6.8: Leverage for observations in the coffee ratings data\n\n\n\n\n\nThere are 114 observations that have large values for the leverage. What does it mean for an observation to have “large leverage” in the context of the coffee data?\n\nLet’s take a look at the observations with the largest values of leverage to get a better understanding about which points may be large leverage.\n\n\n\nTable 6.4: Observations with the top five highest values for leverage\n\n\n\n\nLeverage\naroma\nflavor\n\n\n\n0.047\n5.08\n7.75\n\n\n0.015\n6.17\n6.50\n\n\n0.012\n6.33\n6.50\n\n\n0.011\n8.75\n8.67\n\n\n0.011\n6.42\n6.50\n\n\n\n\n\n\n\n\nRecall that the leverage only depends on the values of the predictor variable, so we only need to consider aroma to better understand why these points are large leverage. From the EDA in Section 6.1, the average aroma grade in the data set is 7.572 points and the standard deviation is 0.316 . The observations with the largest values of leverage have aroma grades that are either much higher (e.g., 8.75) or much lower (e.g., 5.08) than the average. Thus the large leverage points includes coffees that both smell much better and perhaps much worse than the majority of the coffees in the data.\nEven when we identify points with large leverage, we still want to understand how these points are (or are not) influencing the model results. Thus, knowing an observation has large leverage is not enough to warrant any specific action. We’ll examine Cook’s distance to determine if the observation is influential in the model in Section 6.4.4 and discuss approaches to deal with such influential points in Section 6.4.5 .\n\n6.4.3 Standardized residuals\nThe residual is an indication of how well the regression model predicts the value of the response for a given observation. One reason the model may predict poorly for an observation is because it has an observed value of the response that is much different than the rest of the data. Therefore residuals can be used to identify observations that are outliers based on Scenario 2 in Section 6.4. We will examine the magnitude of the residuals \\(|e_i|\\) to identify such points, because residuals can be positive (model underpredicted) or negative (model overpredicted).\nThe residuals have the same units as the response variable, so what is considered a residual with large magnitude depends on the scale and range of the response variable. This means what is considered a “large” residuals can be different for every analysis. We can address this by instead examining standardized residuals, defined as the residual divided by its standard error. Equation 6.2 shows the formula to calculate the standardized residual for the \\(i^{th}\\) observation.\n\\[\nstd.resid_i = \\frac{e_i}{SE_{e_i}} = \\frac{e_i}{\\hat{\\sigma}_{\\epsilon} \\sqrt{1 - h_i}}\n\\tag{6.2}\\]\nThe residuals for every analysis are now on the same scale. Thus, we can use a common threshold to determine residuals that are considered to have large magnitude. An observation is an outlier in the response if it its standardized residual has a magnitude greater than two, \\(|std.resid_i| &gt; 2\\) (\\(std.resid_i &lt; - 2 \\hspace{1mm} \\text{ or }\\hspace{1mm} std.resid_i &gt; 2\\)). For large data sets, we recommend a more restrictive threshold of 3, so there is not an overwhelming number of observations considered outliers that may require further evaluation.\n\nOne assumption for linear regression is that the residuals are normally distributed and centered at 0 (see Section 5.3). This means the standardized residuals are normally distributed with mean of 0 and standard deviation of 1, \\(N(0,1)\\). In a standard normal distribution, we expect about 95% of the observations to have values between -2 and 2. By setting 2 as the threshold, we are essentially identifying the 5% of observations with the most extreme (high and low) residuals.\nIf there are a lot of observations, it may not be practical to closely examine the approximately 5% of observations that were identified as potential outliers. Thus we may use a more restrictive threshold, e.g., \\(|std.resid_i| &gt; 3\\) (or higher) to identify points worth more close examination or only focus on those that are identified as influential points by Cook’s distance.\n\n\n\n\n\n\n\n\n\nFigure 6.9: Standardized residuals versus observation number for model of flavor and aroma\n\n\n\n\n\nFigure 6.9 shows 12 observations with large magnitude residuals based on the threshold \\(\\pm 3\\) . This includes observations that have positive residuals with large magnitude (indicating the model greatly underpredicted) and observations with negative residuals with large magnitude (indicating the model greatly overpredicted).\nThese five observations with the largest magnitude residuals are shown in Table 6.5.\n\n\n\n\nTable 6.5: Observations with large magnitude residuals\n\n\n\n\naroma\nflavor\n.fitted\n.std.resid\n\n\n\n5.08\n7.75\n5.53\n9.90\n\n\n7.17\n6.08\n7.20\n-4.90\n\n\n7.17\n6.17\n7.20\n-4.51\n\n\n7.92\n6.83\n7.80\n-4.25\n\n\n7.00\n6.17\n7.07\n-3.92\n\n\n\n\n\n\n\n\nWe see from Table 6.5 and Figure 6.10 that these observations have flavor ratings that are lower than would be expected from the model based on their aroma ratings. The exception is the observation with the largest magnitude standardized residual that has a higher actual flavor rating than the model predicts based on its very low aroma rating.\n\n\n\n\n\n\n\nFigure 6.10: Flavor grade versus aroma grade. Points marked with a red triangle are the observations with large standardized residuals with magnitude &gt; 3.\n\n\n\n\n\nWe can use a plot of standardized residuals versus predicted values such as the one in Figure 6.9 to check the linearity and equal variance conditions from Section 6.2.\n\nAs with observations that have large leverage, we want to assess whether these points have out-sized influence on the model coefficients to help determine how to deal with these outliers (or if we need to do anything at all). To do so, we will look at the last diagnostic, Cook’s distance.\n\n6.4.4 Cook’s distance\nAt this point we have introduced a diagnostic to identify outliers in the predictor variable (leverage) and one to identify outliers in the response variable (standardized residuals). Now we will use a single measure that combines information from the leverage and standardized residuals to identify potential influential points. This measure is called Cook’s distance.\nCook’s distance is a measure of an observation’s overall impact on the estimated model coefficients. In Equation 6.3, we see that Cook’s distance takes into account an observation’s leverage, \\(h_i\\), the standardized residuals, \\(std.resid_i\\), and the number of predictors in the model, \\(p\\) ( \\(p=1\\) for simple linear regression).\n\\[\nD_i = \\frac{1}{p}(std.resid_i)^2\\Big(\\frac{h_i}{1-h_i}\\Big)\n\\tag{6.3}\\]\n\n\nAs with leverage and standardized residuals, there are thresholds we use to determine if a point is potentially influential. A commonly used threshold is 1. If an observation has a Cook’s distance greater than 1, \\(D_i &gt; 1\\), it is considered an influential point and is worth closer examination given its potential influence on the model.\n\n\n\n\n\n\n\nFigure 6.11: Cook’s distance versus observation number for coffee ratings data\n\n\n\n\nFigure 6.11 shows the values of Cook’s distance for the observations in the data set. It includes a horizontal line at 1 marking the threshold for determining if an observation is influential. There is one observation in the data that has a value of Cook’s distance greater than 1; its value is 2.432. This is the observation with an aroma grade of large_cooks$aroma and flavor grade of large_cooks$flavor.\n\n6.4.5 Handling influential points\nTo better understand the impact of influential points, let’s take a look at our model fit with and without the influential observation.\n\n\n\nTable 6.6: Estimated model coefficients for model with and without influential observation\n\n\n\n\nTerm\nWith Influential Point\nWithout Influential Point\n\n\n\n(Intercept)\n1.47\n1.137\n\n\naroma\n0.80\n0.843\n\n\n\n\n\n\n\n\nIn Table 6.6, the estimates for the intercept and coefficient of aroma change based on whether the influential point is included or not included in the data. Therefore, it is now our task as the data scientist to determine how to best proceed given the influential point.\nOutlier based on predictor\nIf an observation is an outlier based on the predictor variable, first consider if it is the result of a data entry error that can be fixed. If not, one option is to remove this observation from the analysis, particularly if the outlier is an influential point or a result of a data entry error. Doing so limits the scope of the conclusions that can be drawn from the analysis and the range of values for which the model can be used for prediction, because removing the outlying observation narrows the range of possible values for the predictor. When taking this approach, it is important to mention it when communicating results. Additionally, we need to be careful not to extrapolate if using the model for prediction. \nThe influential point identified in Section 6.4.4 is an outlier because it has a value of the predictor much lower than the rest of the data. Therefore, we could remove this observation and thus limit the scope of the analysis to coffee that has an aroma rating of 6 or higher. We can also keep this observation in the analysis and thus have some information about coffees with aroma ratings less than 6. Given there is a single observation with a rating less than 6, however, conclusions about coffees with low coffee ratings should be made with caution.\nOutlier based on response\nIf the observation is an outlier based on the value of the response variable, first consider if it is the result of a data entry error. If it is not and the value of the predictor is within the range of the other observations in the data, the observation should not be removed from the analysis. Removing the observation could result in taking out useful information for understanding the relationship between the response and predictor variable.\nIf the outlier is influential, one approach is to build the model with and without the influential point, then compare the estimated values and inference for the model coefficients. Another approach is to transform the response variable to reduce the effect of outliers (more on transformations in @@sec-ch-transformations). A last approach is to collect more data, so the model information can be informed by more observations; however, collecting more data is often not feasible in practice.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-model-in-practice",
    "href": "06-slr-conditions.html#sec-model-in-practice",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.5 Modeling in practice",
    "text": "6.5 Modeling in practice\nIn this chapter, we have introduced a set of model conditions and diagnostics to evaluate how well the model fits the data and satisfies the assumptions for linear regression. Now we’ll conclude the chapter by discussing how we might incorporate these tools in practice.\n\nExploratory data analysis (EDA): Start every regression analysis with some exploratory data analysis, as in Section 6.1 . The EDA will help you better understand the data, particularly the variables that will be used in the regression model. Through the EDA we may also notice points that stand out as potential outliers and potential influential points that may be of interest for further exploration. The EDA can provide valuable insights about the data; however, it cannot be used alone to confirm whether the model is a good fit or confirm if a point is influential. We need the model conditions and diagnostics to make such a robust assessment. The EDA is important, however, to help determine the best analysis approach and to understand the data enough to more fully understand the model interpretations and influential conclusions.\nModel fitting: Fit the regression model.\n\nModel conditions and diagnostics: Check the model conditions (LINE) to assess whether the model is a good fit for the data.\nThen, check Cook’s distance to determine if there are potential influential points in the data.\n\nIf there are influential points, check the leverage and standardized residuals to try to understand why these points are influential. Use this information, along with the analysis objective and subject matter expertise, to determine how to best proceed with the dealing with these points.\nIf there are no influential points, we can examine the leverage and standardized residuals for a deeper understanding of the data; however, no further action is needed in terms of refitting the model.\n\nMake any adjustments as needed based on the model conditions and diagnostics.\n\nModeling in practice is an iterative process. We repeat Steps 2 and 3 until we find a model that is the best fit for the data.\n\n\nPrediction and inference: Once the model is finalized, and we are satisfied with the results of the model conditions and diagnostics, then we can use the model for prediction and inference to answer the analysis questions.\nCommunication: Once we have the final inferential results and predictions, communicate the overall conclusions in a way that can be easily understood by the intended audience.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#sec-conditions-in-r",
    "href": "06-slr-conditions.html#sec-conditions-in-r",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.6 Model conditions and diagnostics in R",
    "text": "6.6 Model conditions and diagnostics in R\nThe residuals, along with other observation-level statistics, can be obtained using augment() in the broom R package (Robinson, Hayes, and Couch 2023). This package is in the suite of packages that are included as part of tidymodels(Kuhn and Wickham 2020). The output produced by this function is shown in Table 6.7.\n\ncoffee_model &lt;- lm(flavor ~ aroma, data = coffee_ratings)\n\ncoffee_model_aug &lt;- augment(coffee_model)\n\n\n\n\nTable 6.7: First ten rows of output produced by the augment function\n\n\n\n\nflavor\naroma\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n8.83\n8.67\n8.40\n0.4259\n0.00978\n0.229\n0.01715\n1.864\n\n\n8.67\n8.75\n8.47\n0.2019\n0.01114\n0.230\n0.00440\n0.884\n\n\n8.50\n8.42\n8.20\n0.2959\n0.00613\n0.230\n0.00515\n1.293\n\n\n8.58\n8.17\n8.00\n0.5758\n0.00342\n0.229\n0.01084\n2.513\n\n\n8.50\n8.25\n8.07\n0.4319\n0.00419\n0.229\n0.00747\n1.885\n\n\n8.42\n8.58\n8.33\n0.0878\n0.00836\n0.230\n0.00062\n0.384\n\n\n8.50\n8.42\n8.20\n0.2959\n0.00613\n0.230\n0.00515\n1.293\n\n\n8.33\n8.25\n8.07\n0.2619\n0.00419\n0.230\n0.00275\n1.143\n\n\n8.67\n8.67\n8.40\n0.2658\n0.00978\n0.230\n0.00668\n1.164\n\n\n8.58\n8.08\n7.93\n0.6479\n0.00268\n0.229\n0.01072\n2.826\n\n\n\n\n\n\n\n\nWe use the following columns produced by augment() to check model conditions and diagnostics:\n\n\n.fitted: Predicted values\n\n.resid: Residuals\n\n.hat: Leverage\n\n.cooksd: Cook’s distance\n\n.std.resid: Standardized residuals\n\nThe plots used to check the model conditions and diagnostics are produced using ggplot() functions. We can also produce similar plots in base R format using the plot function. The QQ-plot produced from plot() is used to check the normality condition. The distribution of the residuals is approximately normal if the points lie along the diagonal line.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#summary",
    "href": "06-slr-conditions.html#summary",
    "title": "6  Model conditions and diagnostics",
    "section": "\n6.7 Summary",
    "text": "6.7 Summary\nIn this chapter, we introduced model conditions and diagnostics to evaluate the fit of a simple linear regression model. We discussed the LINE model conditions: linearity, independence, normality, and equal variance, and how they are used to check the assumptions for linear regression. We also discussed scenarios in which assumptions are important or may be relaxed. We used model diagnostics, specifically Cook’s distance, leverage, and standardized residuals, to identify outliers. We discussed strategies for handling influential points that may impact model results. We concluded by describing the full process of modeling in practice and how to implement the concepts from this chapter in R.\nThus far, we have discussed simple linear regression, using one predictor variable to understand variability in a response variable. In the next chapter, we will extend what we know about simple linear regression to models with two or more predictor variables, called multiple linear regression.\n\n\n\n\n\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.” https://www.tidymodels.org.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in r. Chapman; Hall/CRC.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. “Broom: Convert Statistical Objects into Tidy Tibbles.” https://CRAN.R-project.org/package=broom.",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "06-slr-conditions.html#footnotes",
    "href": "06-slr-conditions.html#footnotes",
    "title": "6  Model conditions and diagnostics",
    "section": "",
    "text": "The null hypothesis is there is no linear relationship between aroma and flavor \\((H_0: \\beta_1 = 0)\\) and the alternative is that there is a linear relationship \\((H_a:\\beta_1 \\neq 0)\\). The p-value is \\(\\approx 0\\), so we reject the null hypothesis. The data provide evidence of a linear relationship between the two variables. The confidence interval is consistent, because 0 is not in the interval.↩︎",
    "crumbs": [
      "Part 2: Simple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model conditions and diagnostics</span>"
    ]
  },
  {
    "objectID": "07-mlr.html",
    "href": "07-mlr.html",
    "title": "7  Multiple linear regression",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#learning-goals",
    "href": "07-mlr.html#learning-goals",
    "title": "7  Multiple linear regression",
    "section": "",
    "text": "Explain how multiple predictor variables can be used to explain variability and predict a quantitative response variable\nInterpret model coefficients in the context of the data\nCompute predicted values from the multiple linear regression model\nUse different types of predictors in the regression model \n\nConduct multiple linear regression analysis using R",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-lemurs-intro",
    "href": "07-mlr.html#sec-lemurs-intro",
    "title": "7  Multiple linear regression",
    "section": "\n7.1 Introduction: Lemurs",
    "text": "7.1 Introduction: Lemurs\n\nAbout 95% of lemur species are at risk of extinction, so it is important for researchers to study and better understand lemurs to more effectively save these species (Community 2024). The Duke Lemur Center currently has over 250 lemurs from 12 species. It has the most diverse population of lemurs outside of Madagascar, the native home for lemurs (Duke Lemur Center n.d.).  In this chapter, we will use various characteristics to understand the variability in the weight of lemurs who are 24 months old or younger. We refer to this group of lemurs as “young lemurs” throughout the analysis.  The data contains the weight and other features of young lemurs from three taxon (ERUF, PCOQ, and VRUB)  who lived at the Duke Lemur Center at the time the data were collected. The data were originally from Zehr et al. (n.d.) and were featured as part of the TidyTuesday weekly data visualization challenge in August 2021 (Community 2024).\n\n\n\n\n\n\n\n\n\nEulemur rufus (ERUF)\n\n\n\n\n\n\n\n\nPropithecus coquereli (PCOQ)\n\n\n\n\n\n\n\n\nVarecia rubra (VRUB)\n\n\n\n\n\nFigure 7.1: Types of lemurs represented in the data set. Images from Duke Lemur Center (n.d.).\n\n\n\nThe data are in lemurs-sample-young.csv. This analysis in this chapter focuses on the following variables:\n\n\nweight: Weight of the lemur (in grams)\n\ntaxon : Code made as a combination of the lemur’s genus and species. Note that the genus is a broader categorization that includes lemurs from multiple species.  This analysis focuses on the following taxon:\n\nERUF: Eulemur rufus, commonly known as Red-fronted brown lemur\nPCOQ: Propithecus coquereli, commonly known as Coquerel’s sifaka\nVRUB: Varecia rubra, commonly known as Red ruffed lemur\n\n\n\nsex : Sex of lemur (M: Male, F: Female)\n\nage: Age of lemur when weight was recorded (in months)\n\nlitter_size: Total number of infants in the litter the lemur was born into (this includes the observed lemur)\n\n\nThe goal of the analysis is to use age, taxon, sex, and litter size to predict and explain variability in the weights of young lemurs.\n\n\n\n7.1.1 Exploratory data analysis\nThe distribution and summary statistics for the response variable weight are shown in Figure 7.2 and Table 7.1, respectively. The center of the distribution is around the median of 1779.5 grams and the interquartile range (IQR) is 2087 grams. The distribution is bimodal, which may be due to the multiple taxa represented in the data. There is one lemur with a high outlying weight of 6969 grams. We will keep the bimodality and outlier in mind as we proceed with the analysis.\n\n\n\n\n\n\n\nFigure 7.2: Univariate distribution of weight of young lemurs\n\n\n\n\n\n\n\nTable 7.1: Summary statistics for weight\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\nweight\n1839\n1190\n131\n677\n1780\n2764\n6969\n0\n\n\n\n\n\n\n\nNow let’s look at the distributions of the predictors in this analysis. The visualizations of each variable’s distribution and summary statistics for the quantitative predictors are in Figure 7.3 and Table 7.2, respectively.\n\n\n\n\n\n\n\n\n\n(a) age\n\n\n\n\n\n\n\n\n\n(b) taxon\n\n\n\n\n\n\n\n\n\n\n\n(c) sex\n\n\n\n\n\n\n\n\n\n(d) litter_size\n\n\n\n\n\n\nFigure 7.3: Univariate distributions of predictor variables\n\n\n\n\n\nTable 7.2: Summary statistics of quantitative predictors\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\nlitter_size\n1.7\n0.9\n1.0\n1.0\n1.0\n3.0\n4.0\n0\n\n\nage\n9.0\n7.0\n0.6\n2.8\n7.1\n15.6\n23.9\n0\n\n\n\n\n\n\n\n\n\nThe distribution of taxon in Figure 7.3 (b) shows that most lemurs are from the VRUB taxon and the fewest are from the ERUF taxon. The distribution of sex in Figure 7.3 (c) shows a relatively equal distribution, with a small majority of the lemurs being female. Finally, a vast majority of the lemurs were born as the only lemur in their litter as seen in Figure 7.3 (d).\n\nUse the visualizations in Figure 7.3 and summary statistics in Table 7.2 to describe the distribution of age.1\n\nNow, we conduct bivariate exploratory data analysis, shown in Figure 7.4, to explore the relationship between the response and each of the predictor variables.\n\n\n\n\n\n\n\n\n\n(a) weight vs. age\n\n\n\n\n\n\n\n\n\n(b) weight vs. taxon\n\n\n\n\n\n\n\n\n\n\n\n(c) weight vs. sex\n\n\n\n\n\n\n\n\n\n(d) weight vs. litter_size\n\n\n\n\n\n\nFigure 7.4: Bivariate exploratory data analysis\n\n\nFrom Figure 7.4 (a), we see a strong, positive relationship between age and weight. The relationship looks linear; however, the relationship flattens as age increases. This aligns with what we might expect; lemurs experience fairly rapid growth early on, and then their growth rate slows over time. In Chapter 9, we will introduce transformations that can be applied to the predictor and response variables to account for such trends in the model. For now, we will model the relationship as a constant rate of change between age and weight.\nIn Figure 7.4 (b), lemurs from the PCOQ taxon have a lower median weight than lemurs from ERUF or VRUB: however, there is a lot of overlap in the distribution of weights of ERUF and PCOQ lemurs. Lemurs in the VRUB taxon have the highest median weight; however, the middle 50% of the distribution overlaps greatly with the distribution for PCOQ lemurs. There is more variability in the weights of PCOQ lemurs compared to ERUF and VRUB, and we see the lemur with the outlier weight is in the PCOQ taxon.\nBased on the medians and large overlap in the boxplots in Figure 7.4 (c), there does not appear to be much difference in the weights of male and female lemurs. From this plot, we see the lemur with the high outlying weight is a female.\nFigure 7.4 (d) shows the relationship between litter_size and weight. Random noise was added to reduce overlap in the points on the graph. This makes it easier to see the relationship between the two variables. Based on this scatterplot, the relationship between litter_size and weight appears to be positive and relatively weak.\nNow that we have a better understanding of the variables in this analysis, we will use a multiple linear regression model to understand how taxon, litter size, age, and sex explains variability in the weight of lemurs. Before looking at the model specific to this data, we will discuss the details of the multiple linear regression model, in general, and to determine if a multiple linear regression model is appropriate for the data.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-mlr-fit-line",
    "href": "07-mlr.html#sec-mlr-fit-line",
    "title": "7  Multiple linear regression",
    "section": "\n7.2 Fitting the regression line",
    "text": "7.2 Fitting the regression line\nIn Chapter 4, we introduced simple linear regression to quantify the relationship between a response and a single predictor variable. Using multiple linear regression, we can expand on the regression model to quantify the relationship between a response variable and multiple predictors.\n\n7.2.1 Statistical model for multiple linear regression\nRecall from Section 4.3 that the general form of a model for a response variable \\(Y\\) is\n\\[\nY = \\text{Model} + \\text{Error}\n\\tag{7.1}\\]\nIn the case of simple linear regression, \\(\\text{Model} = f(X)\\) where \\(X\\) is the predictor variable used to explain the variability in \\(Y\\). Now we will expand Equation 7.1 to the case in which multiple predictor variables, denoted \\(\\mathbf{X} = X_1, X_2, \\ldots, X_p\\) are used in combination to explain variability in the response variable \\(Y\\). The values of the response variable \\(Y\\) are generated based on the following equation\n\\[\n\\begin{aligned}\nY &= \\text{Model} + \\text{Error} \\\\[5pt]\n& = f(\\mathbf{X}) + \\epsilon \\\\[5pt]\n& = f(X_1, X_2, \\ldots, X_p) + \\epsilon\n\\end{aligned}\n\\tag{7.2}\\]\nwhere \\(p\\) represents the number of predictor variables and \\(\\epsilon\\) is the error term quantifying the amount in which the actual value of \\(Y\\) differs from what is expected from the model \\(f(X_1, X_2, \\ldots, X_p)\\).\nEquation 7.2 is the general form of the model. As with simple linear regression, we know the form of \\(f(X_1,  X_2, \\ldots, X_p)\\) when we assume there is a linear relationship between the response and predictor variables. Equation 7.2 can be more specifically written as a multiple linear regression model (MLR) of the form\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{7.3}\\]\nwhere \\(\\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\\) indicates the error terms are normally distributed with a mean of 0 and variance \\(\\sigma^2_{\\epsilon}\\). Equation 7.3 is the population-level statistical model for multiple linear regression.\nRecall from Section 4.3.1, the function \\(f(X) = \\mu_{Y|X}\\) is the expected value (mean) of \\(Y\\) given a value of the predictor \\(X\\). Extending this to multiple linear regression, \\(f(\\mathbf{X}) = f(X_1, X_2, \\ldots, X_p)\\) outputs the expected value of \\(Y\\) for a combination of the predictors. Therefore,\n\\[\n\\mu_{Y|\\mathbf{X}} = \\mu_{Y|X_1, X_2, \\ldots, X_p} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\tag{7.4}\\]\nThus, when we fit a multiple linear regression model, the output from the fitted model will be the predicted expected value of the response variable for a given combination of values of the predictor variables.\n\n7.2.2 Evaluating whether MLR is appropriate\n\nBefore we move forward with fitting the regression model, we should first ask whether the multiple linear regression model is appropriate or useful for the data. We can ask the same questions we did for simple linear regression; however, now we are considering the relationship between \\(Y\\) and multiple predictor variables rather than a single predictor.\n\nWill a linear regression model be practically useful? Does quantifying and interpreting the relationship between the variables make sense in this scenario?\nDo the observations in the data represent the population of interest, or are there biases that should be addressed when drawing conclusions from the analysis? \n\nWould a linear regression model reasonably summarize the relationship between the response and predictor variables?\n\nThe last question can be more challenging to answer for multiple linear regression than it was in simple linear regression, because it is difficult to visualize the relationship between three or more variables in the exploratory data analysis. As we saw in Section 7.1.1, we can visualize the relationship between the response and each individual predictor. This can give us some indication of linearity and if there are non-linear trends that need to be taken into account (more on this in Chapter 9). We will primarily rely on the assessment of the model conditions (Section 8.5) to ultimately determine whether a linear model adequately captures the relationship between the response and predictor variables.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-mlr-estimate-coef",
    "href": "07-mlr.html#sec-mlr-estimate-coef",
    "title": "7  Multiple linear regression",
    "section": "\n7.3 Estimating the coefficients",
    "text": "7.3 Estimating the coefficients\nThe predicted value \\(Y\\) for the \\(i^{th}\\) observation output from the estimated multiple linear regression is shown in Equation 7.5 is\n\\[\n\\hat{y}_i = \\hat{\\mu}_{y_i|x_{i1},x_{i2}, \\ldots, x_{ip}} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + \\hat{\\beta}_2x_{i2} + \\dots + \\hat{\\beta}_px_{ip}\n\\tag{7.5}\\]\nThe error term for the \\(i^{th}\\) observation, \\(\\epsilon_i\\), is estimated by calculating the residuals as before, \\(e_i = y_i - \\hat{y}_i\\), the difference between what is observed in the data and what is expected from Equation 7.5. As with simple linear regression in Section 4.4, the coefficients are estimated using least-squares, finding \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize the sum of square residuals shown in Equation 7.6\n\\[\n\\sum_{i=1}^{n}e_i^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n(y_i - [\\beta_0 + \\beta_1x_{i1} + \\dots+ \\beta_px_{ip}])^2\n\\tag{7.6}\\]\n\n7.3.1 (Optional) Matrix notation for multiple linear regression\n\n\nThe calculus required to find the values that minimize Equation 7.6 can become quite cumbersome, particularly if there are a lot of predictor variables. We can instead think of Equation 7.3 and Equation 7.5 in terms of matrices and utilize linear algebra and matrix calculus  to compute the estimated coefficients. Suppose there are \\(n\\) observations and \\(p\\) predictors, then\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \\hspace{10mm}\n\\mathbf{X} = \\begin{bmatrix}1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\1& x_{21} & x_{22} & \\dots & x_{2p} \\\\\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\1& x_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\]\nwhere \\(\\mathbf{Y}\\) is an \\(n \\times 1\\) vector of the values of the response, and \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) matrix called the design matrix. The first column of \\(\\mathbf{X}\\) is a vector of 1’s that correspond to the intercept \\(\\beta_0\\). The remaining columns are the observed values for each predictor variable. For example, the row \\(\\begin{bmatrix} 1 & x_{11}  & x_{12} &  \\dots & x_{1p} \\end{bmatrix}\\) contains the observed values for the first observation, and\nthe column \\(\\begin{bmatrix} x_{11}  & x_{21} &  \\dots & x_{n1} \\end{bmatrix}^\\mathsf{T}\\) is the column of observed values for predictor \\(X_1\\).\nAdditionally, let\n\\[\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \\hspace{5mm}\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\]\nsuch that \\(\\boldsymbol{\\beta}\\) is a \\((p + 1) \\times 1\\) vector of the model coefficients and \\(\\boldsymbol{\\epsilon}\\) is an \\(n \\times 1\\) vector of the error terms. Equation 7.3 written in matrix notation is\n\\[\n\\underbrace{\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}}_{\\mathbf{Y}} =\n\\underbrace{\\begin{bmatrix}1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\1& x_{21} & x_{22} & \\dots & x_{2p} \\\\\\vdots & \\vdots & \\ddots & \\vdots\\\\1& x_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}}_{\\mathbf{X}} \\hspace{2mm}\n\\underbrace{\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}}_{\\boldsymbol{\\beta}}  +\n\\underbrace{\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}}_{\\boldsymbol{\\epsilon}}\n\\]\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\tag{7.7}\\]\nSimilarly, Equation 7.5 in matrix notation is\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\boldsymbol{\\hat{\\beta}}\n\\]\nApplying the rules of linear algebra and matrix calculus, we find that the estimated coefficients \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) can be found using Equation 7.8\n\\[\n\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} \\mathbf{Y}\n\\tag{7.8}\\]\nDetails for using the matrix form of the model to find \\(\\hat{\\boldsymbol{\\beta}}\\) in Equation 7.8 are in Section A.3.\n\nMatrix form of simple linear regression\nThe design matrix \\(\\mathbf{X}\\) and the vector of coefficients \\(\\boldsymbol{\\beta}\\) are indexed by the number of predictors \\(p\\). The number of predictors can be any value between 1 and \\(n\\), the number of observations2. Therefore, the linear regression computations in matrix form apply to simple linear regression as well.\n\nOne way to practice using the matrix representation of the model is to compare the results from simple linear regression in Section 4.4 to the results using matrix notation.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-mlr-interpret",
    "href": "07-mlr.html#sec-mlr-interpret",
    "title": "7  Multiple linear regression",
    "section": "\n7.4 Interpreting model coefficients",
    "text": "7.4 Interpreting model coefficients\nThe fitted model for explaining variability in the weight of young lemurs is in Table 7.3. We refer to this model as we interpret the model coefficients.\n\n\n\nTable 7.3: Output of model using various characteristics to explain variability in weight of young lemurs\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n43.8\n121.72\n0.360\n0.719\n\n\nage\n136.9\n4.86\n28.182\n0.000\n\n\nsexM\n-88.8\n68.00\n-1.306\n0.193\n\n\ntaxonPCOQ\n471.7\n95.43\n4.943\n0.000\n\n\ntaxonVRUB\n1037.3\n133.91\n7.747\n0.000\n\n\nlitter_size\n-19.6\n65.80\n-0.298\n0.766\n\n\n\n\n\n\n\n\n\n7.4.1 Interpreting coefficients of quantitative predictors\nThe interpretation for coefficients of quantitative predictors is similar to the interpretation in simple linear regression in Section 4.5. Given a quantitative predictor \\(x_j\\), its model coefficient \\(\\beta_j\\) is the slope describing the relationship between \\(x_j\\) and the response variable, after accounting for the other predictors in the model. For example, the coefficient for age in Table 7.3 is 136.903. Let’s compare this to the coefficient from a simple linear regression model using age shown in Table 7.4.\n\n\n\nTable 7.4: Simple linear regression model of weight versus age\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n555\n67.31\n8.25\n0\n\n\nage\n142\n5.88\n24.12\n0\n\n\n\n\n\n\n\n\nIn the model in Table 7.4 that only uses age to predict weight, the weight is expected to increase by 141.892 grams, on average for each month increase in age. If we also take into account sex, taxon, and litter size in addition to age, then the weight is expected to increase by 136.903 grams, on average, as age increases by one month. This means some of the change in weight as age increases in Table 7.4 can be explained by the other predictors in Table 7.3.\n\nWe write the interpretation of the coefficients in a multiple linear regression models in a way that indicates there are other variables being taken into account in the model. There are two commonly used ways to word this. The first is by interpreting the coefficient \\(\\beta_j\\) as the expected change in the response variable when \\(x_j\\) increases by one unit, adjusting for all other predictors in the model. This interpretation means that once we have taken into account the relationship between all the other predictors and the response variable, then we have isolated the effect of the predictor \\(x_j\\) and thus can explain the relationship between the response variable and the predictor \\(x_j\\). The second way is to specify that all other predictors are held constant. This means that if have two observations with the same values for all other predictors, then we see how the value of the response is expected to change between these two observations as \\(x_j\\) changes. \nLet’s apply this to interpret age in the model in Table 7.3. The following are two acceptable forms of the interpretation:\n\nFor each additional month in a young lemur’s age, their weight is expected to be greater by 136.903 grams, on average, adjusting for taxon, sex, and litter size.\nFor each additional month in a young lemur’s age, their weight is expected to be greater by 136.903 grams, on average, holding taxon, sex, and litter size constant.\n\nThe phrase “holding all else constant” stems from the fact that multiple linear regression is conducted assuming the predictors are independent. Some argue, however, that it is not feasible for the predictors to be completely independent in practice, so the phrasing “adjusting for all other predictors” may better represent the model in practice. We will use both forms of the interpretation throughout the book. We talk more about more the relationship between the predictors in Chapter 8.\n\n\nInterpret the coefficient litter_size in the context of the data.\nRecall from Section 4.5, “in the context of the data” means\n\nusing the variable names or meaningful descriptions of the variables,\nincluding the units in the interpretation, and\nIndicating the population for which this model applies.3\n\n\n\n\n7.4.2 Interpreting coefficients of categorical predictors\nWhen categorical predictors are included in a model, we need a way to transform the levels (or categories) of the predictor into quantitative information that can be used in the regression calculations. To do so, indicator variables are created for each level of the categorical predictor. We then use these indicator variables in the regression model.\nSuppose we have a categorical variable that has \\(k\\) levels. We define \\(k\\) indicator variables, one for each level, such that the indicator takes the value \\(1\\) if the observation belongs to the particular level and \\(0\\) if the observation does not belong to the particular level. Let’s take a look at the variable taxon in the lemurs data.\n\nThe variable taxon has three levels, ERUF, PCOQ, and VRUB, so we create three indicator variables. We will call these indicators taxonERUF, taxonPCOQ, and taxonVRUB. Table 7.5, shows the number of observation at each level of the original variable taxon and the number of observations that take values 1 or 0 for each indicator variable.\n\n\n\nTable 7.5: Indicator variables for taxon\n\n\n\n\ntaxon\ntaxonERUF\ntaxonPCOQ\ntaxonVRUB\nn\n\n\n\nERUF\n1\n0\n0\n48\n\n\nPCOQ\n0\n1\n0\n93\n\n\nVRUB\n0\n0\n1\n111\n\n\n\n\n\n\n\n\nNow we are ready to use the indicator variables in the model. In Table 7.3, we see there are only two indicator variables (not three) for taxon in the model. When there are \\(k\\) levels of a categorical predictor, there are \\(k-1\\) indicator variables for the categorical predictor included in the model. The level that does not have an indicator variable in the model is called the baseline level (this is also known as the reference level). Thus, the interpretation of the coefficients for indicator variables describe how much the expected response for a given level is expected to differ, on average, compared to the expected response at the baseline level, after adjusting for the other predictors in the model. \n\n\nLet’s take a look at the indicator variables for taxon in Table 7.3. The baseline level is “ERUF”, the taxon that is not represented by an indicator variable in the model. Therefore, the interpretations of the coefficients for taxonPCOQ and taxonVRUB will be how the weights for young lemurs in those taxa are expected to differ, on average, to the weight for lemurs in the ERUF taxon, after adjusting for age, sex, and litter size.\n\nThe coefficient for the PCOQ taxon is \\(\\hat{\\beta}_{PCOQ} =\\) 471.715. This means that the expected weight is 471.715 grams greater, on average, for lemurs in the PCOQ taxon compared to the expected weight for lemurs in the ERUF taxon, holding age, sex, and litter size constant.\nThe coefficient for the VRUB taxon is \\(\\hat{\\beta}_{VRUB} =\\) 1037.323. This means that the expected weight is 1037.323 grams greater, on average, for lemurs in the VRUB taxon compared to the expected weight for lemurs in the ERUF taxon, holding age, sex, and litter size constant.\n\n\nThe code in ?sec-mlr-estimate-R shows that we only need to put the name of the categorical predictor into the lm function for fitting the model. R (and other statistical software) will create indicator variables behind the scenes and selects the baseline level as the following:\n\nIf the categorical predictor is nominal (it has no inherent order like taxon), then R chooses the first level alphabetically to be the baseline. R made ERUF the baseline level for taxon, because it is first alphabetically in the list of ERUF, PCOQ, and VRUB.\nIf the categorical predictor is ordinal (there is some order imposed), then R chooses the lowest level to be the baseline.\n\nAs the data scientists, we can determine which level we would like to be the baseline level if neither of these approaches align well with the data. To do so, use functions to impose an ordering with the desired baseline as the first level.\n\nIndicator variables in the model describe how much the intercept of the model shifts based on the levels of the categorical variable. The intercept term in the model is the intercept associated with the baseline level for the categorical predictor. This means the intercepts for taxon are the following:\n\nERUF: 43.844\nPCOQ: 515.559 (43.844 + 471.715)\nVRUB: 1081.167 (43.844 + 1037.323 )\n\nThis shift in the intercept is visually represented in the scatterplot of age versus weight by taxon in Figure 7.5.\n\n\n\n\n\n\n\nFigure 7.5: Different intercepts for each taxon. Purple dotted line = ERUF, Pink dashed line = PCOQ, Yellow sold line = VRUF\n\n\n\n\n\n\nThe other categorical predictor in the model in Table 7.3 is sex.\n\nWhat is the baseline level for sex?\nInterpret the coefficient of sex in the context of the data.\nWhat is the intercept for female lemurs? For male lemurs?4\n\n\n\n\n7.4.3 Interpreting the intercept\nAs with simple linear regression, the intercept is the expected value of the response when all predictors in the model equal 0. Let’s take a look at what this means for our model in Table 7.3. The intercept is at the point such that\n\n\n\nage = 0: The lemur is 0 months old at the time of the weight\n\nsexM = 0: The lemur is female.\n\ntaxonPCOQ = 0 and taxonVRUB = 0: The lemur is from the ERUF taxon. (See Table 7.5 to confirm this)\n\nlitter_size = 0: There were 0 lemurs in the litter size in which the lemurs born. (Note that this is not a possible in the data set).\n\nBased on the model, the expected weight for such lemurs is about 43.844 grams.\nPutting this together in narrative form, we have\n\nThe expected weight for female lemurs from the ERUF taxon who are 0 months old at the time of weight and were born into a litter with 0 lemurs is about 43.844 grams.\n\nAs in SLR, we need the intercept to get the least-squares model. The intercept, however, does not always have a meaningful interpretation in practice. Therefore, as with simple linear regression, the intercept has a meaningful interpretation when the following are true:\n\nThe scenario described by all the predictors in the model being at or near zero is plausible in practice.\nThe data used to fit the model includes observations in which all the predictors are at or near zero.\n\n\nBased on these criteria, do you think the interpretation of the intercept is meaningful for the lemurs data? Why or why not?5",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#prediction",
    "href": "07-mlr.html#prediction",
    "title": "7  Multiple linear regression",
    "section": "\n7.5 Prediction",
    "text": "7.5 Prediction\nThe same process is used to predict for multiple linear regression as the one introduced in Section 4.6 for simple linear regression. Now that there are two or more predictor variables, we need to input a value for every predictor variable.\nFor example, suppose we wish to use the model in Table 7.3 to compute the predicted weight for a 10-month old male lemur in the VRUB taxon who was born into a litter of 4 lemurs. We compute this value by plugging in the value for the new observation into the model equation as shown in Equation 7.9.\n\\[\n\\begin{aligned}\n\\widehat{\\text{weight\\_g}} &= 43.844 \\\\ &+ 136.903 \\times 10 \\\\ &- 88.838 \\times 1 \\\\ &+ 471.715 \\times 0 \\\\ &+ 1037.323 \\times 1 \\\\ &- 19.607 \\times 4 \\\\\n& = 2282.931\n\\end{aligned}\n\\tag{7.9}\\]\n\nUse the equation in Table 7.3 to compute the predicted value for a 14-month old female lemur in the ERUF taxon who was born into a litter of 2 lemurs. It should match the predicted value shown it the output above.6\n\nIt is important to avoid extrapolation when making predictions using a multiple linear regression model. This can sometimes to be challenging to assess, as extrapolation might occur both in terms of a single predictor variable being far outside the range in the data but also in terms of a combination of predictors being outside the general patterns observed in the data. When working with real-world data, we can consider if the combination of predictor values can be realistically observed in the population being studied. If that is the case, and the data are representative of the population, then the combination of values if likely to be within the trend represented in the data.\nOne more consideration for prediction is that we expect there to be some uncertainty in the predicted values, because they are computed based on a model fit using sample data.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-other-predictors",
    "href": "07-mlr.html#sec-other-predictors",
    "title": "7  Multiple linear regression",
    "section": "\n7.6 Centered and standardized predictors",
    "text": "7.6 Centered and standardized predictors\n\n\n7.6.1 Centered quantitative predictors\n\n\nWe need an intercept to fit the least-squares regression model, but it may not have a meaningful interpretation as seen in the example in Section 7.4.3. This is usually because it is not meaningful or realistic for a quantitative variable to take values at or near zero. One way to remedy this is to shift the distribution of one or more quantitative predictors, such that it would be feasible for the shifted predictors to take values near zero. This process is called centering.\n\nCentering a predictor means to shift every observation of that predictor by a single constant, denoted \\(C\\). For example, given a predictor \\(X\\), then the centered version of the predictor is \\(X_{cent} = X - C\\). Thus when \\(X_{cent}\\) is a predictor in the model (instead of \\(X\\)), then the intercept is now the expected response when \\(X_{cent} = X - C = 0\\), which occurs when \\(X = C\\).\nIn the previous section, we determined that it is not possible for the litter size to be 0, because the litter size includes the observed lemur. Therefore, we can use centering to create a new variable litter_size_cent by shifting litter_size by 1, so that litter_size_cent = 0 means litter_size = 1 . Though it is not always the case, the newly centered variable litter_size_cent has a new meaningful interpretation. It is the number of siblings of in the litter. Putting this together, litter_size_cent = 0 means the lemur was born into a litter with no siblings.\nBecause this centered variable has its own meaningful interpretation, we will rename the variable to more clearly indicate what this variable is measuring. We call the centered variable litter_size_siblings and use this name for the centered variable moving forward. The distributions of the original and centered variables are shown in Figure 7.6 and Table 7.6.\n\n\n\n\n\n\n\n\n\n(a) Distribution of litter_size\n\n\n\n\n\n\n\n\n\n(b) Distribution of litter_size_siblings\n\n\n\n\n\n\nFigure 7.6: Original versus centered values of litter_size\n\n\n\n\n\nTable 7.6: Distributions of litter_size vs litter_size_siblings\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\n\n\n\nlitter_size\n1.7\n0.9\n1\n1\n1\n3\n4\n\n\nlitter_size_siblings\n0.7\n0.9\n0\n0\n0\n2\n3\n\n\n\n\n\n\n\n\nFrom Figure 7.6 and Table 7.6, we see that the only statistics that have changed are those about location (e.g., \\(Q1\\), \\(Q3\\), min, max) and those related the center (mean and median). The shape of the distribution and statistics regarding the spread (standard deviation and IQR) have remained the same after centering. By centering the variable we have essentially moved the distribution along the number line by some constant amount.\nFigure 7.7 shows the relationship between weight versus the original and centered values of litter_size. Both plots have the same slope, illustrating that the slope of the relationship between the predictor and response has not changed by centering. All that has changed is what in means to be “zero”, what observations whose expected response is at the intercept. This is illustrated by the different intercepts for the models shown in the two plots.\n\n\n\n\n\n\n\n\n\n\n(a) weight versus litter_size\n\n\n\n\n\n\n\n\n\n(b) weight versus litter_size_siblings\n\n\n\n\n\n\nFigure 7.7: Weight versus original and centered values of litter size\n\n\nA commonly used type of centering is mean-centering, in which \\(C = \\bar{X}\\), meaning a quantitative predictor is shifted by its mean. Let’s mean-center age, so that the intercept does not represent lemurs who are 0 months old, but rather lemurs are who are the mean age in the data set.\n\\[\\text{age\\_cent} = \\text{age} - \\text{mean(age)}\n\\]\nTable 7.7 is a table of the summary statistics for the age and age_cent. As before, the location statistics have changed but the variability is the same.\n\n\n\nTable 7.7: Distributions of age and age_cent\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\n\n\n\nage\n9\n7\n0.6\n2.8\n7.1\n15.6\n23.9\n\n\nage_cent\n0\n7\n-8.5\n-6.3\n-2.0\n6.6\n14.9\n\n\n\n\n\n\n\n\nNote that we do not mean-center the categorical predictors sex and taxon, because there is already a meaningful interpretation for the indicator variables for these predictors being equal to 0, and there is not a realistic notion of the “average taxon” or “average sex”. Unlike the categorical predictors, we could mean-center litter_size as well.; however, we created the new variable litter_size_siblings by shifting by 1.\n\n.\n\nThe output for the model with age_cent and litter_size_siblings, the centered and litter size respectively, is is in Table 7.8 .\n\n\n\nTable 7.8: Model for lemurs data with centered quantitative predictors\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n1263.2\n81.59\n15.482\n0.000\n\n\nage_cent\n136.9\n4.86\n28.182\n0.000\n\n\nsexM\n-88.8\n68.00\n-1.306\n0.193\n\n\ntaxonPCOQ\n471.7\n95.43\n4.943\n0.000\n\n\ntaxonVRUB\n1037.3\n133.91\n7.747\n0.000\n\n\nlitter_size_siblings\n-19.6\n65.80\n-0.298\n0.766\n\n\n\n\n\n\n\n\n\nA comparison of the model coefficients from the original model and the model with the mean-centered values for the quantitative predictors is shown in Table 7.9. Let’s consider in what ways the two models are the same and in what ways they are different.\n\n\n\nTable 7.9: Model coefficients for original model and model with centered quantitative predictors\n\n\n\n\nTerm\nOriginal\nCentered\n\n\n\n(Intercept)\n43.8\n1263.2\n\n\nage\n136.9\n136.9\n\n\nsexM\n-88.8\n-88.8\n\n\ntaxonPCOQ\n471.7\n471.7\n\n\ntaxonVRUB\n1037.3\n1037.3\n\n\nlitter_size\n-19.6\n-19.6\n\n\n\n\n\n\n\n\nThe only term that changed between the two models is the intercept; the coefficients for all other predictors remained the same. This shows that centering one or more quantitative predictors, by shifting by the mean or some other constant, does not change the relationships between the predictor variables with the response variable. With this in mind, let’s interpret the intercept for the model with the centered variables.\nAs before, the intercept is the expected weight when all the predictors are equal to 0. Thus, using the model in Table 7.8, the intercept is the expected weight when\n\nage_cent = 0: The lemur is the mean age in the sample, 9.05 months old.\nsexM = 0: The lemur is female.\ntaxonPCOQ = 0 and taxonVRUB = 0: The lemur is from the ERUF taxon.\nlitter_size_siblings = 0: The lemur was born into a litter with 0 other lemurs (litter_size = 1).\n\nThe interpretation of the intercept in narrative form when the quantitative predictors are centered is\n\nThe expected weight for female lemurs from the ERUF taxon who are 9.05 months old and were born into a litter with no siblings is about 1263.178 grams.\n\nBecause the coefficients for the other predictors are the same, the interpretations for these predictors are the same as in Section 7.4.1 and Section 7.4.2.\n\n7.6.2 Standardized quantitative predictors\nThus far, we have interpreted the coefficients to understand the relationship between the response and each predictor variable. Sometimes, however, the primary objective of the analysis may be less focused on interpretation and more focused on identifying which predictors are most important in understanding variability in the response variable. When that is the case, we can fit a model using standardized values of the quantitative predictors.\n\nStandardizing a variable is shifting every observation of that variable by its mean (as in mean-centering) and dividing by its standard deviation. Thus, given a quantitative predictor \\(X\\), the standardized value of \\(X\\), denoted \\(X_{std}\\), is\n\\[\nX_{std} = \\frac{X - \\bar{X}}{s_X}\n\\]\nwhere \\(\\bar{X}\\) is the mean of \\(X\\) and \\(s_{X}\\) is the standard deviation of \\(X\\). In addition to shifting the centered of the new distribution to be equal to 0, the standardized variable is rescaled such the standard deviation (and variance) of the standardized variable is 1. This is similar to computing a \\(z\\)- score from the normal distribution.\n\n\nUnlike centering, standardizing a variable changes its units, because it is rescaled by the standard deviation. Therefore, the units are no longer the original units but rather the number of standard deviations an observation is away from the mean value. Because the units have changed, the value (but not the sign!) of the coefficient for standardized variables will change as well. Table 7.10 shows the output of the model using standardized coefficients for age and litter_size. Similar to centering, we only standardize quantitative predictors, as there is no meaningful notion of “average” or “standard deviation” for categorical predictors such as taxon or sex.\n\n\n\nTable 7.10: Model with standardized coefficients for quantitative predictors\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n1249.0\n89.4\n13.978\n0.000\n\n\nage_at_wt_std\n960.1\n34.1\n28.182\n0.000\n\n\nsexM\n-88.8\n68.0\n-1.306\n0.193\n\n\ntaxonPCOQ\n471.7\n95.4\n4.943\n0.000\n\n\ntaxonVRUB\n1037.3\n133.9\n7.747\n0.000\n\n\nlitter_size_std\n-18.1\n60.8\n-0.298\n0.766\n\n\n\n\n\n\n\n\nNow let’s compare the model with standardized quantitative predictors to the original model and the model using centered quantitative predictors. Table 7.11 shows the comparison of these models.\n\n\n\nTable 7.11: Model coefficients for original model, model with centered quantitiatve predictors, and model with standardized quantitative predictors\n\n\n\n\nTerm\nOriginal\nCentered\nStandardized\n\n\n\n(Intercept)\n43.8\n1263.2\n1249.0\n\n\nage\n136.9\n136.9\n960.1\n\n\nsexM\n-88.8\n-88.8\n-88.8\n\n\ntaxonPCOQ\n471.7\n471.7\n471.7\n\n\ntaxonVRUB\n1037.3\n1037.3\n1037.3\n\n\nlitter_size\n-19.6\n-19.6\n-18.1\n\n\n\n\n\n\n\n\n\nThe coefficients for sexM , taxonPCOQ, and taxonVRUB are the same across all three models. Why do these coefficients stay the same?7\n\nThe interpretation for the standardized predictors is in terms of a one standard deviation (one “unit”) increase in the predictor. Below is the interpretation of the coefficient of age_std, the standardized value of age.\n\nFor every one standard deviation increase in the age, the weight of lemurs is expected to increase by 960.139 grams, on average, holding taxon, sex, and litter size constant.\n\nThough this interpretation is technically correct, it is not meaningful to the reader to merely say a “one standard deviation increase”. Therefore, we can use the exploratory data analysis in Table 7.1 to be more specific about how much a one standard deviation increase in the age is and make this interpretation more easily understood to readers.\n\nFor every 7 months increase in age, the weight of lemurs is expected to increase by 960.139 grams, on average, holding taxon, sex, and litter size constant.\n\nWe often standardize predictors when we want to identify which predictors are most important in explaining variability in the response variable. Because all the quantitative predictors are now on the same scale, we can look at the magnitude of the coefficients to rank the predictors in terms of importance.  From the coefficients in Table 7.10, we see that taxonVRUB is the most important predictor, because it has the coefficient with the highest magnitude, i.e., the highest absolute value. This means that just knowing the lemur is from the VRUB taxon results in the largest expected change in the weight, on average, even after adjusting for the other predictors.\n\nBased on Table 7.10, which predictor is the least important for predicting the weight of lemurs?8",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#sec-mlr-interaction",
    "href": "07-mlr.html#sec-mlr-interaction",
    "title": "7  Multiple linear regression",
    "section": "\n7.7 Interaction terms",
    "text": "7.7 Interaction terms\n\n\nThus far, we have fit models in which the coefficient of each predictor is the same for all lemurs. What if, however, the effect of one predictor on the response variable differed by values of another predictor? For example, what if the relationship between age and weight differed based on the lemur’s taxon? Or what if the relationship between litter size and weight differed by age? Or what if the relationship between sex and weight differed by taxon? We can explore all of these, and similar, questions by including interaction terms in the regression model.\nSometimes the relationship between one predictor and the response variable differs based on values of another predictor variable. We account for this difference by including interaction terms in the regression model. There are three types of interaction terms between two variables: interaction between a quantitative and categorical predictor, interaction between two categorical predictors, and interaction between two quantitative predictors. We discuss all three types of potential interaction terms here. Note, however, we most often include interaction terms involving a categorical predictor, because we want to account for the unique relationship between the response and predictor variable for different subgroups of the population.\n\n\n\n7.7.1 Interaction between a quantitative and a categorical predictor\nDo different types of lemurs grow at different rates? We can explore this question by looking at the relationship between weight and age and how that differs (or not) by taxon. Before fitting the models, we can use exploratory data analysis to get an initial understanding about the relationship between age and weight for each taxon. In Section 7.1.1, we used univariate and bivariate exploratory data analysis to explore one variable and two variables, respectively. Now when we look at potential interaction effects, we use multivariate exploratory data analysis as introduced in Chapter 3.\n\n\n\n\n\n\n\nFigure 7.8: Multivariate EDA to explore potential interaction effect between age and taxon\n\n\n\n\nFigure 7.8 shows the relationship between age and weight by taxon. Based on the lines, it appears that there is some difference in the relationship between age and weight depending on the taxon, as the slopes of the lines appear to differ. Thus, the plot suggests a potential interaction effect that is worth exploring in the model. If there was no potential interaction effect, then the lines would be parallel (i.e., the same slope). As with any exploratory data analysis, we do not use this plot to conclude whether or not there is an interaction effect; we will use the regression model and statistical inference for those purposes. This plot provides some intuition and guidance of what we might expect as we look at the results of the regression model.\n\nLemurs from which taxon appear to grow at the fastest rate?9\n\n\n\n\nTable 7.12: Output for regression model including interaction between age and taxon\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n578.8\n135.43\n4.274\n0.000\n\n\nage\n78.6\n9.95\n7.900\n0.000\n\n\nsexM\n-85.2\n60.97\n-1.398\n0.163\n\n\ntaxonPCOQ\n-355.7\n134.37\n-2.647\n0.009\n\n\ntaxonVRUB\n634.0\n162.69\n3.897\n0.000\n\n\nlitter_size\n-36.9\n58.64\n-0.629\n0.530\n\n\nage:taxonPCOQ\n96.1\n12.05\n7.971\n0.000\n\n\nage:taxonVRUB\n49.4\n11.96\n4.125\n0.000\n\n\n\n\n\n\n\n\nThe model wit the interaction between age and taxon is in Table 7.12. The interaction terms are indicated in the output in the format var1:var2; for example, the interaction term of age for the PCOQ taxon is labeled as age:taxonPCOQ. This individual term is an estimate of how much the slope of age differs between the taxa PCOQ and ERUF, the baseline. Thus, the interpretation is\n\nThe slope of age for lemurs in the PCOQ taxon is about 96.085 greater for lemurs in PCOQ compared to lemurs in the ERUF taxon. This means for each additional month older a lemur is, the weight of a PCOQ lemur is expected to increase by 96.085 grams more, on average, compared to an ERUF lemur, holding sex and litter size constant.\n\nThis interpretation aligns with the steep slope of the line for the PCOQ species in Figure 7.8. Note from the interpretation that coefficient of the interaction effect is how much the relationship between age and weight differ for the PCOQ species compared to the ERUF species, but it is not the overall relationship between age and weight for PCOQ lemurs. To get that value, we must add the main effect of age (age) and the interaction effect (age:taxonPCOQ).\n\nThe overall slope of age for PCOQ lemurs is 174.664 (78.579 + 96.085 ). This means that among PCOQ lemurs, for each additional month in age, the weight is expected to be greater by 174.664 grams, on average, holding sex and litter size constant.\n\n\nWhat is the slope of age for lemurs in the VRUB taxon? Interpret it in the context of the data.10\n\nBy now we have quantified the relationship between age and weight for PCOQ and VRUB lemurs, but what about this relationship for ERUF lemurs, the baseline category? The interaction effect age:taxonPCOQ is the adjustment in the slope of age for PCOQ lemurs; the same is true about age:taxonVRUB for VRUB lemurs. Therefore, neither of these interaction effects apply to ERUF lemurs. Putting all this together, the coefficient of age for ERUF lemurs is 78.579. The term in the model age is called a main effect, because it describes the relationship between the response and predictor variable (among the baseline subgroup). The terms age:taxonPCOQ and age:taxonVRUB called interaction effects, because they are how much the relationship between the response and one predictor differs by values of another predictor.\n\n\n\n\nWe used the original variables age and litter_size in this section; however, everything we’ve shown about interaction terms also applies to centered and standardized variables.\n\n\n7.7.2 Interaction between two categorical predictors\nAnother type of interaction effect is between two categorical predictors. Let’s consider the interaction between taxon and sex for this data. We might include this interaction effect if we want to explore whether the relationship between sex and weight differs by taxon.11\n\n\n\n\n\n\n\nFigure 7.9: weight versus sex by taxon\n\n\n\n\nFigure 7.9 shows the relationship between sex and weight by taxon. Recall that an interaction effect means that the relationship between one predictor and the response variable differs by values of another predictor. Therefore, as we examine this plot, we are looking to see whether the relationship between sex and weight appears similar or different for each taxon (see Section 3.6). When we look at the boxplots for male and female relative to one another in the figure, we don’t see much difference across the three taxa, as there is a lot of overlap between the box plots for each taxon. The median weight for males versus females appears to differ for PCOQ lemurs compared to the other two groups. Thus, from the EDA we might expect a useful interaction for sex and the PCOQ species, but maybe not for VRUB.\nWe add this interaction term in to the model using the same syntax as before. The output from the regression model is in Table 7.13.\n\n\n\nTable 7.13: Output for regression model including interaction between sex and taxon\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n-60.3\n134.96\n-0.447\n0.656\n\n\nage\n137.1\n4.88\n28.102\n0.000\n\n\nsexM\n169.2\n159.26\n1.063\n0.289\n\n\ntaxonPCOQ\n633.2\n124.57\n5.083\n0.000\n\n\ntaxonVRUB\n1124.0\n152.67\n7.363\n0.000\n\n\nlitter_size\n-14.4\n65.58\n-0.219\n0.826\n\n\nsexM:taxonPCOQ\n-387.4\n192.94\n-2.008\n0.046\n\n\nsexM:taxonVRUB\n-253.2\n188.75\n-1.341\n0.181\n\n\n\n\n\n\n\n\n\n\nSimilar to the interpretations in the previous section, the interaction terms tell us how much the effect of sex among PCOQ or VRUB lemurs differ compared to the effect of sex for the baseline, ERUF lemurs. For example, the interaction term for PCOQ lemurs, sexM:taxonPCOQ = -387.382. This interpretation of this value is as follows:\n\nThe difference in the average weight of male lemurs compared to female lemurs is predicted to be 387.382 grams less for lemurs from the PCOQ taxon compared to those in the ERUF taxon, holding age and litter size constant.\n\nAs before, to get the overall difference in the average weight between male and female lemurs for the PCOQ species, we add the coefficients for the main effect sexM and the interaction effect sexM:taxonPCOQ. The overall effect of sex for PCOQ lemurs is -218.144 (169.238 + -387.382). The interpretation of this value is as follows:\n\nFor lemurs in the PCOQ taxon, the weight of male lemurs is expected to be 218.144 grams less than the weight of female lemurs, on average, holding age and litter size constant.\n\n\n\nWhat is the overall effect of sexM for lemurs in the VRUB taxon?\nWhat is the overall effect of sexM for lemurs in the ERUF taxon?12\n\n\n\n\n7.7.3 Interaction between two quantitative predictors\nThe last type of interaction is between two quantitative predictors. This type of interaction is less common than the interactions including at least one categorical predictor, but they can be found in some scientific contexts. For the lemurs analysis, we will look at the interaction effect between age and litter_size. As with the interaction effect between two categorical predictors, we can interpret the interaction between age and litter_size as how much the effect of litter size changes as age changes or how much the effect of age changes as litter size changes. While either of these interpretations would be statistically valid, we will focus on the how the effect of litter size changes as age changes. This will allow us to more directly explore whether the impact of a lemur’s birth conditions (represented by the litter size) changes as the lemur gets older.\nVisualizations for the potential interaction effect between two quantitative predictors can be complex, because a continuous predictor can take on infinitely many values. Therefore, when visualizing the potential interaction effect, we will create bins for one of the predictors and use the bins to see how the value of one of the predictors changes based on the other.  Figure 7.10 is a scatterplot exploring the potential interaction between age and litter size.\n\n\n\n\n\n\n\nFigure 7.10: weight versus litter_size by age\n\n\n\n\nBecause we are specifically interested in interpreting how the effect of litter size changes as age changes, we have created bins to mark off age ranges and looked a the scatterplot of litter size and weight for each bin.\nIn Figure 7.10, the relationship between litter size and weight appears flatter for the older age groups, in particular the 20 - 25 month age group. This means it appears that litter size has much more impact on the weight among very young lemurs compared to those closer to two years old. As with all exploratory data analysis, we can not make a definitive conclusion based on this visualization alone, so we will fit a model with this interaction using the code below.\n\n\n\nTable 7.14: Output for regression model including interaction between age and litter_size\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n-132.3\n148.49\n-0.891\n0.374\n\n\nage\n155.6\n10.34\n15.046\n0.000\n\n\nsexM\n-69.6\n68.22\n-1.020\n0.309\n\n\ntaxonPCOQ\n477.7\n94.86\n5.036\n0.000\n\n\ntaxonVRUB\n1042.5\n133.07\n7.834\n0.000\n\n\nlitter_size\n77.4\n80.78\n0.958\n0.339\n\n\nage:litter_size\n-10.9\n5.33\n-2.045\n0.042\n\n\n\n\n\n\n\n\nThe output for the model with the interaction between age and litter_size is in Table 7.14. The coefficient for the term age:litter_size is the amount the effect of litter size changes as age increases by 1 month. It is interpreted as follows:\n\nFor each additional month in age, the slope of litter size and weight decreases by 10.902, on average, holding taxon and sex constant.\n\nAs with previous interaction effects, we add the coefficients for the main effect term and interaction term to get the overall effect of litter size at different ages. Recall that interaction effects are just two predictor variables multiplied together, so we need to plug in a value of age to get the overall effect of litter for that age. When we had interaction effects with at least one categorical predictor, we plugged in the 1 or 0 for the indicator variable. Now we can choose any age in the range of our data to plug in. We may also want to plug in a few different ages throughout the range to see how the effect of litter size compares for different ages.\nLet’s interpret the effect of litter size for lemurs who are 12 months old. The effect of litter size for lemurs of this age is -53.4 (77.424 + 12 \\(\\times\\) -10.902). The interpretation is as follows:\n\nFor lemurs who are 12 months old, for each additional lemur in their litter, their weight is expected to be less by 53.4 grams, on average, holding sex and taxon constant.\n\nThe coefficient of the main effect of litter_size is the effect of litter size when age = 0. There are no lemurs who are 0 months old in the data, so we should be cautious about trying to derive meaning from the main effect of litter size in this model or refit the model using the centered values of the quantitative predictors.\n\nWhat is the overall effect of litter size for lemurs who are 20 months old? Interpret this value in the context of the data.13\n\n\nIn this section we only discussed interactions between two variables, but there can be interactions between three or more variables. Though such interactions are possible, they should be used with caution to avoid fitting the model too closely to the sample data and losing generalizability (called model overfit). These complex interactions also become difficult to interpret and can be harder to explain to a general audience.\nTherefore, we recommend only using interaction effects with two variables unless there is a specific research or analysis objective that benefits from the more complex interaction effects.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#multiple-linear-regression-in-r",
    "href": "07-mlr.html#multiple-linear-regression-in-r",
    "title": "7  Multiple linear regression",
    "section": "\n7.8 Multiple linear regression in R",
    "text": "7.8 Multiple linear regression in R\n\n7.8.1 Estimating coefficient using R\nSimilar to simple linear regression, we use the lm function to find the coefficient estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\). The code to fit the the model using age, sex, taxon, and litter size to understand variability in the weight of young lemurs is shown below.\n\nlemurs_model &lt;- lm(weight ~ age + sex + \n                     taxon + litter_size,\n                   data = lemurs)\n\ntidy(lemurs_model) |&gt;\n  kable(digits = 3)\n\nThe tidy function structures the model output into a data data frame, and kable() is used to display the results as a neatly formatted table.\n\n7.8.2 Prediction\nWe use predict() to compute predicted values from multiple linear regression models. We begin by making a tibble that contains the values fo the new observation. The variable names in the tibble of new observations must match the column names in the original data exactly, including letter case. Additionally, the observed values of the categorical predictors are put in the tibble, not indicator variables. As with variable names, the values of the categorical predictors must be input exactly as they show in the data set (including case).\n\nnew_lemur &lt;- tibble(age = 10, \n                    taxon = \"VRUB\", \n                    sex = \"M\", \n                    litter_size = 4)\n  \n  \npredict(lemurs_model, new_lemur)\n\n   1 \n2283 \n\n\nWe can also use predict() to compute the predicted values for multiple new observations. Suppose we wish to predict the weight of the following lemurs:\n\nLemur 1: 10-month old male lemur in the VRUB taxon who was born into a litter of 4 lemurs (same as before)\nLemur 2: 14-month old female lemur in the ERUF taxon who was born into a litter of 2 lemurs\n\nWe use the same process as before, extending the tibble to include the observed values for both new lemurs.\n\nnew_lemurs &lt;- tibble(age = c(10, 14), \n                    taxon = c(\"VRUB\", \"ERUF\"), \n                    sex = c(\"M\",\"F\"), \n                    litter_size = c(4, 2)\n)\n  \n  \npredict(lemurs_model, new_lemurs)\n\n   1    2 \n2283 1921 \n\n\n\nPredictions using augment()\nThe augment function, introduced in Section 6.2, can be used to compute predicted values for each observation in the data set used to fit the model. The predicted values are stored in the column .fitted in the augmented data frame, as shown in Table 7.15.\n\n\nlemurs_model_aug |&gt;\n  select(weight:.fitted) |&gt;\n  slice(1:5) |&gt;\nkable(digits = 3)\n\n\nTable 7.15: Partial output from the lemurs model augumented data frame\n\n\n\n\nweight\nage\nsex\ntaxon\nlitter_size\n.fitted\n\n\n\n410\n2.27\nF\nERUF\n1\n335\n\n\n137\n0.72\nF\nERUF\n1\n123\n\n\n206\n0.69\nF\nPCOQ\n1\n590\n\n\n185\n1.05\nM\nPCOQ\n1\n551\n\n\n373\n2.66\nF\nPCOQ\n1\n860\n\n\n\n\n\n\n\n\n\n7.8.3 Interaction terms\nWe use the asterisks * to include interaction terms between any two variables in R. The code to include the interaction between taxon and age is below.\n\nlemurs_age_taxon_int &lt;- lm(weight ~ age + sex + taxon + \n                             litter_size + taxon * age, \n                           data = lemurs)\n\ntidy(lemurs_age_taxon_int)\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    579.     135.       4.27  2.76e- 5\n2 age             78.6      9.95     7.90  9.51e-14\n3 sexM           -85.2     61.0     -1.40  1.63e- 1\n4 taxonPCOQ     -356.     134.      -2.65  8.65e- 3\n5 taxonVRUB      634.     163.       3.90  1.26e- 4\n6 litter_size    -36.9     58.6     -0.629 5.30e- 1\n# ℹ 2 more rows\n\n\nThe syntax to include the interaction between two quantitative variables is the same as the other interactions. Note also that even though we used the binned values of age to help visualize the potential interaction effect in Figure 7.10, we use the original non-binned values of age in the model.\n\nlemurs_age_litter_int &lt;- lm(weight ~ age + sex + taxon + \n                             litter_size + age * litter_size, \n                           data = lemurs)",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#summary",
    "href": "07-mlr.html#summary",
    "title": "7  Multiple linear regression",
    "section": "\n7.9 Summary",
    "text": "7.9 Summary\nIn this chapter, we introduced multiple linear regression, in which two or more predictor variables are used to understand variability and compute predictions for a quantitative response variable. We introduced a various different predictors that can be in the model, including categorical predictors and interaction terms. We showed how to use R to estimate model coefficients and compute predictions from the model. We expect there is uncertainty in the coefficient estimates produced by the regression model, given it was fit using sample data. In the next chapter, we will discuss inference for multiple linear regression and use it to draw conclusions about the relationships between the response and predictor variables.\n\n\n\n\n\n\n\n\n\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nDuke Lemur Center. n.d. “Meet the Lemurs.” https://lemur.duke.edu/discover/meet-the-lemurs/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer.\n\n\nZehr, SM, RG Roach, D Haring, J Taylor, FH Cameron, and AD Yoder. n.d. “Life History Profiles for 27 Strepsirrhine Primate Taxa Generated Using Captive Data from the Duke Lemur Center. Sci Data. 2014; 1: 140019.”",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "07-mlr.html#footnotes",
    "href": "07-mlr.html#footnotes",
    "title": "7  Multiple linear regression",
    "section": "",
    "text": "The distribution of age is unimodal and skewed right, so the center is best described by the median 7.1 months and spread by the IQR of 12.8 months. There does not appear to be any outliers for age in the data.↩︎\nIt is possible to fit models such that \\(p &gt; n\\). These models are beyond the scope of this text, so we refer the reader to Chapter 6 of James et al. (2021) for further reading.↩︎\nFor each additional lemur in the litter in which a young lemur was born, the weight is expected to be lower by 19.607 grams, holding taxon, sex, and age constant.↩︎\nThe baseline level for sex is “F” (Female). The average weight for male lemurs is about 88.838 grams less than the average weight for female lemurs, holding taxon, age, and litter size constant. The intercept for female lemurs is 43.844 .The intercept for male lemurs is -44.994 (43.844 + -88.838 ).↩︎\nIt is not, because it is not possible for a lemur to be born into a litter of size 0. The litter size must be at least 1.↩︎\n\\(\\widehat{\\text{weight\\_g}} = 43.844 + 136.903 \\times 14 - 88.838 \\times 0 + 471.715 \\times 0 + 1037.323 \\times 0 - 19.607 \\times 2 = 1921.272\\)$↩︎\nWe only centered or standardized the quantitative predictors; we have not changed the categorical predictors. Recall that the indicator variables shift the intercept, and thus this did not change though the relationship between the response and the quantitative predictors may have changed.↩︎\nThe variable litter_size , because it has the coefficient with the smallest magnitude, |18.128 |= 18.128 .↩︎\nLemurs in the PCOQ taxon appear to grow at the fastest rate, because its line in Figure 7.8 (green dotted line) is the steepest.↩︎\nThe slope of age for lemurs in the VRUB taxon is 127.931 (78.579 + 49.352). This means that among VRUB lemurs, for each additional month in age, the weight is expected to be greater by 127.931 grams, on average, holding sex and litter size constant.↩︎\nThe interaction between sex and taxon would also tell us if the relationship between taxon and weight differs by sex. We have decided to interpret the interaction effect in the other direction given it is more practically meaningful to understand differences between sexes within each taxon versus the other way around.↩︎\nThe overall effect of sexM for lemurs in the VRUB taxon is -83.971 (169.238 + -253.209 ). The overall effect of sexM for lemurs in the ERUF taxon is 169.238↩︎\nThe overall effect of litter size for lemurs who are 20 months old is 66.522 . This means that for lemurs who are 20 months old, for each additional sibling in their litter, their weight is expected to be less by 140.616 (77.424 + 20 \\(\\times\\) -10.902 ), on average, holding sex and taxon constant.↩︎",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html",
    "href": "08-mlr-inference.html",
    "title": "8  Inference for multiple linear regression",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#learning-goals",
    "href": "08-mlr-inference.html#learning-goals",
    "title": "8  Inference for multiple linear regression",
    "section": "",
    "text": "Explain how statistical inference is used to draw conclusions about coefficients in multiple linear regression\nConduct inference using simulation-based methods\nConduct inference using mathematical models based on the Central Limit Theorem\nInterpret results from statistical inference in the context of the data\nEvaluate model conditions and diagnostics",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#sec-mlr-inf-intro",
    "href": "08-mlr-inference.html#sec-mlr-inf-intro",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.1 Introduction: Inference for lemurs",
    "text": "8.1 Introduction: Inference for lemurs\nIn Chapter 7, we introduced the data set lemurs-sample-young.csv that includes the weight and other characteristics of young lemurs (24 months old or younger) living in the Duke Lemur Center. We will continue analyzing the data set here as we use statistical inference to draw conclusions about the relationships between various characteristics and weight. The analysis focuses on the variables below. See Section 7.1 for exploratory data analysis.\n\n\nweight: Weight of the lemur (in grams)\n\ntaxon : Code made as a combination of the lemur’s genus and species. Note that the genus is a broader categorization that includes lemurs from multiple species.  This analysis focuses on the following taxon:\n\nERUF: Eulemur rufus, commonly known as Red-fronted brown lemur\nPCOQ: Propithecus coquereli, commonly known as Coquerel’s sifaka\nVRUB: Varecia rubra, commonly known as Red ruffed lemur\n\n\n\nsex : Sex of lemur (M: Male, F: Female)\n\nage: Age of lemur when weight was recorded (in months)\n\nlitter_size: Total number of lemurs the litter the lemur was born into\n\nThe multiple linear regression model using taxon, age, sex, and litter size, to explain variability in the weight of young lemurs is in Table 8.1.\n\n\n\nTable 8.1: Model to predict weight of young lemurs using various characteristics\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n24.2\n93.63\n0.259\n0.796\n\n\nage\n136.9\n4.86\n28.182\n0.000\n\n\nsexM\n-88.8\n68.00\n-1.306\n0.193\n\n\ntaxonPCOQ\n471.7\n95.43\n4.943\n0.000\n\n\ntaxonVRUB\n1037.3\n133.91\n7.747\n0.000\n\n\nlitter_size\n-19.6\n65.80\n-0.298\n0.766\n\n\n\n\n\n\n\n\nThe interpretations of the coefficients for the model in Table 8.1 describe the exact relationship between the response and each predictor for the 252 young lemurs in the sample (see Section 7.4). In practice, the analysis objective is not to just understand the observations in our sample, but instead use the sample to draw conclusions about a broader population. We do so using statistical inference.\n\nUse statistical inference to answer the following:\n\nIs age a useful predictor of weight, after accounting for sex, taxon, and number of lemurs in the litter?\nAfter taking into account sex, taxon, and number of lemurs in the litter, about how much does a young lemur’s weight increase, on average, as they get older?\n\n\n\nFor the remainder of the chapter, we will build on the concepts introduced in Chapter 5 as we discuss statistical inference for multiple linear regression. We will also build on the model conditions and diagnostics from Chapter 6 and show how they apply in the context of multiple linear regression.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#recap-overview-of-statistical-inference",
    "href": "08-mlr-inference.html#recap-overview-of-statistical-inference",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.2 Recap: Overview of statistical inference",
    "text": "8.2 Recap: Overview of statistical inference\nHere we will do a brief review of the general ideas and methods for statistical inference. A more detailed discussion of these ideas is in Chapter 5.\nThe objective of statistical inference to use the sample data to draw conclusions about the population of interest. In the context of linear regression, this means using the sample data to draw conclusions about the relationship between the response variable and the predictors. This done through one of two inferential procedures:\n\nHypothesis tests: Test a claim about a population coefficient\nConfidence intervals: A range of values that the population-level coefficient may reasonably take\n\n\nRecall the inference questions introduced in Section 8.1:\n\nIs age a useful predictor of weight, after accounting for sex, taxon, and number of lemurs in the litter?\nAfter taking into account sex, taxon, and number of lemurs in the litter, about how much does a young lemur’s weight increase, on average, as they get older?\n\nCan we answer the question using a confidence interval, hypothesis test, or both?1\n\nWe are using the results from a single sample to draw conclusions about the population. If we were to take another random sample of 252 young lemurs and fit a model using sex, age, taxon, and litter_size to explain variability in weight, we would expect the model coefficient of age, for example, to be similar but not exactly the same as the coefficient in Table 8.1. If we repeat this process many times, we will have a distribution of the coefficient of age, and we can use this distribution to understand the sample-to-sample variability in the estimated coefficients of age, called the sampling variability . This is true for the other predictors as well.\nIt is not feasible (or sometimes even possible) to take repeated samples from the population. Thus, we rely on methods to quantify the sampling variability in the estimated coefficients. This variability can be quantified in two ways:\n\nSimulation-based methods: Quantify the sampling variability by generating a sampling distribution directly from the data\nTheory-based methods: Quantify the sampling variability using mathematical models based on the Central Limit Theorem\n\nThe concepts and general steps to conduct inference is the same in multiple linear regression as in simple linear regression. Therefore, much of what we see in the following sections will overlap with Chapter 5. The primary difference is that now we are drawing conclusions about a model coefficient for a given predictor, after taking into account the other predictors in the model. Thus, we expect the sampling variability for a coefficient to be different in the multiple linear regression model compared to the simple linear regression model.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#sec-mlr-sim-inf",
    "href": "08-mlr-inference.html#sec-mlr-sim-inf",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.3 Simulation-based inference",
    "text": "8.3 Simulation-based inference\nAs introduced in Section 5.4, with simulation-based inference, we use the sample data, instead of mathematical equations, to generate the relevant distributions for confidence intervals and hypothesis tests. This approach can be preferred at times, because it does not rely on as strict adherence to the LINE model conditions (Chapter 6), particularly in regards to the equal variance condition. These methods can be computationally intensive, however, so it is up to the data scientist to weigh these advantages and disadvantages when deciding whether to use this approach.\nWe use two simulation procedures to conduct statistical inference for a population coefficient - bootstrapping for confidence intervals and permutation for hypothesis tests. Here we will show these methods in the context of multiple linear regression. See Section 5.4 and Section 5.6 for a detailed introduction to these procedures.\n\n8.3.1 Bootstrap confidence intervals\nLet \\(\\beta_j\\) be the population coefficient for the predictor \\(X_j\\). The estimated coefficient \\(\\hat{\\beta}_j\\) is the “best guess” for the value of \\(\\beta_j\\); however, we do not expect that \\(\\hat{\\beta}_j\\) is exactly equal to \\(\\beta_j\\). In fact, if we take another sample of the same size and fit a model of the same form, we will likely get a different (and hopefully close) estimate of the coefficient. Therefore, instead of relying solely on \\(\\hat{\\beta}_j\\) to tell us something about the true population coefficient, we compute a confidence interval, a range of values that is reasonable for \\(\\beta_j\\) to take based on our data  This confidence interval is found based on the sampling distribution constructed by bootstrap sampling.\nLet’s use a bootstrap confidence interval to answer the following question posed in Section 8.1:\n\nAfter taking into account sex, taxon, and number of lemurs in the litter, about how much does a young lemur’s weight increase, on average, as they get older?\n\nThe question focuses on the predictor age, so we are computing the confidence interval for the coefficient of age, \\(\\beta_{age}\\). Note that the question states “after taking into account sex, taxon, and number of lemurs.” This means that we want to compute the confidence interval for the coefficient of age given a model of the form in Table 8.1.\nBootstrapping is the process of sampling, with replacement, to generate a new sample the same size as the observed data. For the analysis of weights for young lemurs, each bootstrap sample will have 252 observations. When obtaining the bootstrap sample, each observation remains in tact. This means the values of the response and predictor variable for an individual lemur in the bootstrap sample are the same as the combination of values for a lemur in the original sample.\nWe will use 1000 bootstrap samples in this analysis.  Table 8.2 shows five observations from the first and last bootstrap samples.  The column replicate indicates the bootstrap sample.\n\n\n\nTable 8.2: Five observations from the first and last bootstrap samples\n\n\n\n\nreplicate\nweight\nage\nsex\ntaxon\nlitter_size\n\n\n\n1\n3340\n20.52\nM\nVRUB\n1\n\n\n1\n1580\n11.54\nF\nERUF\n0\n\n\n1\n3620\n17.49\nF\nVRUB\n2\n\n\n1\n3390\n23.64\nF\nPCOQ\n0\n\n\n1\n212\n0.82\nF\nPCOQ\n0\n\n\n1000\n720\n1.84\nM\nVRUB\n1\n\n\n1000\n538\n2.20\nM\nPCOQ\n0\n\n\n1000\n712\n3.19\nM\nPCOQ\n0\n\n\n1000\n350\n2.30\nM\nPCOQ\n0\n\n\n1000\n3300\n18.35\nF\nVRUB\n2\n\n\n\n\n\n\n\n\nUsing bootstrapping, we now have 1000 “new” samples. Next, we fit a model using each bootstrap sample. Table 8.3 shows the model coefficients for the first five and last five bootstrap samples. \n\n\n\n\nTable 8.3: Coefficients from the first five and last five bootstrap samples\n\n\n\n\nreplicate\nintercept\nage\nsexM\ntaxonPCOQ\ntaxonVRUB\nlitter_size\n\n\n\n1\n80.73\n142\n-113.6\n451\n974\n-88.896\n\n\n2\n36.99\n138\n-131.6\n489\n891\n19.288\n\n\n3\n130.63\n136\n-178.6\n376\n960\n-4.860\n\n\n4\n2.22\n138\n-52.6\n452\n1051\n-33.453\n\n\n5\n108.07\n148\n-198.1\n361\n822\n0.405\n\n\n996\n102.65\n140\n-65.6\n368\n904\n19.692\n\n\n997\n22.83\n139\n-114.6\n475\n1100\n-27.400\n\n\n998\n-29.93\n142\n-41.0\n420\n980\n7.807\n\n\n999\n-159.85\n149\n-67.5\n629\n1119\n-29.045\n\n\n1000\n86.17\n137\n-121.8\n423\n747\n140.139\n\n\n\n\n\n\n\n\nWe include all the original predictors when we fit the model to each bootstrap sample, so we get coefficient estimates for each predictor. Notice within each column that the estimated coefficients for a given predictor are similar but not exactly the same for each bootstrap sample. At this point, we are interested in conducting inference for age, so we will focus on the estimated coefficients for that variable and ignore the others for now.\n\n\n\n\n\n\n\nFigure 8.1: Bootstrap sampling distribution for age\n\n\n\n\nFigure 8.1 is the bootstrap sampling distribution for \\(\\hat{\\beta}_{age}\\). The mean of the distribution is 136.809 and the standard deviation is 5.97. The mean of the bootstrap distribution is very close to the estimated coefficient of age in the original model in Table 8.1. These values will not be exactly equal, because the bootstrap distribution is constructed from a simulation-based process that itself involves sampling. Given a confidence level \\(C\\), the \\(C\\%\\) confidence interval is the middle \\(C\\%\\) of the bootstrap distribution. \\(C\\) is generally set between 90 and 99, to balance accuracy and precision (see Section 5.4), with 95 being a commonly used default value.\n\nExplain what 5.97 means in the context of the analysis.2\n\n\n\n\n\n\n\n\nFigure 8.2: Bootstrap sampling distribution for age with 95% confidence interval\n\n\n\n\nFigure 8.2 shows the upper and lower bounds for a 95% confidence interval on the bootstrap sampling distribution. The 95% confidence interval for the \\(\\beta_{age}\\), the coefficient of age is 125.74 to 149.29. Similar to the confidence interval in simple linear regression, we are 95% confident that this interval contains the true coefficient for age. Let’s interpret what this means in terms of the relationship between age and weight for young lemurs.\n\nWe are 95% confident that the weight of young lemurs increase between 125.74 and 149.29 grams, on average, for each additional month older, holding taxon, sex, and number of lemurs in the litter constant.\n\nAs before, the “confidence” referenced in this statement is the long-run confidence derived from the statistical process used to compute the interval. This means that if we repeat the process outlined in this section thousands of times, and thus have thousands of bootstrap distributions and confidence intervals, then about 95% of the confidence intervals would contain the true coefficient \\(\\beta_{age}\\). Because we don’t know what \\(\\beta_{age}\\) is (if we knew \\(\\beta_{age}\\), we wouldn’t need inference!), we cannot state for certain whether the interval we computed is one of the 95% of intervals that contains the true coefficient or the 5% that miss the mark. Thus, we state in the interpretation that we are “95% confident”.\n\nThe 90% bootstrap confidence interval for taxonPCOQ based on the model in Table 8.1 is 314.442 to 630.842 . Interpret the interval in the context of the data.3\n\n\n\n8.3.2 Permutation tests\nSometimes we wish to evaluate a claim made about the relationship between the response variable and one of the predictors, \\(X_j\\) using a hypothesis test. When conducting hypothesis tests to evaluate such a claim, the null hypothesis is the “baseline” condition of no linear relationship between the response and the predictor variable (\\(H_0: \\beta_j = 0\\)) , and the alternative hypothesis is that there is a linear relationship (\\(H_a: \\beta_j \\neq 0\\)). Hypothesis tests can be used to evaluate other claims as well; however, we will focus on evaluating whether there is a linear relationship, because this is the claim being evaluated in output of most statistical software. See Section 5.5 for an introduction to hypothesis tests.\nSection 5.6 provides an introduction more specifically to permutation tests, so here we will show how to conduct the test for multiple linear regression and answer the question posed in Section 8.1 :\n\nIs age a useful predictor of weight, after accounting for sex, taxon, and number of lemurs in the litter?\n\nWe begin by stating the hypotheses in the context of the data:\n\n\nNull: There is no linear relationship between age and weight after accounting for sex, taxon, and number of lemurs in the litter.\n\nAlternative: There is a linear relationship between age and weight after accounting for sex, taxon, and number of lemurs in the litter.\n\nThe hypotheses in mathematical notation are the following:\n\\[\nH_0: \\beta_{age} = 0 \\text{ vs. }H_a: \\beta_{age} \\neq 0\n\\]\nIn Section 5.5, we explained how hypothesis tests are conducted assuming null hypothesis is true, and the aim is to evaluate the strength of the evidence against the null hypothesis. Therefore, we need to obtain the sampling distribution of \\(\\hat{\\beta}_{age}\\) under the assumption that there is no linear relationship between age and weight, after accounting for the other predictors in the model. This sampling distribution is the null distribution, and we construct it using permutation sampling.\nPermutation sampling is the process of shuffling one or more columns of data, to get many new combinations of the data, i.e., “new” samples. In this case, because we are conducting inference for a single predictor age, we will create new samples by shuffling the values of age, so that they are randomly paired with values of weight. We do not shuffle the columns of the other predictors, because we are evaluating the relationship between age and weight after accounting for the relationships between weight and the predictors sex, taxon, and litter_size. By shuffling the columns of age, we are simulating the null condition of no linear relationship between age and weight.\n\n\n\nTable 8.4: First five permutations for an individual lemur\n\n\n\n\n\nweight\nage\nsex\ntaxon\nlitter_size\n\n\n\nOriginal Sample\n410\n2.27\nF\nERUF\n0\n\n\nPermutation 1\n410\n20.52\nF\nERUF\n0\n\n\nPermutation 2\n410\n13.51\nF\nERUF\n0\n\n\nPermutation 3\n410\n16.41\nF\nERUF\n0\n\n\nPermutation 4\n410\n12.20\nF\nERUF\n0\n\n\nPermutation 5\n410\n22.06\nF\nERUF\n0\n\n\n\n\n\n\n\n\n\nTable 8.4 shows the values from the original data and values from the first five permutations for an individual lemur. In the table, we see the values for weight, sex, taxon, and litter_size are the same for each permutation. The values of age are randomly shuffled, and there is no linear relationship (or any relationship) between age and weight in the permutation samples.\nLike bootstrapping, each permutation sample is the same size as the original data. In contrast to bootstrapping, permutation sampling is merely shuffling the values of a column of data rather than sampling complete observations with replacement. We will use 1000 permutation samples for this analysis. Like bootstrapping, for each permutation sample, we fit the linear regression model and use \\(\\hat{\\beta}_{age}\\), the estimated coefficient of age from each permutation sample to create the null distribution.\n\n\nWhat is the approximate center of the null distribution of age created from the permutation samples?\nHow does the standard deviation of the null distribution compare to the standard deviation of the bootstrap distribution?4\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Null distribution for age from 1000 permutation samples\n\n\n\n\n\nFigure 8.3 shows the null distribution with the value of \\(\\hat{\\beta}_{age}\\) observed in the original data marked with the red dotted line. The null distribution is centered around 0, the hypothesized value. The coefficient estimated from the observed data is very far away from the center of the distribution, so the data appears to provide strong evidence against the null hypothesis.\nThe evidence against the null hypothesis is not always so visually clear, so we compute a p-value to provide a measure for evaluating the evidence against the null hypothesis. Recall that the p-value is the probability of observing results at least as extreme as the results observed from the data, given the null hypothesis is true. In this instance, this means the probability of getting an estimated coefficient of 136.903 or more extreme in a sample of 252 lemurs, given the null hypothesis is true. Because the alternative hypothesis is \\(\\beta_{age} \\neq 0\\), “more extreme” is greater than 136.903 and less than -136.903 .\nBased on the simulation, the p-value is 0. This means in 1000 permutation samples drawn under the assumption the null hypothesis is true, we did not compute an estimated coefficient for age equal to 136.903 or more extreme. We compare this p-value to a decision-making threshold \\(\\alpha\\) (see Section 5.6.4) to draw the conclusion. The p-value of 0 is less than \\(\\alpha\\) for any chosen threshold, so we reject the null hypothesis. The data provide strong evidence in favor of the alternative hypothesis that there is a linear relationship between age and weight after accounting for sex, taxon, and number of lemurs in the litter.\n\nThough the p-value from the simulation-based hypothesis test is equal to 0, the true p-value is very close to but not exactly 0 (but it is still very small!). R will produce a warning like the one below to highlight this.\n\n\n\n\n\nInfer warning about reporting p-value of 0.\n\nWe can report such p-values as “approximately 0” to make clear to the reader that this is an approximation and not an exact result. We can also increase the number of iterations to get a closer approximation of the p-value or use theory-based methods from Section 8.4 to get the exact p-value.\n\nHere we used permutation sampling to construct the null distribution for the hypothesis test to evaluate a claim about the relationship between age and weight for young lemurs. Because we conducted a two-sided hypothesis test, we can use the bootstrap confidence interval from Section 8.3.1 to evaluate this claim as well. The \\(C\\%\\) confidence interval corresponds to a test with a decision-making threshold of \\(\\alpha = (1-\\frac{C}{100})\\). See Section 5.7 for more detail about the connection between hypothesis test and confidence intervals. \n\nThe 95% bootstrap confidence interval for \\(\\beta_{age}\\) is 125.74 to 149.29.\n\nThis interval corresponds to a hypothesis test with what \\(\\alpha\\) -level?\nIs this interval consistent with the conclusion that there is evidence of a linear relationship between age and weight, after accounting for sex, taxon, and number of lemurs in the litter? Explain. 5",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#sec-mlr-theory-inf",
    "href": "08-mlr-inference.html#sec-mlr-theory-inf",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.4 Theory-based inference",
    "text": "8.4 Theory-based inference\nThe other approach for inference is using theory-based methods based on mathematical models and the Central Limit Theorem. Theory-based methods have formulas to quantify the sampling variability in the estimated coefficient and uses distributions to compute exact confidence intervals and p-values. Because these methods rely on formulas rather than resampling methods, they are very computationally efficient. Results from theory-based inferential methods are displayed in the model output from most statistical software. For example, you can see the inferential results we’ll discuss in the model output in Table 8.1.\nTheory-based inference for coefficients in a multiple linear regression model is very similar to inference in simple linear regression. Therefore, this section will provide a brief description of these methods, show how they apply to answer the questions in Section 8.1, and highlight differences from simple linear regression. See Section 5.8 for a thorough introduction to these methods. The mathematical details utilize linear algebra and are found in Section A.5 and Section A.6.\n\n8.4.1 Foundations for inference\nWhen doing multiple linear regression, we assume a model structure of the from in Equation 8.1.\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{8.1}\\]\nThe equation written in terms of the distribution of \\(Y\\) given a combination of predictors \\(X_1, X_2, \\ldots, X_p\\) in Equation 8.2.\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p, \\sigma^2_{\\epsilon})\n\\tag{8.2}\\]\nBy Equation 8.2, the distribution of the response variable \\(Y\\) given a combination of predictors \\(X_1, X_2, \\ldots, X_p\\) is normally distributed, centered at \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\) with variance \\(\\sigma^2_{\\epsilon}\\). Based on this distribution, the following assumptions are made when we conduct multiple linear regression:\n\nThe distribution of the response \\(Y\\) is normal for a given combination of the predictors\\(X_1, X_2, \\ldots, X_p\\).\nThe expected value (mean) of the distribution of \\(Y\\) given \\(X_1, X_2, \\ldots, X_p\\) is \\(\\beta_0 + \\beta_1 X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\). There is a linear relationship between the response and the predictor variables.\nThe variance of the distribution of \\(Y\\) given \\(X_1, X_2, \\ldots, X_p\\) is \\(\\sigma^2_{\\epsilon}\\). This variance is equal for all combination of predictors \\(X_1, X_2, \\ldots, X_p\\) and does not depend on the predictors.\nThe error terms \\(\\epsilon\\) are independent of one another. This also means the values of the response variable, and the observations more generally, are independent of one another.\n\nNotice that these assumptions are generally the ones for simple linear regression in Section 5.3. The difference is that the assumptions about \\(Y\\) are made for a combination of predictors rather than a single predictor.\nIn Chapter 7, we showed how to compute \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_p\\), the estimated model coefficients. The remaining parameter \\(\\sigma_{\\epsilon}\\), called the regression standard error, is the variability of the observations about the regression line. The distance between the observed values and the regression line (the predicted value of \\(Y\\) for a given combination of predictors) is the residual. Equation 8.3 shows how we use the residuals to estimate the regression standard error.\n\\[\n\\hat{\\sigma}_{\\epsilon} = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n - p -1 }} = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - p -1 }}\n\\tag{8.3}\\]\nThis is very similar to Equation 5.8, the equation to estimate the regression standard error in simple linear regression. The difference is the denominator, \\(n - p - 1\\), where \\(p\\) is the number of predictor terms. The value \\(n - p - 1\\) is the degrees of freedom, the number of observations available to understand variability about the regression line. This stems from the fact that we “use” \\(p+1\\) observations to estimate the model coefficients. In simple linear regression, there is one predictor, so \\(p = 1\\), and the degrees of freedom are \\(n - 2\\) as shown in Section 5.8.2. Let’s compute the degrees of freedom for the regression standard error associated with the model in Table 8.1. There are \\(n =\\) 252 observations in the data and \\(p = 5\\) terms for predictors in the model. (Note that even though taxon is a single predictor, we need to account for the fact there are two terms for taxon estimated in the model). The degrees of freedom are\n\\[\ndf = 252 - (5 + 1) = 252 - 5 - 1 = 246\n\\]\nThus, there are 246 degrees of freedom, i.e., observations available to understand variability about the regression line for the lemurs analysis.\nThe goal is to conduct inference for a model coefficient \\(\\beta_j\\), so we ultimately need to estimate the sampling variability in the estimated coefficients \\(\\hat{\\beta}_j\\). This variability is measured by \\(SE_{\\hat{\\beta}_j}\\), the standard error of \\(\\hat{\\beta}_j\\). The formula to compute \\(SE_{\\hat{\\beta}_j}\\) in Equation 8.4 is more complex than in the case of simple linear regression, because we need to understand the sampling variability of \\(\\hat{\\beta}_j\\) in the context of a regression model that includes other predictors.\n\\[\nSE_{\\hat{\\beta}_j} = j^{th} \\text{diagonal element of }\\hat{\\sigma}_{\\epsilon}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\n\\tag{8.4}\\]\nMathematical details for Equation 8.4 are in Section A.6. For now, we’ll focus on a conceptual understanding about why the standard error computed here may be different than the standard error for the same predictor variable in the case of simple linear regression. To illustrate this, let’s look at the standard error for age in a simple linear regression model and in the model in Table 8.1. The coefficient estimates and standard errors are printed side-by-side in Table 8.5.\n\n\nTable 8.5: Comparison of standard error for age in simple and multiple linear regression modelss\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n(Intercept)\n555\n67.31\n\n\nage\n142\n5.88\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n(Intercept)\n24.2\n93.63\n\n\nage\n136.9\n4.86\n\n\nsexM\n-88.8\n68.00\n\n\ntaxonPCOQ\n471.7\n95.43\n\n\ntaxonVRUB\n1037.3\n133.91\n\n\nlitter_size\n-19.6\n65.80\n\n\n\n\n\n\n\n\nThe estimated standard error for age in the simple linear regression model is 5.883 compared to 4.858 in the multiple linear regression model. In the case of multiple linear regression, the other predictors in the model are accounted for in two ways when computing \\(SE_{\\hat{\\beta}{age}}\\) . First the other predictors are accounted for when computing \\(\\hat{\\sigma}_\\epsilon\\), as they are used to compute the \\(\\hat{y}_i\\) in the Equation 8.3. Second, the other predictors are accounted for in \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\), as this matrix quantifies any relationship between the predictor variables (see Section A.6 for details). Thus, based on the models in Table 8.5, there is less sampling variability in \\(\\hat{\\beta}_{age}\\) after we account for the other potential predictors of weight.\nIn addition to estimating the sampling variability for model coefficients, by the Central Limit Theorem, we know the full sampling distribution of the model coefficient \\(\\hat{\\beta}_j\\) is normally distributed, with a mean at the true population coefficient \\(\\beta_j\\) and variance \\(SE_{\\hat{\\beta}_j}^2\\), the standard error squared as shown in Equation 8.5. We use this distribution as the basis for conducting hypothesis tests and computing confidence intervals.\n\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, SE_{\\hat{\\beta}_j}^2)\n\\tag{8.5}\\]\n\n\n8.4.2 Hypothesis tests\nThe hypothesis test in multiple linear regression follows the same steps as simple linear regression in Section 5.6.1:\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate a p-value.\nDraw a conclusion.\n\nUsing these steps, we’ll conduct a hypothesis test with decision-making threshold \\(\\alpha = 0.05\\), to answer the question from Section 8.1:\n\nIs age a useful predictor of weight, after accounting for sex, taxon, and number of lemurs in the litter?\n\nStep 1: State the null and alternative hypotheses.\nThe null and alternative hypotheses are the same for theory-based inference as they are for simulation-based inference. Thus, the null and alternative hypotheses are the following:\n\n\nNull: There is no linear relationship between age and weight after accounting for sex, taxon, and number of lemurs in the litter.\n\nAlternative: There is a linear relationship between age and weight after accounting for sex, taxon, and number of lemurs in the litter.\n\nThese hypothesis in mathematical notation are the following:\n\\[\nH_0: \\beta_{age} = 0 \\text{ vs. }H_a: \\beta_{age} \\neq 0\n\\]\nStep 2: Calculate a test statistic.\nWe conduct hypothesis testing assuming the null hypothesis is true. Applying this to Equation 8.5, we assume that \\(\\hat{\\beta}_{age} \\sim N(0, SE_{\\hat{\\beta}_{age}})\\). Let’s use this to compute the test statistic, the number of standard errors \\(\\hat{\\beta}_{age}\\), the coefficient estimated from the data, is from the hypothesized center of the distribution 0.\nThe test statistic for \\(\\hat{\\beta}_{age}\\) is\n\\[\nT = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}} = \\frac{136.903 - 0}{4.858} = 28.181\n\\]\nThis means that given the null hypothesis is true (\\(\\beta_{age} = 0\\)) , the coefficient estimated from the model 136.903 is about 28.181 standard errors above the mean of the distribution.\n\nThe test statistic is 28.181. Do you think this provides evidence in support of or against the null hypothesis?6\n\nStep 3: Calculate a p-value.\nSometimes the test statistic has a magnitude that is so small or so large that we can easily use it to draw conclusions about the test. There are many times, however, it is unclear what the conclusion should be based on the test statistic alone, so we compute a p-value and compare it to the decision-making threshold \\(\\alpha\\) to make a conclusion.\nThe definition of the p-value is similar for theory-based inference as with simulation-based inference. The key difference is that we will compare the test statistic, instead of the estimated coefficient, against a distribution. Thus, the p-value is the probability of observing a test statistic in the distribution at least as extreme as the observed test statistic, assuming the null hypothesis is true.\nUnder the null hypothesis, the test statistic follows a \\(t\\) distribution with \\(n- p-1\\) degrees of freedom, \\(T \\sim t_{n-p-1}\\) . The \\(t\\) distribution is used, instead of the standard normal distribution, to account for the extra variability that results from using the estimated regression standard error \\(\\hat{\\sigma}_\\epsilon\\) to compute the standard error of the coefficient in Equation 8.4 (see Section 5.6.1 for more detail about \\(t\\) distribution). Because the alternative hypothesis is not equal to, we conduct a two-sided test and compute the p-value as \\(P(|t| &gt; |T|) = P(t &lt; -|T|) + P(t &gt; |T|)\\), the probability of obtaining a test statistic with magnitude greater than the test statistic \\(T\\).\nIn the hypothesis test for \\(\\beta_{age}\\), the p-value is computed as\n\\[\nP(|t| &gt; |28.181|) = P(t &lt; -28.181) + P(t &gt; 28.181)\n\\]\nusing a \\(t\\) distribution with 246 degrees of freedom. Using statistical software, the p-value is equal to 5.563^{-79} \\(\\approx 0\\). Now we have an exact p-value that was estimated to be 0 using the permutation test in Section 8.3.2.\nStep 4: Draw a conclusion.\nThe final step is to make a conclusion by comparing the p-value to the predefined decision-making threshold, \\(\\alpha\\). If the p-value is less than \\(\\alpha,\\) the evidence against the null is sufficiently strong, so we reject the null hypothesis and conclude the alternative. Otherwise, the evidence against the null hypothesis is not sufficiently strong, so we fail to reject the null hypothesis.\nWhen writing the conclusion, we indicate whether to reject or fail to reject the null hypothesis then state what that means in the context of the data. Thus, for the hypothesis test for age, the conclusion is as follows:\n\nThe p-value is less than \\(\\alpha = 0.05\\), so we reject the null hypothesis. The data provide sufficient evidence of a linear relationship between age and weight, after accounting for sex, taxon, and number of lemurs in the litter.\n\n\n8.4.3 Confidence intervals\nIn Section 8.3.1, we used bootstrapping to simulate the sampling distribution of \\(\\hat{\\beta}_j\\), and marked the bounds of the middle \\(C\\%\\) of the distribution to make the \\(C\\%\\) confidence interval. Here, we will use the mathematical results about the distribution of \\(\\hat{\\beta}_j\\) and compute the confidence intervals using an equation of the form\n\\[\n\\hat{\\beta}_j \\pm t^*_{n-p-1} \\times SE_{\\hat{\\beta}_j}\n\\tag{8.6}\\]\nwhere \\(t^*_{n-p-1}\\) is the critical value marking the middle \\(C\\%\\) of the \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom. Let’s construct a 95% confidence interval for age to answer the question posed in Section 8.1 :\n\nAfter taking into account sex, taxon, and number of lemurs in the litter, about how much does a young lemur’s weight increase, on average, as they get older?\n\nWe have previously computed \\(\\hat{\\beta}_{age}\\) and \\(SE_{\\hat{\\beta}_{age}}\\), so we just need to compute the critical value \\(t^*_{n-p-1}\\) using statistical software. This critical value is the point on the \\(t\\) distribution with \\(n-p-1\\) degrees of freedom such that 95% of the distribution (area under the curve) is between \\(-t^*_{n-p-1}\\) and \\(t^*_{n-p-1}\\). In this analysis, the critical value to construct the 95% confidence interval for \\(\\beta_{age}\\) using a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom is 1.651. \nNow we have all the components needed to compute the confidence interval using Equation 8.6. By using the formula, we translate the interval from standardized values to the original units of the model coefficient by rescaling (multiplying) by \\(SE_{\\hat{\\beta}_j}\\) and shift by \\(\\hat{\\beta}_j\\). Thus to get the bounds of the confidence interval from \\(\\pm 1.651\\) to values in terms of grams of weight per month older, we have\n\\[\n136.903 \\pm 1.651 \\times 4.858\n= [128.882, 144.924]\n\\]\n\n\nThus, we are 95% confident that the interval 128.882 to 144.924 contains \\(\\beta_{age}\\), the true slope for age.7 More specifically in terms of the question, this means we are 95% confident that after accounting for sex, taxon, and number of lemurs in the litter, the weight of young lemurs increases between 128.882 to 144.924 grams, on average, as the age increases by one month. Recall that the “confidence” referenced here is our confidence in the statistical methods. That if we were to repeat the process of collecting 252 observations, fitting the same model, and making a confidence interval for the coefficient of age, about 95% of those intervals will actually contain the population coefficient. We cannot say for certain, however, whether this individual interval does or does not contain the true population coefficient for age.\n\nConfidence intervals typically have confidence levels between 90% and 99%. Suppose you are considering three possible confidence levels: 90%, 95%, and 99%.\n\nWhich confidence level will you choose to get the most accurate interval?\nWhich confidence level will you choose to get the most precise interval?8",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#sec-mlr-conditions-diagnositics",
    "href": "08-mlr-inference.html#sec-mlr-conditions-diagnositics",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.5 Model conditions and diagnostics",
    "text": "8.5 Model conditions and diagnostics\nThe simulation- and theory-based inference methods rely on a set of assumptions about the relationship between the response and predictor variables in the population (Section 8.4.1). We are unable to check whether these assumptions truly hold in the population, so we use the sample data to evaluate whether they hold in the data using model conditions. We also use model diagnostics to assess whether there are outliers in the data that are having an out-sized influence on the model results. These model conditions and diagnostics are the same the ones for simple linear regression (Chapter 6) with some additional considerations for the multiple predictor variables. Here we will show the conditions and diagnostics applied to the lemur model. See Chapter 6 for a detailed introduction to these topics.\n\n8.5.1 Model conditions\nThere are four model conditions that align with the assumptions outlined in Section 8.4. These conditions, commonly know by the mnemonic LINE, are the following:\n\n\nLinearity: There is a linear relationship between the response and predictor variables.\n\nIndependence: The residuals are independent of one another. \n\n\nNormality: The distribution of the residuals is approximately normal.\n\nEqual variance: The spread of the residuals is approximately equal for all predicted values.\n\nLinearity\nIn the context of multiple linear regression, the linearity condition means that the relationship between the response and the predictor variables can be adequately described using the linear regression model. When this is the case, the residuals \\(y_i - \\hat{y}_i\\) captures random variability that stems from the fact that there are factors other than the ones in the model that explain variability in weight and the natural random differences in weight we’d expect in a group of lemurs. To examine this, we use a plot of the residuals versus predicted values. Ideally, the residuals are randomly scattered, indicating the systematic relationship between the response and predictor variables has been capture by the model. The linearity condition is important for simulation-based and theory-based inference, so any violations in linearity should be addressed before conducting inference.\n\n\n\n\n\n\n\nFigure 8.4: Residuals versus predicted for lemurs model\n\n\n\n\nBased on Figure 8.4, the linearity condition is satisfied. The scatterplot shows no discernible pattern and the points are randomly scattered around \\(\\text{residual } = 0\\). There is one point that stands out as an outlier with a large magnitude residual around 3500 grams. We will use model diagnostics to determine whether this outlier is, in fact, an influential point.\nWhen the plot of residuals versus fitted values is randomly scattered as the one in this analysis, it is often sufficient to stop there when evaluating the linearity condition. If the plot of residuals versus predicted values shows some discernible pattern (for example, a parabola), we need to more closely examine the residuals versus each predictor variable to determine which variable has a non-linear relationship with the response. We may also choose to evaluate the residuals versus each predictor for more information about how well the model captures the underlying relationship with each predictor.\n\n\n\n\n\n\n\n\n\n(a) Residuals versus age\n\n\n\n\n\n\n\n\n\n(b) Residuals versus taxon\n\n\n\n\n\n\n\n\n\n\n\n(c) Residuals versus sex\n\n\n\n\n\n\n\n\n\n(d) Residuals versus lemurs in litter\n\n\n\n\n\n\nFigure 8.5: Residuals verus each predictor for lemurs model\n\n\nFigure 8.5 shows plots of the residuals versus each of the four predictor variables. We use the same criteria when evaluating the plots of residuals versus quantitative predictors. For example, the plot of residuals versus age in Figure 8.5 (a) shows there may be a non-linear relationships between age and weight. This is the same non-linearity we observed in the EDA in Section 7.1. Thus, we may consider using a transformed version of age in the model that more accurately represents the change in weight as age increases (see Chapter 9). We can then compare the model with the transformed age to the current model to determine which one is an overall better fit for the data. We discuss model comparison and selection in Chapter 10.\nWe use boxplots to examine the relationship between the response variable and categorical predictors. We expect the boxplots to be mostly vertically centered around \\(\\text{residual }=0\\) for each category of a given predictor. For example, in the plot of taxon versus residual in Figure 8.5 (b), we see the center of each boxplot is close (though not exactly) equal to 0 and that in general the boxes for each taxon are spread around 0. One way to think about it is given a particular taxon, PCOQ for example, we cannot determine with almost any confidence whether the residual will be positive or negative. Therefore, the model sufficiently captures the relationship between taxon and weight. The same is true for sex in Figure 8.5 (c).\nThe residuals versus litter_size in Figure 8.5 (d) is randomly scattered, so the relationship between litter_size and weight is adequately captured in the model.\nIndependence\nThe next condition in LINE is independence. This condition is based on the assumption that the residuals are independent of one another; however, we often use information about the observations to evaluate this condition. As with simple linear regression, we use what we know about the subject matter and data collection process to evaluate whether it is reasonable to treat the observations as independent in the model. If the data have a time-ordered or spatial component, or were collected from subgroups not accounted for in the model, we can analyze the residuals to assess if there are violations in independence due to time or spatial correlation, or correlation with in a cluster.\nBased on what we know about the lemur data, we can conclude that the independence condition is satisfied, and the observations can reasonably be treated as independent of one another. This means that the residuals for one lemur do not provide information about the residuals of another lemur.\nThe independence condition is important for simulation-based and theory-based inference, so any violations in independence should be addressed before conducting inference.\nNormality\nThe next condition in LINE is normality. This condition is based on the assumption that the error terms are normally distributed, \\(\\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\\). We we use a plot of the residuals to evaluate this claim.\n\n\n\n\n\n\n\nFigure 8.6: Distribution of residuals with normal density curve\n\n\n\n\nThe distribution of the residuals with a normal curve applied is shown in Figure 8.6. We see the outlier with the large residual again in this plot. Focusing on the distribution without the outlier, the normal curve reasonably (though not perfectly) describes the distribution. Though not exactly, the distribution is approximately unimodal and symmetric, when the outlier is ignored. Therefore, we conclude the normality condition is satisfied.\nSimulation-based inference does not rely on any assumptions about the distribution of the residuals, so it is robust to violations in the normality condition. From the Central Limit Theorem, theory-based methods for inference on the model coefficient are also robust to violations in normality when the sample is large enough. Typically 30 is used as the threshold for a “large enough” sample, but this value should not be treated as absolute. Thus, the closer the sample size is to 30, the closer the distribution of residuals needs to fit a normal distribution for reliable inferential results.\nEqual variance\nThe final model condition in LINE is the equal variance condition. This condition comes from the assumption about the distribution of the error terms, \\(\\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\\), that the variance in this distribution, \\(\\sigma^2_{\\epsilon}\\) is the same for all observations regardless of the values of the predictors. Thus, the variability of the residuals should be the same for all predicted values (combinations of predictors).\n\n\n\n\n\n\n\nFigure 8.7: Residuals versus predicted with markers to check equal variance\n\n\n\n\nFigure 8.7 shows a plot of th residuals versus predicted values, similar to Figure 8.4. As we move along the \\(x\\)-axis, we are looking to see if the vertical spread is approximately equal for all observations. To help evaluate this, we have added vertical lines encompassing the majority of the data. Note as we move along the \\(x\\)-axis, the vertical spread fo the residuals is largely captured in by these vertical lines. Thus, the equal variance condition is satisfied. As with other conditions, we can ignore the high outlier when assessing the condition. Additionally, we are looking for noticeable violations in the conditions rather than perfect adherence when making the determination.\nSimulation-based inference does not rely on any assumptions about the distribution of the residuals, so it is robust to violations in the equal variance condition. The regression standard error \\(\\hat{\\sigma}_{\\epsilon}\\) is used to compute \\(SE_{\\hat{\\beta}_j}\\) in theory-based inference, so violations in the equal variance condition could lead to unreliable inference results. Therefore, any violations should be addressed before moving forward with inference. Chapter 9 introduces model transformations that can be used to address such violations.\n\n8.5.2 Model diagnostics\nAfter checking the model conditions, we evaluate model diagnostics to identify whether there are observations that have out-sized influence on the model. In Figure 8.4, we observed the outlying observation with the very large residual. Using model diagnostics, we can assess whether this observation is merely an outlier or, in fact, an influential point.\nCook’s distance\nWe begin by using Cook’s distance to determine whether there are influential points in the data.\n\n\n\n\n\n\n\nFigure 8.8: Cook’s distance for each observation in lemur analysis\n\n\n\n\nFigure 8.8 shows Cook’s distance for each observation. There is a horizontal line at \\(\\text{Cook's distance} = 1\\) marking the threshold for influential observations. In this analysis, all the values of Cook’s distance are well below 1 (the maximum value is 0.218), so there are no influential points in the data. Thus, even though we have identified the an outlier, this observations did not have an out-sized impact on the estimated model coefficients.\nUsing the modeling workflow outlined in Section 6.5, if we have no influential points, we do not need to examine the components of Cook’s distance, leverage and standardized residuals. If we do identify an influential point, however, we can use leverage and standardized residuals to better understand how a given observation differs from the general trend of the data.\nThough we did not find an influential point in this analysis, we will briefly discuss leverage and standardized residuals for multiple linear regression. See Section 6.4 for a introduction to these topics.\nLeverage\nIn simple linear regression, an observation’s leverage is a measure of how far its value of the predictor is from the mean value of the predictor in the data. That definition is expanded in multiple linear regression, where an observation’s leverage is how far its combination of predictors is from the average (typical) combination of predictors. Mathematical details for computing leverage in multiple linear regression are in Section A.4.\n\nSuppose we decide to fit a model using taxon, age, sex, and litter_size to predict the body length of young lemurs (instead of the weight)? How does the leverage for each observation compare in this model to the leverage in the model we’ve examined using these variables to predict weight?9\n\nAn observation has large leverage if its combination of predictors is far away from the typical combination in the data. We use a threshold of \\(\\frac{2(p+1)}{n}\\), where \\(p+1\\) is the number of terms in the model and \\(n\\) is the sample size, to determine whether an observation has large leverage.\n\n\n\n\n\n\n\nFigure 8.9: Leverage for observations in lemurs analysis\n\n\n\n\nFigure 8.9 shows the leverage for each observation in the data. The leverage threshold 0.048 is marked on the graph. Based on this graph, there are 13 observations with large leverage. These are observations who have a combination of predictors that is far from the typical combination in the data. Given the multidimensionality of the data, it is not as straight forward to identify what makes these observations have large leverage as it was in simple linear regression. We can examine the data further to see what (if anything) these large leverage observations have in common and how they differ from the typical combination of predictors in the data. From Cook’s distance computed earlier; however, we know that these observations are not influential.\nStandardized residuals\nStandardized residuals are used to identify observations that have large magnitude residuals and thus the model poorly fit. We already noticed such an observation in Figure 8.4, so why do we need standardized residuals? Recall that residuals are on the same scale and have the same units as the response variable. Thus, if we use the raw residuals, we would need to determine a new threshold for identifying outliers in each analysis. On the other hand, standardized residuals are on the same scale in every analysis (mean of 0 and standard deviation of 1) with no units, so we can use a universal threshold that applies to every analysis.\n\n\n\n\n\n\n\nFigure 8.10: Standardized residuals versus predicted values for lemur model\n\n\n\n\nFigure 8.10 shows the standardized residuals versus predicted values, with lines at -3 and 3, the thresholds for identifying outliers. There is one observation that is an outlier in the \\(Y\\) direction with a standardized residual of 6.442. Though this observation is a noticeable outlier in the model performance, it is not an influential point based on Cook’s distance computed earlier.\n\nConsider the outlier observation. The residual for this observation is 3380.42 and the standardized residual for this observation is 6.442.\n\nExplain what the value of the residual means in the context of the data.\nExplain what the value of the standardized residual means in the context of the data.10\n\n\n\nWe did not have any influential points in this analysis, but there will be times when do have such points in modeling. Section 6.4.5 discuses strategies for handling influential points in modeling.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#sec-mlr-multicollinearity",
    "href": "08-mlr-inference.html#sec-mlr-multicollinearity",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.6 Multicollinearity",
    "text": "8.6 Multicollinearity",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#inference-for-multiple-linear-regression-in-r",
    "href": "08-mlr-inference.html#inference-for-multiple-linear-regression-in-r",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.7 Inference for multiple linear regression in R",
    "text": "8.7 Inference for multiple linear regression in R\n\n8.7.1 Simulation-based inference in R\nAs with simulation-based inference for simple linear regression, we use the infer R package (Couch et al. 2021) for bootstrap confidence intervals and permutation tests. The code is shown below with brief explanations in the comments. It is described in Section 5.9.1 and Section 5.9.2.\nThe code to construct the 95% bootstrap confidence interval for the coefficient of age is shown below.\n\nset.seed(1234)\nniter = 1000\n\n# construct bootstrap distribution\nboot_dist &lt;- lemurs_sample_young |&gt; \n  specify(weight ~ age + sex + taxon + litter_size) |&gt; \n  generate(reps = niter, type = \"bootstrap\") |&gt; \n  fit() \n\n# compute 95% confidence interval for age\nboot_dist |&gt; \n  filter(term == \"age\") |&gt; \nget_confidence_interval(level = 0.95, type = \"percentile\") \n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     127.     149.\n\n\nNext, the code to construct the null distribution and compute p-value for the permutation test is below. Note that the argument variables = age is used in generate() to specify that age is the only column that is permuted (shuffled). The default for generate() is to permute all columns.\n\nset.seed(12345) \nniter = 1000 \n\nage_coef &lt;- lemurs_model |&gt; \n  tidy() |&gt; \n  filter(term == \"age\") |&gt; \n  pull(estimate)\n\n# construct null distribution\nnull_dist &lt;- lemurs_sample_young |&gt; \n  specify(weight ~ age + sex + taxon + litter_size) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = niter, type = \"permute\", variables = age) |&gt; \n  fit() \n\n# compute p-value\nnull_dist |&gt;\n  filter(term == \"age\") |&gt; \n  get_p_value(obs_stat = age_coef, direction = \"two-sided\") \n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n8.7.2 Theory-based inference in R\nThe statistics associated with the hypothesis test for each coefficient are produced by default when printing the output using tidy(). We can add the arguments conf.int = and conf.level = to the tidy function to output the confidence interval for each model coefficient.\nBelow is the code to fit and output the model from Section 8.1 with the 95% confidence interval for the model coefficients. The kable function is used to display the results in a neatly formatted table rounding all values to three digits.\n\nlemurs_model &lt;- lm(weight ~ age + sex + taxon +\n                          litter_size,\n                        data = lemurs_sample_young)\n\ntidy(lemurs_model, conf.int = TRUE, conf.level = 0.95) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n24.2\n93.63\n0.259\n0.796\n-160\n208.7\n\n\nage\n136.9\n4.86\n28.182\n0.000\n127\n146.5\n\n\nsexM\n-88.8\n68.00\n-1.306\n0.193\n-223\n45.1\n\n\ntaxonPCOQ\n471.7\n95.43\n4.943\n0.000\n284\n659.7\n\n\ntaxonVRUB\n1037.3\n133.91\n7.747\n0.000\n774\n1301.1\n\n\nlitter_size\n-19.6\n65.80\n-0.298\n0.766\n-149\n110.0\n\n\n\n\n\n\n8.7.3 Model conditions and diagnostics\nWe use the augment function to produce a tibble of the statistics used for model conditions and diagnostics as with simple linear regression. See Section 6.6 for a description of the output produced by augment() and how these values are used to check model conditions and diagnostics.\n\nlemur_aug &lt;- augment(lemurs_model)\n\nlemur_aug |&gt;\n  slice(1:5)\n\n# A tibble: 5 × 11\n  weight   age sex   taxon litter_size .fitted .resid   .hat .sigma    .cooksd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1    410  2.27 F     ERUF            0    335.   75.0 0.0277   534. 0.0000968 \n2    137  0.72 F     ERUF            0    123.   14.2 0.0298   534. 0.00000374\n3    206  0.69 F     PCOQ            0    590. -384.  0.0201   533. 0.00182   \n4    185  1.05 M     PCOQ            0    551. -366.  0.0183   533. 0.00150   \n5    373  2.66 F     PCOQ            0    860. -487.  0.0178   533. 0.00257   \n# ℹ 1 more variable: .std.resid &lt;dbl&gt;",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#summary",
    "href": "08-mlr-inference.html#summary",
    "title": "8  Inference for multiple linear regression",
    "section": "\n8.8 Summary",
    "text": "8.8 Summary\nIn this chapter, we built upon inference for simple linear regression introduced Chapter 5 and showed how inference can be used to draw conclusions about model coefficients in multiple linear regression models. We showed how to conduct simulation-based inference using bootstrapping for confidence intervals and permutation tests for hypothesis testing. Then, we showed how to compute the values shown the regression output by conducting inference using mathematical models based on the Central Limit Theorem. Lastly, we showed how to check model conditions and diagnostics for multiple linear regression, building off of the introduction to these topics in Chapter 6.\nSometimes the data shows violations in the model conditions that need to be addressed to make reliable conclusions and predictions from the model. In the next chapter, we will introduce variable transformations that can be used to address violations in the model conditions, particularly in the linearity and equal variance conditions.\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “Infer: An r Package for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "08-mlr-inference.html#footnotes",
    "href": "08-mlr-inference.html#footnotes",
    "title": "8  Inference for multiple linear regression",
    "section": "",
    "text": "Question 1: This can be answered by hypothesis test and confidence interval. Question 2: This question can be answered by a confidence interval.↩︎\nIt is estimate of \\(SE_{\\hat{\\beta}_{age}}\\), the standard error of the coefficient of age.↩︎\nWe are 90% confident that the weight of PCOQ lemurs is 314.442 to 630.842 grams higher, on average, compared the weight of ERUF lemurs, holding age, sex, and litter size constant.↩︎\n\n\nThe center of the null distribution is approximately equal to 0, the hypothesized value. Because the null is created using a simulation-based procedure, it will not be exact but very close as the number of samples increases.\nThe standard deviation of the null distribution is approximately equal to the standard deviation of the bootstrap distribution. They are both approximations of \\(SE_{\\hat{\\beta}_{age}}\\).\n\n\n↩︎\n\nIt corresponds to a hypothesis test with \\(\\alpha = 0.05\\). Yes, the interval is consistent, because the null hypothesized value 0 is not in the interval.↩︎\nThis is a large test statistic, so it appears to provide evidence against the null hypothesis. We will complete the last two steps of hypothesis testing to see the conclusion.↩︎\nOur calculations do not exactly match the confidence interval produced by software, because we have used rounded values of the estimated slope, critical value, and standard error.↩︎\nThe confidence level 99% produces the most accurate interval. The confidence level 90% produces the most precise interval.↩︎\nThe leverage of each observation will be the same for the two models. Leverage only depends on the predictor variables. It does not depend on the response variable.↩︎\nThe residual means that the observed weight for this lemur is 3380.42 grams greater than the weight predicted by the model. The standardized residual means that the residual for this observation is 6.442standard errors above the mean of the distribution of residuals.↩︎",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inference for multiple linear regression</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html",
    "href": "09-mlr-transformations.html",
    "title": "9  Variable transformations",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#learning-goals",
    "href": "09-mlr-transformations.html#learning-goals",
    "title": "9  Variable transformations",
    "section": "",
    "text": "Identify when a variable transformation is needed to summarize the relationship between a response variable and predictor variable(s)\nExplain the role of variance-stabilizing transformations in regression analysis\nFit and interpret models with a log-transformed response variable\nFit and interpret models with one or more log-transformed predictor variables",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#sec-transform-eda",
    "href": "09-mlr-transformations.html#sec-transform-eda",
    "title": "9  Variable transformations",
    "section": "\n9.1 Introduction: College basketball expenditures",
    "text": "9.1 Introduction: College basketball expenditures\n\nThe analysis in this chapter focuses on the amount of money colleges and universities spend in total on men’s and women’s basketball programs in a single year. The data come from the Equity in Athletics Data Analysis (EADA) tool from the Office of Postsecondary Education in the United States Department of Education (ope.ed.gov/athletics). Every college and university in the United States that offers both student financial aid and intercollegiate programs (this includes nearly all colleges and universities) are required by law to submit information about its spending and revenue on athletic programs annually (U.S. Department of Education, Office of Postsecondary Education 2025).\nThe EADA is a very rich data set with over 300 hundred variables about basketball and all other collegiate sports for all 2-year and 4-year institutions that meet the federal reporting requirements.1 This analysis will focus on data from schools in the National Collegiate Athletics Association (NCAA) Division I. According to the NCAA, Division I institutions are those that “have to sponsor at least seven sports for men and seven for women (or six for men and eight for women) with two team sports for each gender. Each playing season has to be represented by each gender as well. There are contest and participant minimums for each sport, as well as scheduling criteria” (NCAA 2013). We will refer to these institutions as “NCAA Division I colleges” or just “colleges” throughout the remainder of the chapter.\nThe data in ncaa-basketball-DI-2023-2024.csv contains data from the 2023 - 2024 academic year. We will use the following variables that have been derived from variables in the EADA.\n\n\nexpenditure_m: Total expenditure on basketball programs (in millions of US dollars)\n\nenrollment_th: Total student enrollment (in thousands)\n\ntype: Whether the college is public or private\n\nregion: Region where the college is located. The regions are defined using the state.region vector in R.\n\n\nThe goal of this analysis is to use the student enrollment, region, and type of college to explain variability in the total expenditure on basketball programs in the 2023 - 2024 academic year.\n\n\n9.1.1 Exploratory data analysis\n\n\n\n\n\n\n\nFigure 9.1: Distribution of the response variable expenditure_m\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) enrollment_th\n\n\n\n\n\n\n\n\n\n(b) region\n\n\n\n\n\n\n\n\n\n\n\n(c) type\n\n\n\n\n\n\nFigure 9.2: Distributions of predictor variables\n\n\n\n\n\nTable 9.1: Summary statistics for enrollment_th and expenditure_m\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\nexpenditure_m\n8.3\n7.0\n1.3\n3.7\n5.0\n10.8\n38.1\n0\n\n\nenrollment_th\n11.9\n9.9\n1.0\n4.6\n7.9\n17.8\n59.6\n0\n\n\n\n\n\n\n\n\nThe response variable expenditure_m in Figure 9.1 is unimodal and skewed right. The median expenditure on basketball programs is 5 million dollars, but there are a few colleges that spent over 30 million dollars on basketball programs in 2023 - 2024. Similarly, enrollment_th in Figure 9.2 (a) is unimodal and skewed right, with a median enrollment of about 7900 students. There are some very large colleges in the data set with enrollments of over 40,000 students.\nAlmost half of the 355 colleges in the data are located in the South region (Figure 9.2 (b)), and about two-thirds of the colleges in the data set are public institutions (Figure 9.2 (c)).\n\n\n\n\n\n\n\n\n\n(a) expenditure_m versus enrollment_th\n\n\n\n\n\n\n\n\n\n(b) expenditure_m versus region\n\n\n\n\n\n\n\n\n\n\n\n(c) expenditure_m versus type\n\n\n\n\n\n\nFigure 9.3: Bivariate exploratory data analysis\n\n\nFrom the bivariate exploratory data analysis in Figure 9.3 (a), there appears to be a strong positive relationship between the enrollment and expenditure. This aligns with what we might expect; colleges with more students are likely to spend more on basketball programs. We note that this trend does seem to level off for colleges with very high enrollment, because their expenditure is lower than what we would expect if expenditure increased as a constant rate for values of enrollment (called monotonically increasing).\nThe plot also shows two other interesting features of the relationship. The first is that the variability in the expenditure increases as enrollment increases. Thus, there is more variability in how much colleges with larger enrollments spend on basketball programs compared to colleges with small enrollments. We will keep this issue of the changing variability in mind as we fit a model and evaluate the model conditions. The second observation is the appearance of a “V” shape in the trend, indicating two different trajectories. This could be an indication of a potential interaction, so let’s look at multivariate plots to explore a potential interaction between enrollment and type and one between enrollment and region.\n\n\n\n\n\n\n\n\n\n(a) expenditure_m versus enrollment_th by type\n\n\n\n\n\n\n\n\n\n(b) expenditure_m versus enrollment_th by region\n\n\n\n\n\n\nFigure 9.4: Exploration of potential interaction terms\n\n\n\nBased on the plots in Figure 9.4, do there appear to be any potential interaction effects? Explain.2\n\n\n9.1.2 Initial model\nWe include the interaction between type and enrollment_th in the first modeling attempt, shown in Table 9.2. The plot of the residuals versus fitted values along with the distribution of the residuals are in Figure 9.5.\n\n\n\nTable 9.2: Model for NCAA basketball expenditure with 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n7.247\n1.021\n7.100\n0.000\n5.239\n9.254\n\n\nenrollment_th\n0.557\n0.102\n5.476\n0.000\n0.357\n0.756\n\n\nregionNortheast\n-3.044\n0.985\n-3.090\n0.002\n-4.981\n-1.106\n\n\nregionSouth\n-1.146\n0.786\n-1.458\n0.146\n-2.692\n0.400\n\n\nregionWest\n-3.009\n0.973\n-3.093\n0.002\n-4.922\n-1.096\n\n\ntypePublic\n-5.049\n1.042\n-4.845\n0.000\n-7.099\n-3.000\n\n\nenrollment_th:typePublic\n-0.060\n0.107\n-0.566\n0.572\n-0.271\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Residuals versus predicted\n\n\n\n\n\n\n\n\n\n(b) Distribution of residuals\n\n\n\n\n\n\nFigure 9.5: Residual plots from original NCAA basketball expenditure model\n\n\nThere is a “fan” shape in Figure 9.5, so the equal variance condition is not met. Because the equal variance condition is not met, the results of theory-based inference output in Table 9.2 cannot reliably be used to draw conclusions about the relationships between the response and predictor variables. For example, the inferential results suggest the interaction between enrollment and institution type is not useful in this model, despite what we observed in Figure 9.4. Before making a final conclusion about the interaction term ad any other term in the model, let’s first address the issue with the equal variance condition.\nHere is a summary of some of the points of interest we’ve observed thus far in the exploratory data analysis and initial modeling attempt.\n\nThe relationship between enrollment_th and expenditure_m seems to “level off” for high-enrollment colleges, thus the relationship may be non-linear.\nThere is a fan shape in the plot of the residuals versus predicted values, indicating a violation of the equal variance condition.\nThe exploratory data analysis suggests there is a potential interaction between enrollment_th and type.\n\nNote that items (1) and (2) are about the model fit and item (3) is about a conclusion drawn from the model. Thus we need to address items (1) and (2) before digging further into item (3). To do so, we will examine three additional models that differ based on transformations on the response variable expenditure_m and/or enrollment_th, the quantitative predictor variable. Transformations only apply to quantitative variables, so type and region will remain unchanged in the subsequent models throughout the chapter.\nThroughout the chapter, we will interpret the coefficients and evaluate the model conditions for each of the new candidate models. We’ll then use model performance statistics along with the conditions to select the model that best fits the data. The candidate models are the following:\n\nOriginal model with no transformations (shown in Table 9.2)\nLog-transformation on the response variable, \\(\\log(y)\\)\n\nLog-transformation on the quantitative predictor variable, \\(\\log(x)\\)\n\nLog-transformation on the response and quantitative predictor variables, \\(\\log(x)\\) and \\(\\log(y)\\)\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n(b) log(y)\n\n\n\n\n\n\n\n\n\n\n\n(c) log(x)\n\n\n\n\n\n\n\n\n\n(d) log(x) and log(y)\n\n\n\n\n\n\nFigure 9.6: Relationship between expenditure_m and enrollment_th for four candidate models\n\n\n\nWhich relationship visualized in Figure 9.6 would be best summarized by a linear regression model?3\n\nThe plots in Figure 9.6 can help provide intuition on what to expect as we do the analysis. In this chapter, we will look at the transformations presented in Plots B - D in the order they are plotted. In practice, however, you can use insights from the exploratory data analysis to start with the model that appears to be the best fit for the data. We do not draw definitive conclusions from the exploratory data analysis, so we will use inference, model conditions, and model fit statistics to select the best fit model. For now, we will just discuss these transformations in terms of a main effects model. In Section 9.5, we add the interaction term to our selected model.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#sec-transform-logy",
    "href": "09-mlr-transformations.html#sec-transform-logy",
    "title": "9  Variable transformations",
    "section": "\n9.2 Transformation on the response variable",
    "text": "9.2 Transformation on the response variable\n\nThe plot of residuals versus predicted variables in Figure 9.5 shows a clear violation in the equal variance condition based on the fan-shaped pattern of the residuals when moving from left to right. Recall from Section 6.3 that it is necessary to satisfy the equal variance condition when doing theory-based inference (the inference results produced by output), because it relies on a single estimate of \\(\\hat{\\sigma}_{\\epsilon}\\), the regression standard error (variability about the regression line). We can easily see in Figure 9.5 that a single value would not be representative of the vertical spread of the residuals for all predicted values. Thus, the inferential results in Table 9.2 should be interpreted with caution.\nOne solution to this is to use the simulation-based inference methods introduced in Section 8.3, because the sampling distribution and null distribution for an estimated coefficient \\(\\hat{\\beta}_j\\) are constructed relying only on the data, rather than any underlying assumptions about the distribution of \\(\\hat{\\beta}_j\\). Another approach is to use a transformation on the response variable to reduce the impacts of the different vertical spreads in the response variable. Such transformations are called variance-stabilizing transformations.\n\n\nSome commonly used variance-stabilizing transformations are the square-root transformation, \\(\\sqrt{y}\\) , and the reciprocal transformation, \\(1/y\\) . In fact, the square root and reciprocal transformations are just two examples from the class of transformations called the Box-Cox transformations (Box and Cox 1964). These transformations are designed to handle a non-normal response variable of any shape. While Box-Cox transformations, such as \\(\\sqrt{y}\\) and \\(1/y\\), can be the optional choice for stabilizing variance and making the distribution of the response variable more normal (recall the normality condition), it is often difficult to interpret the coefficients for models with such transformed response variables in a way that an be easily understood by readers. Therefore, we will not focus on Box-Cox transformations in this text, because one of our primary objectives is to produce models that are easily interpretable.\n\n\nLet’s consider another transformation for the response variable that can help address the violations in the equal variance condition while also being easily interpreted by the reader. We can apply a natural log transformation on the response variable, \\(\\log(y)\\) (also written as \\(\\ln(y)\\)), such that \\(\\log(y)\\) has the linear relationship with the predictor variables. Equation 9.1 shows the statistical model when applying such transformation to the response variable.\n\\[\n\\log(y_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}+ \\dots + \\beta_px_{ip} + \\epsilon_i \\hspace{8mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{9.1}\\]\n\nlog = ln\nIn statistics, the term “log” refers to the natural log (typically denoted \\(\\ln\\) in mathematics).\n\nAs shown in Equation 9.1, the log transformation is applied to the individual values of \\(Y\\), and the transformed values are used in the model. Therefore, when we use a log (or any) transformation on the response variable, we assume the assumptions from Section 5.3 apply to the relationship between the predictors and the transformed values of \\(Y\\). Using Equation 9.1, this means we make assumptions about the relationship between the predictors and \\(\\log(y_i)\\). The full analysis, including evaluating conditions and diagnostics, conducting inference, and computing predictions are done in terms of \\(\\log(y)\\). Interpretations, however, are in terms of the original variable \\(y\\). Although we can write interpretations in terms of \\(\\log(y)\\) (we’ll see an example of this shortly), it is not as intuitive for readers to understand interpretations on a log-scale as on the natural scale.\nLet’s begin by looking at the distribution of log(expenditure_m), its relationship with enrollment_th, and how these compare to the exploratory data analysis in Section 9.1.\n\n\n\n\n\n\n\n\n\n(a) Distribution of expenditure_m\n\n\n\n\n\n\n\n\n\n(b) Distribution of log-transformed expenditure_m\n\n\n\n\n\n\n\n\n\n\n\n(c) expenditure_m versus enrollmenth_th\n\n\n\n\n\n\n\n\n\n(d) Log-transformed expenditure_m versus enrollment_th\n\n\n\n\n\n\nFigure 9.7: Univariate and bivariate EDA for original and log-transformed expenditure_m\n\n\nNow we fit the main effects model using log(expenditure_m), the log-transformed expenditure, as the response variable. The output for the new is in Table 9.3 and the equation is in Equation 9.2.\n\n\n\nTable 9.3: Model with log-transformed expenditure as the response variable\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n1.783\n0.084\n21.21\n0.000\n1.617\n1.948\n\n\nenrollment_th\n0.056\n0.003\n17.14\n0.000\n0.050\n0.062\n\n\nregionNortheast\n-0.311\n0.098\n-3.19\n0.002\n-0.503\n-0.119\n\n\nregionSouth\n-0.176\n0.078\n-2.26\n0.025\n-0.329\n-0.023\n\n\nregionWest\n-0.218\n0.096\n-2.26\n0.024\n-0.408\n-0.028\n\n\ntypePublic\n-0.672\n0.074\n-9.11\n0.000\n-0.817\n-0.527\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{\\log(\\text{expenditure\\_m})} = & 1.783 + 0.056 \\times \\text{enrollment\\_th}\\\\\n&- 0.311 \\times \\text{Northeast} - 0.176 \\times \\text{South} \\\\\n& - 0.218 \\times \\text{West} - 0.672 \\times \\text{Public}\n\\end{aligned}\n\\tag{9.2}\\]\nBefore using the model, we analyze the residuals versus predicted values to evaluate whether the concerns with equal variance have been addressed by the transformation. Figure 9.8 shows the original plot of the residuals versus predicted values and the plot for the model using the log-transformed response variable.\n\n\n\n\n\n\n\n\n\n(a) Original model\n\n\n\n\n\n\n\n\n\n(b) Model with log-transformed expenditure_m\n\n\n\n\n\n\nFigure 9.8: Residuals versus predicted values for original model and model with log-transformed expenditure_m\n\n\nComparing the plots in Figure 9.8, the first observation is that the \\(y\\)-axis differs on the plots. This is the because the residuals are on the same scale and use the same units as the response variable. The residuals in Figure 9.8 (a) are in terms of the original variable expenditure_m, and the residuals in Figure 9.8 (b) are in terms of log(expenditure_m). The same is true for the predicted values on the \\(x\\)-axis. The second observation is that the vertical spread of the residuals is much more equal Figure 9.8 (b) with the log-transformed response variable. Given this, it appears applying a log transformation on the response variable has addressed some of the problems with the equal variance condition.\nWe can consider other transformations to improve the model fit even more; we’ll discuss those as the chapter continues. For now, let’s interpret the coefficients for the model in Table 9.3. The approach for interpreting the model coefficients in terms of the log-transformed response variable is the same as the one introduced in Section 9.2 for other multiple linear regression models. Below are the interpretations for enrollment_th and regionNortheast in terms of log(expenditure).\n\nFor each additional 1000 students at an NCAA Division I college, the total log(expenditure) on basketball programs is expected to increase by 0.056 log(millions of dollars), on average, holding region and type constant.\nThe total log(expenditure) on basketball programs for NCAA Division I colleges is expected to be 0.311 log(millions of dollars) less, on average, for colleges in the Northeast region compared to those in the North Central region, holding enrollment and type constant.\n\nThough these interpretations are technically correct, it is difficult to fully comprehend what a change in log(millions of dollars) to the log(expenditure) really means. Therefore, we need to get back to the original variable (expenditure_m) and back to the original units (millions of dollars) to write an interpretation that is more intuitive.\n\n\n\\(e^{\\log(a)} = a\\)\n\\(e^{a+b} = e^ae^b\\)\n\n\nGiven Equation 9.1 with a log-transformed response variable, the predicted values in terms of the original response variable is\n\\[\n\\begin{aligned}\n\\hat{y}_i &= e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + \\hat{\\beta}_2x_{i2}+ \\dots + \\hat{\\beta}_px_{ip}} \\\\[10pt]\n& = e^{\\hat{\\beta}_0}e^{\\hat{\\beta}_1x_{i1}}e^{\\hat{\\beta}_2x_{i2}} \\dots e^{\\hat{\\beta}_px_{ip}}\n\\end{aligned}\n\\tag{9.3}\\]\nFrom Equation 9.3, the interpretations in terms of the original response variable are about a multiplicative change in the response when the predictor variable changes. Specifically, when \\(X_j\\) increases by one unit, \\(Y\\) is expected to multiply by a factor of \\(e^{\\hat{\\beta}_j}\\). The mathematical details for Equation 9.3 and the interpretations are in Section A.9.1.\nNow let’s use Equation 9.3 to interpret the coefficients of enrollment_th and regionNortheast in terms of expenditure_m.\n\nFor each additional 1000 students at an NCAA Division I college, the total log(expenditure) on basketball programs is expected to multiply by a factor of 1.058 (\\(e^{0.056}\\)), holding region and type constant.\nThe total expenditure on basketball programs for NCAA Division I colleges in the Northeast region is expected to be 0.733 (\\(e^{-0.311}\\)) times the expenditure at colleges in the North Central region, holding enrollment and type constant.\n\n\nWrite the interpretation for typePublic in Table 9.3 in terms of the change in enrollment. 4\n\nThe intercept, the expected response when all predictors are equal to 0, is \\(e^{\\hat{\\beta}_0}\\). Thus, the interpretation of the intercept for Table 9.3 in terms of the original response variable is\n\nThe total expenditure on basketball programs for private NCAA Division I colleges in the North Central region with no students enrolled is expected to be 5.948 \\((e^{1.783})\\) million dollars.\n\n\nIs the interpretation of the intercept for the model in Table 9.3 meaningful? If not, what can we do to make the interpretation meaningful?5",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#sec-transform-logx",
    "href": "09-mlr-transformations.html#sec-transform-logx",
    "title": "9  Variable transformations",
    "section": "\n9.3 Transformation on predictor variable(s)",
    "text": "9.3 Transformation on predictor variable(s)\nTransformations on the response variable are often done to address violations in the equal variance condition, as shown in the previous section. In this section, we introduce transformations on one or more quantitative predictor variables that are used to model non-linear relationships with the response variable.\nIn the exploratory data analysis in Section 9.1.1, we observed that the relationship between expenditure_m and enrollment_th flattened as enrollment_th increased. For Colleges with larger enrollments generally have a smaller increase in expenditures on basketball programs for each additional 1000 students compared to the increase in expenditures for colleges with smaller enrollments. We account for this decreasing effect by applying a log transformation on the predictor variable. Here we will transform a single predictor but we can apply a log transformation to multiple quantitative predictors in a model.\nThe general form of the linear regression model with a log-transformed predictor variable \\(x_j\\) is shown in Equation 9.4 .\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_j\\log(x_{ij}) + \\dots + \\beta_px_{ip} + \\epsilon_i  \\hspace{5mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{9.4}\\]\nEquation 9.5 shows the form of the model for NCAA Division I basketball expenditure using log-transformed values of enrollment.\n\\[\n\\begin{aligned}\n\\text{expenditure\\_m}_i = \\beta_0 &+ \\beta_1 \\log(\\text{enrollment\\_th}_i) + \\beta_2\\text{Northeast}_i + \\beta_3\\text{South}_i \\\\\n&+ \\beta_4\\text{West}_i + \\beta_5\\text{Public}_i + \\epsilon_i \\hspace{5mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n\\end{aligned}\n\\tag{9.5}\\]\nAt this point, we are using the original response variable, expenditure_m, so we can focus on understanding the log-transformed predictor variable. In the next section, we will look at the scenario in which both the response variable and one or more predictor variables is transformed. The output of the model with log-transformed enrollment is in Table 9.4.\n\n\n\nTable 9.4: Output of model with log-transformed enrollment_th\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n1.297\n1.026\n1.265\n0.207\n\n\nlog(enrollment_th)\n5.992\n0.400\n14.994\n0.000\n\n\nregionNortheast\n-3.242\n0.991\n-3.271\n0.001\n\n\nregionSouth\n-0.662\n0.793\n-0.835\n0.404\n\n\nregionWest\n-2.919\n0.978\n-2.985\n0.003\n\n\ntypePublic\n-6.531\n0.783\n-8.337\n0.000\n\n\n\n\n\n\n\n\nLet’s start by interpreting the intercept for this model. Because we have used the log-transformed value of enrollment in the model, the intercept represents the subgroup of NCAA Division 1 colleges such that log(enrollment_th) and all the other predictors are equal to 0. Based on the rules of logarithms, log(enrollment_th) is equal to 0 when enrollment_th is equal to 1, corresponding to an institution with 1000 students enrolled. The interpretation of the intercept for this model is the following:\n\nThe expenditure on basketball programs for private NCAA Division 1 colleges in the North Central region with 1000 students enrolled (log(enrollment_th) = 0) is expected to be 1.297 million dollars.\n\nNow let’s look at the interpretation of the predictor variable enrollment_th. Using the coefficient interpretations introduced in Section 7.4.1, the interpretation is the following:\n\nWhen the log-transformed enrollment increases by 1, the expenditure on basketball programs at NCAA Division 1 colleges is expected to increase by 5.992 million dollars, holding type and region constant.\n\nThough this interpretation is technically accurate, it is difficult to comprehend what an increase in the log-transformed enrollment actually means. Therefore, rather than interpreting the coefficient in terms of the log-transformed enrollment, we interpret enrollment in terms of its original units of 1000 students.\nBecause `enrollment_th` is on the logarithmic scale in the model, it is most straightforward  to interpret the change in expenditure when enrollment is multiplied by a constant \\(C\\). In general terms, given a model of the form in Equation 9.4, the interpretation of predictor \\(X_j\\) is below. The mathematical details for this interpretation are in Section A.9.2. \n\nWhen \\(X_j\\) is multiplied by a constant \\(C\\), \\(Y\\) is expected to change (increase or decrease) by \\(\\beta_j \\log(C)\\) units, holding all else constant.\n\n\n\\(\\log(ab) = \\log(a) +  \\log(b)\\)\nApplying this to Equation 9.4,\n\\(\\beta_j\\log(CX_j) = \\beta_j[\\log(C) + \\log(X_j)] = \\beta_j\\log(C) + \\beta_j \\log(X_j)\\)\n\nNow let’s interpret the effect of enrollment_th in terms of the expected change in expenditure_m when enrollment_th doubles (\\(C = 2\\)).\n\nWhen the enrollment doubles (is multiplied by 2), the expenditure on basketball programs at NCAA Division 1 institutions is expected to increase by 4.153 (5.992 \\(\\times\\) log(2)) million dollar, on average, holding type and region constant.\n\nWhen writing these interpretations, the constant \\(C\\) can take any value between \\(-\\infty\\) and \\(\\infty\\). When choosing \\(C\\), however, we need to be mindful about potential extrapolation and writing interpretations for values far outside the range of the data. Use values of \\(C\\) between 1 and 2 to reduce potential extrapolation. Values in this range provide straightforward interpretations about the expected change when there is some percentage increase in the predictor (for example, \\(C = 1.1\\) equals a 10% increase in the value of the predictor). Note, however, that there is a risk of extrapolating for observations with values at or near the maximum value of the predictor. For example, there is a college in the data with enrollment around 59,000. We should be cautious when using the model to interpret the expected change when enrollment doubles for this institution. This would equal an enrollment of over 118,000 students, which is far outside the range of the data. We cannot safely assume the relationship between enrollment_th and expenditure_m observed in Table 9.4 is the same for colleges with such high enrollment (or that such colleges even exist!).\n\nWrite the interpretation of the expected change in expenditure on basketball programs when enrollment increases by 50% \\((C = 1.5)\\).6\n\n\n\n\n\n\n\n\nFigure 9.9: Residuals vs. fitted values for plot with log-transformed enrollment_th\n\n\n\n\nThe plot of the residuals versus fitted values for the model Table 9.4 is shown in Figure 9.9. This plot has a fan shape similar to the one observed in Figure 9.5. Therefore, even though the log transformation on enrollment_th accounted for its non-linear relationship with expenditure_m, it did not address the violation in the equal variance condition like the log transformation on the response variable in Section 9.2. In the next section, we introduce a model that includes both a log-transformed response variable and log-transformed predictor.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#sec-transform-logx-logy",
    "href": "09-mlr-transformations.html#sec-transform-logx-logy",
    "title": "9  Variable transformations",
    "section": "\n9.4 Transformation on the response variable and predictor variable(s)",
    "text": "9.4 Transformation on the response variable and predictor variable(s)\nModels with transformations on both the response variable and one or more quantitative predictor variables can both address violations in the equal variance condition and capture non-linear relationships. Given what we have observed in the exploratory data analysis in Section 9.1.1 and the residual plots in Figure 9.5, Figure 9.8 , and Figure 9.9, we will now fit a model using both log(expenditure_m) and log(enrollment_th).\nEquation 9.6 is the general form of the model with a log-transformed response variable and a log-transformed predictor variable \\(x_j\\) is shown in\n\\[\n\\log(y_i) = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_j\\log(x_{ij}) + \\dots + \\beta_px_{ip} + \\epsilon_i  \\hspace{5mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{9.6}\\]\nEquation 9.7 shows the form of the model for NCAA Division I basketball expenditure, and the output of the estimated model is in Table 9.5.\n\\[\n\\begin{aligned}\n\\log(\\text{expenditure\\_m}_i) = \\beta_0 &+ \\beta_1 \\log(\\text{enrollment\\_th}_i) + \\beta_2\\text{Northeast}_i + \\beta_3\\text{South}_i \\\\\n&+ \\beta_4\\text{West}_i + \\beta_5\\text{Public}_i + \\epsilon_i \\hspace{5mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n\\end{aligned}\n\\tag{9.7}\\]\n\n\n\nTable 9.5: Output of model with log-transformed expenditure and log-transformed enrollment\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n1.025\n0.097\n10.58\n0.000\n\n\nlog(enrollment_th)\n0.707\n0.038\n18.75\n0.000\n\n\nregionNortheast\n-0.337\n0.094\n-3.60\n0.000\n\n\nregionSouth\n-0.115\n0.075\n-1.53\n0.126\n\n\nregionWest\n-0.221\n0.092\n-2.39\n0.017\n\n\ntypePublic\n-0.830\n0.074\n-11.22\n0.000\n\n\n\n\n\n\n\n\nTo interpret the model in Table 9.5 that has both a log-transformed response and predictor variable, we will combine the approach to interpretations from Section 9.2 and Section 9.3.\nThe intercept is the expected value of log(y) when all the predictors are equal to 0. Because there is a log-transformed predictor \\(x_j\\) in the model in Equation 9.6, this will be when \\(\\log(x_j)\\) is equal to 0, meaning \\(x_j\\) is equal to 1. In terms of the model on NCAA Division I basketball expenditures shown in Table 9.5, the intercept 1.025 is interpreted as follows:\n\nThe expected log-transformed expenditure on basketball programs at private NCAA Division I colleges in the North Central region with 1000 students enrolled is 1.025.\n\nFor readability, we won’t present results in terms of log-transformed expenditure, so we get back to the original response variable expenditure_m.\n\nThe expected expenditure on basketball programs at private NCAA Division I colleges in the North Central region with 1000 students enrolled is 2.787 (exp(1.025)) million dollars.\n\nThe interpretation for \\(\\beta_j\\), the coefficient for \\(\\log(X_j)\\) in Equation 9.6 is a bit more involved, as it incorporates the multiplicative change in both \\(X\\) and \\(Y\\). We’ll start by writing the interpretation in terms of \\(\\log(Y)\\).\n\nWhen \\(X_j\\) multiplies by a factor of \\(C\\), \\(\\log(Y)\\) is expected to change (increase or decrease) by \\(\\beta_j \\log(C)\\) , on average, holding all else constant.\n\n\n\\(e^{a\\log(b)} = b^a\\)\n\nNote that this interpretation is very similar to the interpretation from Section 9.3, when there is a log-transformed predictor variable. The full mathematical details to get back to the original response variable are in Section A.9.3 .The final interpretation in terms of the original response variable when \\(X_j\\) is multiplied by \\(C\\) is\n\nWhen \\(X_j\\) multiplies by a factor of \\(C\\), \\(Y\\) is expected multiply by a factor of \\(C^{\\beta_j}\\) , holding all else constant.\n\nNow let’s use this to interpret enrollment_th for the model in Table 9.5. Below is the interpretation in terms of a 20% increase in enrollment, \\(C = 1.2\\).\n\nWhen enrollment at NCAA Division I colleges increases by 20% (multiples by 1.2), the expenditure on basketball programs is expected to multiply by a factor of 1.138 ( \\(1.2^{0.707}\\)), holding type and region constant.\n\n\nWrite the interpretation for typePublic in Table 9.5 in the context of the data. Write the interpretation in terms of the original variable expenditure_m. 7\n\nThe plot of residuals versus predicted for the model in Table 9.5 is shown in Figure 9.10. This plot shows improvement in the equal variance condition compared to the original residuals plot in Figure 9.5, given the vertical spread of the residuals is more equal around the majority of the data. There is less variability in the residuals for institutions with low predicted expenditure, but there are few institutions with predicted log(expenditure) less than 1.\n\n\n\n\n\n\n\nFigure 9.10: Residuals vs. fitted values for plot with log-transformed enrollment and expenditure",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#sec-transform-choose-model",
    "href": "09-mlr-transformations.html#sec-transform-choose-model",
    "title": "9  Variable transformations",
    "section": "\n9.5 Choosing a model",
    "text": "9.5 Choosing a model\nSo far we have looked at four potential models using enrollment_th, type and region to understand variability in expenditure_m. The next step is to select one of these models to use for prediction and inference based on the original analysis objectives. In addition to checking the residual plots and model conditions, we use performance statistics, such as \\(R^2\\), to help select the model that best fits the data. We discuss model evaluation and selection in-depth in Chapter 10.\n\n\n\nTable 9.6: R-Squared values for models presented in this chapter\n\n\n\n\nOriginal\nlog(y)\nlog(x)\nlog(x) & log(y)\n\n\n0.409\n0.473\n0.4\n0.516\n\n\n\n\n\n\n\nTable 9.6 shows the \\(R^2\\) values for the four candidate models. Based on the \\(R^2\\) values and the plots of the residuals versus predicted values, the model that includes both log(expenditure_m) and log(enrollmet_th) is the one that best fits the data.\n\n9.5.1 Adding interaction term\nNow that we have selected the main effects, let’s reconsider the interaction between enrollment_th and type. Figure 9.11 visualizes log(expenditure_m) versus log(enrollment_th) by type to see if there is a potential interaction effect using the transformed variables.\n\n\n\n\n\n\n\nFigure 9.11: Expenditure versus enrollment by institution type\n\n\n\n\nThe plot shows that there may be an interaction effect, because the slope of the relationship between log(expenditure_m) and log(enrollment_th) is steeper for public colleges than for private colleges. Given this plot, we include the interaction term to the model selected in Section 9.5. The output is in Table 9.7.\n\n\n\nTable 9.7: Model including interaction term with 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n1.308\n0.131\n9.96\n0.000\n1.050\n1.566\n\n\nlog(enrollment_th)\n0.524\n0.069\n7.58\n0.000\n0.388\n0.660\n\n\nregionNortheast\n-0.339\n0.092\n-3.67\n0.000\n-0.521\n-0.158\n\n\nregionSouth\n-0.117\n0.074\n-1.58\n0.114\n-0.262\n0.028\n\n\nregionWest\n-0.232\n0.091\n-2.54\n0.011\n-0.411\n-0.053\n\n\ntypePublic\n-1.284\n0.162\n-7.94\n0.000\n-1.602\n-0.966\n\n\nlog(enrollment_th):typePublic\n0.255\n0.081\n3.15\n0.002\n0.095\n0.414\n\n\n\n\n\n\n\n\nThe confidence interval for log(enrollment_th):typePublic gives evidence that the interaction is useful to include in the model. Additionally, the \\(R^2\\) for this model is 0.53, which is higher than the model chosen from Table 9.6. The equation for the model with the interaction term is\n\\[\n\\begin{aligned}\n\\widehat{\\log(\\text{expenditure\\_m}_i)} &= 1.308 + 0.524 \\times \\log(\\text{enrollment\\_th}_i)\\\\\n&- 0.339 \\times \\text{Northeast}_i - 0.117 \\times \\text{South}_i \\\\\n&- 0.232 \\times \\text{West}_i - 1.284 \\times \\text{Public}_i \\\\\n& + 0.255 \\times \\log(\\text{enrollment\\_th}_i) \\times \\text{Public}_i\n\\end{aligned}\n\\]\nWe put together the concepts from Section 7.7 and Section 9.4 to interpret the model coefficients describing the relationship between enrollment_th, type, and expenditure_m on basketball programs. The coefficient of the interaction term \\(\\hat{\\beta}_{interaction} = 0.255\\) means that \\(\\log(\\text{expenditure})\\) changes (increases or decreases) by an additional \\(C \\times 0.255\\) for public colleges compared to private colleges when enrollment is multiplied by a factor \\(C\\). This means that the expenditure is expected to multiply by an additional \\(C^{0.255}\\) for public institutions compared to private ones.\nBelow are the interpretations of the relationship between enrollment and expenditure for public and private schools given a 10% increase in the enrollment.\n\nFor each additional 10% increase ( \\(C = 1.1\\)) in enrollment at private NCAA Division I private colleges, the expenditure on basketball programs is expected to multiply by 1.689 \\((e^{0.524})\\), holding region constant.\nFor each additional 10% increase ( \\(C = 1.1\\)) in enrollment at public NCAA Division I colleges, the expenditure on basketball programs is expected to multiply by 2.179 \\((e^{0.524 + 0.255})\\), holding region constant.\n\n\n\n\nWe interpreted the relationship between enrollment and expenditure and how it differs for public and private colleges. But what if we wish to just describe how expenditure differs between public and private colleges, on average?\nThe coefficient of Public, \\(\\hat{\\beta}_{\\text{public}} = -1.284\\) is the expected difference in the expenditure on basketball programs between public and private colleges with enrollment_th = 0. A college with zero students does not make sense in practice, so we cannot meaningfully interpret this value in isolation.\n\nTherefore, we need to include both the main effect and the interaction term in order to describe how expenditures at public and private colleges differ, on average (-1.284 + 0.255 * enrollment_th). We plug in different values of enrollment to interpret the difference.\n\nFor example, let’s consider how the average expenditure differs between public and private colleges for colleges with 5000 students. Assume region is the same.\n\nPublic NCAA Division I institutions with 5000 students are expected to spend 1273.716 (-1.284 + 0.255 * 5000) times the expenditure at private colleges with the same enrollment in the same region.\n\n\n\nDescribe how the expenditure on basketball programs compare between public and private NCAA Division I colleges with 10,000 students enrolled. Assume both are in the South region.8",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#variable-transformations-in-r",
    "href": "09-mlr-transformations.html#variable-transformations-in-r",
    "title": "9  Variable transformations",
    "section": "\n9.6 Variable transformations in R",
    "text": "9.6 Variable transformations in R\nThe code for fitting a linear regression model with one or more transformed variables is very similar to the code shown in Section 7.8.1. Use log() to apply the (natural) log transformation on a variable. The code for all the models introduced in this chapter is shown below:\nOriginal model\n\nncaa_model_orig &lt;- lm(expenditure_m ~ enrollment_th + region + type, \n                      data = ncaa_basketball)\n\nModel with log(expenditure_m)\n\nncaa_model_logy &lt;- lm(log(expenditure_m) ~ enrollment_th + region + type, \n                      data = ncaa_basketball)\n\nModel with log(enrollment_th)\n\nncaa_model_logx &lt;- lm(expenditure_m ~ log(enrollment_th) + region + type, \n                      data = ncaa_basketball)\n\nModel with log(enrollment_th) , log(expenditure_m) , and interaction\n\nncaa_model_final &lt;- lm(log(expenditure_m) ~ log(enrollment_th) + region + type + \n                         log(enrollment_th) * type,\n                       data = ncaa_basketball)\n\nThe function exp() is used to exponentiate coefficients when writing interpretations for models with a log-transformed response variable. For example, \\(e^2\\) is computed as exp(2) in R. Below is code and output to make a new column with the exponentiated coefficients for the final model with log(enrollment_th), log(expenditure_m), and the interaction term. Note when displaying results using tidy() the coefficients are contained in the column estimate .\n\ntidy(ncaa_model_final) |&gt; \n  mutate(exp_coefficient = exp(estimate))\n\n# A tibble: 7 × 6\n  term               estimate std.error statistic  p.value exp_coefficient\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 (Intercept)           1.31     0.131       9.96 9.73e-21           3.70 \n2 log(enrollment_th)    0.524    0.0691      7.58 3.17e-13           1.69 \n3 regionNortheast      -0.339    0.0924     -3.67 2.78e- 4           0.712\n4 regionSouth          -0.117    0.0739     -1.58 1.14e- 1           0.890\n5 regionWest           -0.232    0.0912     -2.54 1.14e- 2           0.793\n6 typePublic           -1.28     0.162      -7.94 2.83e-14           0.277\n# ℹ 1 more row\n\n\nInstead of adding a new column, we may wish to automatically produce the exponentiated coefficients. The output produced by tidy() is in terms of the response variable by default. Therefore, the output is in terms of \\(\\log(y)\\) when fitting a model with a log-transformed response variable. The argument exponentiate = TRUE in tidy() is used to output the exponentiated coefficients, \\(e^{\\beta_j}\\), so the model output is in terms of the original variable.\nBelow is the code and output for the model in Table 9.7 in terms of the original response expenditure_m.\n\nncaa_model_final &lt;- lm(log(expenditure_m) ~ log(enrollment_th) + region + type + \n                         log(enrollment_th) * type, \n                       data = ncaa_basketball)\n\ntidy(ncaa_model_final, exponentiate = TRUE) |&gt; \n  kable(digits = 3)\n\n\nTable 9.8: Exponentiated coefficients for model using log-transformed repsonse and predictor\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n3.698\n0.131\n9.96\n0.000\n\n\nlog(enrollment_th)\n1.689\n0.069\n7.58\n0.000\n\n\nregionNortheast\n0.712\n0.092\n-3.67\n0.000\n\n\nregionSouth\n0.890\n0.074\n-1.58\n0.114\n\n\nregionWest\n0.793\n0.091\n-2.54\n0.011\n\n\ntypePublic\n0.277\n0.162\n-7.94\n0.000\n\n\nlog(enrollment_th):typePublic\n1.290\n0.081\n3.15\n0.002",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#summary",
    "href": "09-mlr-transformations.html#summary",
    "title": "9  Variable transformations",
    "section": "\n9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, we introduced transformations for non-linear relationships, with a focus on log transformations. We used exploratory data analysis, plots of residuals versus fitted values, and subject matter knowledge to identify scenarios in which a transformation is needed to best capture the relationships in the data. We introduced variance-stabilizing transformations on the response variable to address violations of the equal variance condition and a log transformation on the predictor variable to capture decreasing effect as the predictor increases. We also presented and interpreted models with log transformations on both the response and predictor variables.\nWe used \\(R^2\\) to select the model that best fit the data; however, there are other model performance statistics available to compare and select a model. In Chapter 10, we will discuss model evaluation and selection in more depth, including stepwise methods and cross validation.\n\n\n\n\nBox, George EP, and David R Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society Series B: Statistical Methodology 26 (2): 211–43.\n\n\nNCAA. 2013. “Divisional Differences and the History of Multidivision Classification.” National Collegiate Athletic Association. https://www.ncaa.org/sports/2013/11/20/divisional-differences-and-the-history-of-multidivision-classification.aspx.\n\n\nU.S. Department of Education, Office of Postsecondary Education. 2025. “Equity in Athletics Data Analysis Cutting Tool.” U.S. Department of Education. https://ope.ed.gov/athletics/.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "09-mlr-transformations.html#footnotes",
    "href": "09-mlr-transformations.html#footnotes",
    "title": "9  Variable transformations",
    "section": "",
    "text": "Interested readers are encouraged to visit the EADA for further data exploration!↩︎\nThere appears to be a potential interaction between enrollment_th and type . The slope of the relationship between enrollment_th and expenditure appears to be different based on the type of college.↩︎\nFrom the figure, it appears Figure 9.6 (d) with with a log transformation on both enrollment_th and expenditure_m would be best represented by a linear model, because the shape is most linear.↩︎\nThe total expenditure on basketball programs for NCAA Division I colleges that are public is expected to be 0.511 (\\(e^{-0.672}\\)) times the expenditure at private institutions, holding enrollment and region constant.↩︎\nNo. The interpretation of the intercept is not meaningful, because there cannot be a college with 0 students. We could center enrollment_th , so that the intercept represents colleges with some other value of enrollment (e.g., the mean enrollment in the data).↩︎\nWhen the enrollment increases by 50% (is multiplied by 1.5), the expenditure on basketball programs at NCAA Division I colleges is expected to increase by 2.43 (5.992 \\(\\times\\) log(1.5)) million dollars, on average, holding type and region constant.↩︎\nThe expenditure on basketball programs at public NCAA Division I colleges is expected to be 0.436 times the expenditure at private institutions, holding region and enrollment constant. Note, since the there is no transformation on the predictor type (or any categorical predictor), the interpretation is very similar to the interpretations in Section 9.2.↩︎\nPublic NCAA Division I institutions with 10,000 students are expected to spend 2548.716 (-1.284 + 0.255 * 10000) times the expenditure at private colleges with the same enrollment, holding region constant.↩︎",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variable transformations</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html",
    "href": "10-model-eval.html",
    "title": "10  Model selection",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#learning-goals",
    "href": "10-model-eval.html#learning-goals",
    "title": "10  Model selection",
    "section": "",
    "text": "Describe how model performance statistics are used to compare multiple models\nConduct model selection using stepwise procedures\nUse cross-validation to compare models\nImplement a model selection workflow using testing and training sets and cross validation\nImplement a model selection workflow in R",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#sec-spotify-intro",
    "href": "10-model-eval.html#sec-spotify-intro",
    "title": "10  Model selection",
    "section": "10.1 Introduction: Moody music",
    "text": "10.1 Introduction: Moody music\nThere has long been an interest in the connection of music to our focus and well-being. A 2022 article in Harvard Health Online discusses the benefits of music therapy for patients undergoing treatment for health conditions (Kubicek 2022), and a 2024 study by researchers at the Institute for Healthcare Policy and Innovation at the University of Michigan found that listening to music was associated with lower stress, improved mood, and other benefits among adults age 50 - 80 years old (Howell et al. 2024). In April 2025, The Guardian published an article about the “flurry” of new books on the relationship between music and health and well-being (MacGregor 2025).\nIn addition to the influence of music in general, researchers have examined the impacts of songs that have different moods. For example, Taruffi et al. (2017) found that sad music is associated with greater mind wandering and introspection compared to happy music. Campbell, Berezina, and Hew (2020) concluded happy music is associated with positive affects describing increased energy, while sad music is associated with decreases in both positive and some negative affects.\nGiven the potential differences in music’s impact based on its mood, the analysis in this chapter focuses on understanding variability in the moods of songs based on a variety of musical features. In particular, the analysis aims to identify the musical features that are associated with how positive or negative the song is. Songs that are positive tend to have happy and cheerful sounds. In contrast, songs that are negative tend to have sad and more solemn or angry sounds. When listening to a song, we are not given a measure of how positive or negative a song is. Thus, by understanding the musical features associated with the song’s mood, we can more easily identify songs that align with the mood we wish to evoke.\nWe will analyze a data set containing the musical features and other information from 3000 songs that have appeared on Spotify (spotify.com), a popular music streaming platform. The data is a subset of a data set curated and analyzed by data scientist Kaylin Pavlik in Pavlik (2019). It was featured as part of the TidyTuesday weekly data visualization challenge (Community 2024) in January 2020. The data were originally collected from Spotify’s application programming interface (API) that allows uses to access data about the songs on the streaming platform. Because musical artists often define themselves by having a distinct sound, we would expect that multiple songs by the same artist could have similar features. To avoid potential issues from violations in the independence condition, we randomly sampled one song from artists who had multiple songs in the data. Thus, there is only one song for each artist in the data that is analyzed in this chapter.\nThe data are available in spotify-songs-sample.csv. The variables below will be used in the analysis. The definitions are adapted from Spotify Track’s Audio Features documentation (Spotify for Developers n.d.).\nThe response variable is valence, defined as the following:\n\nMeasure ranging 0 to 1 that describes the song’s positiveness. Songs with high valence (values close to 1) sound more positive, and songs with lower valence (values close to 0). sound more negative. For example, a song that sounds happy and cheerful will have high valence, and a song that sounds sound or angry will have a low valence.\n\nThe predictors are the following:\n\n\nplaylist_genre: Playlist genre \ndanceability: Measure ranging 0 to 1 that describes how well suited the song is for dancing. Songs with values close to 0 are the least danceable, and those with values close to 1 are the most danceable.\nenergy: Measure ranging from 0 to 1 that describes the intensity and activity of a song. Songs with values close to 1 may feel fast, loud, and noisy, while songs with values close to 0 may feel quiet and slow.\nloudness: Measure typically ranging from -60 and 0 of the volume in decibels (dB). It is computed by averaging the volume across the entire song.\nmode: Description of the type of scale for the musical melody of a song. Possible values are “Major” and “Minor”.\nacousticness: Measure ranging from 0 to 1 of the confidence a song is acoustic.  Values close to 0 are low confidence the song is acoustic, and values close to 1 are high confidence the song is acoustic.\nliveness: Measure ranging from 0 to 1 detecting the presence of live audience in the song. Values close to 1 (0.8 or higher) indicate a strong likelihood the song track was recorded in front a live audience.\ntempo: Measure of the average pace of a song in beats per minute (BPM).\nduration_min: Length of a song in minutes\n\n\nThe goal of this analysis is to explain variability in valence using other musical features. This model can be used for prediction, but the primary objective is explanation.\n\n\n10.1.1 Exploratory data analysis\n\n\n\n\n\n\n\n\n\n\n\n\n(a) valence\n\n\n\n\n\n\n\n\n\n\n\n(b) playlist_genre\n\n\n\n\n\n\n\n\n\n\n\n(c) danceability\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) energy\n\n\n\n\n\n\n\n\n\n\n\n(e) loudness\n\n\n\n\n\n\n\n\n\n\n\n(f) mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) acousticness\n\n\n\n\n\n\n\n\n\n\n\n(h) liveness\n\n\n\n\n\n\n\n\n\n\n\n(i) tempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n(j) duration_min\n\n\n\n\n\n\n\nFigure 10.1: Univariate exploratory data analysis for Spotify data\n\n\n\n\nFigure 10.1 (a) shows the distribution of the response variable valence. The distribution is unimodal and approximately symmetric with many songs having values of valence around 0.5 (perhaps indicating a more neutral mood to the song). There is one song that has almost a completely negative mood (valence = 10^{-5}), and one song with an almost completely positive mood (valence = 0.991).\nIt is worth noting that valence is bounded between 0 and 1, meaning it is not valid to obtain values of valence outside that range. This is not much concern for the purposes of this analysis, because the purpose is primarily explanation. If we intend to build a model whose primary purpose is prediction, then we should more carefully examine any issues and potential transformations for the bounded response variable.\nGiven the number of potential predictors, we’ll leave observations about the distributions of the predictor variables to the reader.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) valence vs. playlist_genre\n\n\n\n\n\n\n\n\n\n\n\n(b) valence vs. danceability\n\n\n\n\n\n\n\n\n\n\n\n(c) valence vs. energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) valence vs. loudness\n\n\n\n\n\n\n\n\n\n\n\n(e) valence vs. mode\n\n\n\n\n\n\n\n\n\n\n\n(f) valence vs. acousticness\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) valence vs. instrumentalness\n\n\n\n\n\n\n\n\n\n\n\n(h) valence vs. liveness\n\n\n\n\n\n\n\n\n\n\n\n(i) valence vs. tempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n(j) valence vs. duration\n\n\n\n\n\n\n\nFigure 10.2: Bivariate exploratory data analysis for Spotify data. In each plot, the predictor is on the x-axis and the response, valence is on the y-axis.\n\n\n\n\n\nBased on Figure 10.2, which variable(s) do you think have an association with a song’s valence? Describe the direction of the relationship(s).1\n\nFigure 10.2 shows the plots for the bivariate exploratory data analysis. In general, it is more challenging than earlier analyses to see relationships between the valence and the musical features from the bivariate plots. This could be because these features are not actually associated with the valence, or the relationships result from a combination of features rather than individual relationships between each feature and the response. As with previous analyses, the exploratory data analysis is helpful for providing initial intuition about the data and analysis results, but it cannot be used to draw final conclusions. Thus, we choose a model to be used for interpretation and conclusions based on model fit and performance statistics.\nIn Section 9.5, we compared multiple models to determine which one was the best fit for the data. In these instances, we were comparing models that had the same variables but differed in whether transformations were applied to the response or predictors or whether interactions or higher-order terms were included. Often, however, we do not know which variables to include in the final model and need a way to choose from a set of potential predictors.\n\nWhen building a regression model, we aim to select the most parsimonious (the simplest) model possible. This means choosing the model with the fewest predictors, interaction terms, and transformations that still effectively summarizes the relationship between the response and predictor variables.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#sec-model-compare-stats",
    "href": "10-model-eval.html#sec-model-compare-stats",
    "title": "10  Model selection",
    "section": "10.2 Statistics for model comparison",
    "text": "10.2 Statistics for model comparison\n\nWe begin by discussing model fit and performance statistics used to compare models in the model selection process. In Section 4.7, we introduced \\(R^2\\) and RMSE in to evaluate model fit and predictive performance of regression models. In this section, we introduce new statistics Adjusted \\(R^2\\), AIC, and BIC, that are also used to compare models. In practice, we often use multiple statistics for model comparison, and sometimes these statistics can suggest we choose different models. Throughout the chapter, we will demonstrate how the analysis objective helps us determine which statistics to prioritize when doing model selection.\n\n10.2.1 Adjusted \\(R^2\\)\nRecall from Section 4.7 that \\(R^2\\) is the proportion of variability in the response variable that is explained by the model. This value is based on the process Analysis of Variance (ANOVA), in which the variability in the response variable is decomposed into variability that can be explained by the model and variability left unexplained in the residuals. Thus, \\(R^2\\) is computed as\n\\[\nR^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}\n\\tag{10.1}\\]\nwhere \\(SSM\\) is the model sum of squares (explained variability), \\(SSR\\) is the residual sum of squares (unexplained variability), and \\(SST\\) is the total variability in the response variable.\nThus far, we have used \\(R^2\\) to describe how well a model fits the data and to compare two models with the same number of predictors (for example, in Section 9.5). Now as we prepare to do model selection, we will often be comparing models that have different numbers of predictor variables, since the model selection process is often us answering the question “should we include predictor \\(X_j\\) in the model given the other predictors currently in the model?”. Recall from Section 7.3 that the coefficients for a least-squares regression model are the ones that minimize the sum of square residuals (\\(SSR\\)). Thus, if we estimate non-zero coefficients for new predictors, \\(SSR\\) will decrease (even if it’s just by a very small amount) by definition. Thus, we cannot use \\(R^2\\) to compare models with different numbers of predictors, because \\(R^2\\) tends to preference the model with more predictors. This can lead to overfitting, the case in which a model fits so closely to the sample data that it cannot be reliably used on new data.\nWhen comparing models with different numbers of predictors, we can use a related statistic, called Adjusted\\(R^2\\) that takes into account the number of predictor variables in the model.\nThe formula for Adjusted \\(R^2\\) is\n\\[\n\\text{Adjusted }R^2 = 1 - \\frac{SSR / (n - p - 1)}{SST / (n - 1)}\n\\tag{10.2}\\]\nwhere \\(SSR\\) and \\(SST\\) are defined as before, \\(n\\) is the number of observations in the data, and \\(p\\) is the number of predictors.\nThe numerator \\(SSR / (n - p - 1)\\), the residual sum of squares divided by the associated degrees of freedom, is a measure of the average variability about the regression line. We showed in Section 8.4.1 that the regression standard error, \\(\\hat{\\sigma} = \\sqrt{\\frac{SSR}{n - p -1}}\\), is the square root of this value. The variability about the regression line will be smaller for models that are a better fit for the data. Thus, \\(SSR / (n - p - 1)\\) will decrease only if adding new predictors makes the model a better fit for the data.\nThe denominator, \\(SST/(n-1)\\), is the sum of squares total divided by the associated degrees of freedom. This is the variance of the response variable, \\(s^2_{Y}\\) . It will remain the same for all candidate models, because the same sample data is used throughout the analysis.\nPutting what we know about the numerator and denominator together, the second term in the Equation 10.2 is smaller for models that better fit the data. Therefore, the Adjusted \\(R^2\\) will be greater for models that are a better fit. Adjusted \\(R^2\\) ranges from 0 to 1, so we choose models that have values close to 1.\n\nThe degrees of freedom (df) are the number of observations available to understand variability.\nWhen we fit a regression line with \\(p\\) predictors, we need a minimum of \\(p + 1\\) observations to fit the line, where \\(p+1\\) is the number of terms in the model (including the intercept). We are left with \\(n - (p + 1) = n - p - 1\\) observations to understand any variability in the residuals about the line. Thus, the degrees of freedom associated with SSR is \\(n -  p - 1\\).\n\nThe degrees of freedom associated with \\(SST\\) are the number of observations needed to understand variability about the mean of the distribution of the response variable. We need a minimum of one observation to compute a mean, so we are left with \\(n - 1\\) observations to understand variability about the mean.\n\nLet’s look at an example of two candidate models to explain a song’s valence. The first model includes the predictor danceability, and the second model includes the predictors danceability, mode, and liveness.\n\n\n\n\nTable 10.1: Comparing \\(R^2\\) and Adjusted \\(R^2\\) for two models for valence\n\n\n\n\n\n\nModel\nR-sq\nAdj. R-sq\n\n\n\n\nDanceability\n0.0957\n0.0954\n\n\nDanceability, Mode, Liveness\n0.0960\n0.0951\n\n\n\n\n\n\n\n\nFrom Table 10.1, we see the model fits are very similar. Though \\(R^2\\) is slightly higher for the model with the additional predictors mode and liveness, Adj. \\(R^2\\) is higher for the model that only includes danceability . This means that adding mode and liveness to the model dose not explain much additional variability in a song’s valence after accounting for danceability. Thus, we might prefer the model that only contains danceability given the goal of choosing of the simplest model possible that is still effective.\n\nBelow are guidelines for using \\(R^2\\) and Adjusted \\(R^2\\) in practice.\n\\(R^2\\)\n\nUse to describe the proportion of variability in the response that is explained by the model.\nDo not use to compare models with different numbers of predictors.\n\nAdjusted \\(R^2\\)\n\nUse for model selection and to compare models with different numbers of predictors.\nDo not use as a direct interpretation of the proportion of variability in the response that is explained by the model.\n\n\n\n\n10.2.2 Root Mean Square Error (RMSE)\nAnother statistic for model comparison is the Root Mean Square Error introduced in Section 4.7. The Root Mean Square Error (RMSE) is a measure of the average error in the model predictions. From Equation 10.3, we can see it is very similar to \\(\\hat{\\sigma}\\) (Equation 8.3). In fact, the difference between these values is negligible when the number of observations, \\(n\\), is large.\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{SSR}{n}}\n\\tag{10.3}\\]\nThe RMSE is typically used over other model comparison statistics, such as Adjusted \\(R^2\\), when the primary modeling objective is prediction.\n\nWe use RMSE to compare two models. In general, do we select the model with the higher or lower value of RMSE?2\n\nTable 10.2 shows the RMSE for the two models compared in Section 10.2.1.\n\n\n\n\nTable 10.2: RMSE for two candidate models of valence\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\nDanceability\n0.225\n\n\nDanceability, Mode, Liveness\n0.225\n\n\n\n\n\n\n\n\nRecall that RMSE has the same units and scale as the response variable, so we take these into consideration when interpreting RMSE and using it to compare models. Based on Table 10.2, the values of RMSE for the two models are equal down to three digits. From Section 10.1, valence takes values 0 to 1, so a difference less than 0.001 in the prediction error is very small. This is consistent with the very small difference in Adjusted \\(R^2\\) values in Table 10.1. In general, we select the model that has the smallest value of RMSE; however, in cases like this with negligible to no difference between the two models, we use our judgment as the data scientist, along with subject matter expertise and the analysis objectives to choose a model.\n\n\n10.2.3 AIC and BIC\nAdjusted \\(R^2\\) and RMSE are computed by quantifying the variability in the residuals (the unexplained variability). In addition to these statistics are two other statistics, AIC and BIC, that are based on another measure of model fit called a likelihood function. We will talk about the likelihood in more detail in Chapter 11, but in short, a likelihood function quantifies how plausible it is to obtain the observed data given a particular model is the true model of the relationship between the response and predictor variables.  Models with higher values for the likelihood are a better fit for the data compared to models with lower values. It is common to work with the natural log of the likelihood function, denoted \\(\\log L\\), for ease of calculations. The natural log is monotonic, so higher values of \\(\\log L\\) correspond to higher values of the likelihood and vice versa.\nThe Akaike’s Information Criterion (AIC) (Akaike 1974) and Schwarz’s Bayesian Information Criterion (BIC) (Schwarz 1978) are two measures of model fit based on the likelihood. Their equations are\n\\[\n\\begin{aligned}\nAIC &= -2\\log L + 2(p+1) \\\\\nBIC &= -2\\log L + \\log(n)(p+1)\n\\end{aligned}\n\\tag{10.4}\\]\nwhere \\(\\log L\\) is the log-likelihood, \\(n\\) is the number of observations, and \\(p+1\\) is the number of terms in the model.\nIn Equation 10.4, the first term for both AIC and BIC is \\(-2 \\log L\\). Because the log-likelihood is greater for better fitting models, this term decreases as the model fit improves. The second term in each equation is the “penalty” for the number of predictors in the model. As new predictors are added to a model, AIC and BIC are a balance between the decreasing first term and increasing second term. Where AIC and BIC differ is on the strength of the penalty for the number of terms in the model. When \\(n &gt; 8\\) (essentially all the time!), \\(\\log(n)(p+1) &gt; 2(p + 1)\\), so we say BIC has the “stronger penalty”. This results in BIC tending to preference models with fewer predictor variables than AIC.\nThe AIC and BIC for a single model is generally not meaningful in practice, so these values are used to compare two or more models. Small values of AIC and BIC correspond to better model fit, so we select the model that minimizes these statistics when comparing multiple models. In practice, a difference of 10 or greater in the AIC or BIC between models, is considered meaningful enough to choose one model over the other. Table 10.3 provides more specific guidelines for model comparison based on the differences in AIC and BIC.\n\n\n\nTable 10.3: Guidelines for comparing two models, \\(M_1\\) and \\(M_2\\), using AIC and BIC\n\n\n\n\n\n\n\n\n\n\nStrength of evidence in favor of \\(M_2\\)\n\\(AIC_{M_1} - AIC_{M_2}\\)3\n\\(BIC_{M_1} - BIC_{M_2}\\)4\n\n\n\n\nNone\n0 - 2\n0 - 2\n\n\nWeak\n2 - 7\n2 - 6\n\n\nStrong\n7 - 10\n6 - 10\n\n\nVery strong\n\\(&gt;\\) 10\n\\(&gt;\\) 10\n\n\n\n\n\n\nBelow are the values of AIC and BIC from the candidate models introduced in Section 10.2.1.\n\n\n\n\nTable 10.4: Comparing models using AIC and BIC\n\n\n\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nDanceability\n-427\n-409\n\n\nDanceability, Mode, Liveness\n-424\n-394\n\n\n\n\n\n\n\n\n\nUsing Table 12.8, which model would you choose based on AIC? Based on BIC? 5\n\nIn this example, the same model minimizes both AIC and BIC. In fact, based on the guidelines in Table 10.3, the difference in the values of BIC provides very strong support of the model that only includes danceability. Sometimes, a model may have the minimum value of AIC but not have the minimum value of BIC or vice versa. Thus, we can choose which statistic to prioritize based on the analysis objective. If the primary objective is explanation, the model selected by AIC is preferred, because AIC tends to choose the model with more variables that can help explain variability in the response. If the primary objective is prediction, the model selected by BIC is preferred, because BIC selects the model with fewer predictors, thus reducing the variability in the model predictions.\n\nModel comparison statistics\nBelow is a summary of the model comparison statistics:\n\n\n\nTable 10.5: : Model comparison statistics\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nEquation\nValues that indicate good fit\nPrioritize when analysis objective is…\n\n\n\n\nAdjusted \\(R^2\\)\n\\(\\frac{SSR/ (n - p - 1}{SST / (n - 1)}\\)\nHigher\nExplanation\n\n\nRMSE\n\\(\\sqrt{\\frac{SSR}{n}}\\)\nLower\nPrediction\n\n\nAIC\n\\(-2\\log L + 2(p+1)\\)\nLower\nExplanation\n\n\nBIC\n\\(-2\\log L + \\log(n)(p+1)\\)\nLower\nPrediction",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#sec-stepwise-selection",
    "href": "10-model-eval.html#sec-stepwise-selection",
    "title": "10  Model selection",
    "section": "10.3 Stepwise model selection",
    "text": "10.3 Stepwise model selection\nNow that we have a suite of model comparison statistics, let’s develop a workflow for using these statistics to choose a final model. In Section 10.2, we compared two models with a small number of predictor variables. From Section 10.1, we see there are nine potential predictor variables. Even with nine potential predictors, it would be time consuming and computationally intensive to test every possible combination of predictors.  In practice, we may even have as many as 20, 50, or many more predictors to consider for inclusion in a model. Therefore, we need methods that help us choose variables to include in the model in a systematic way. This is called model selection.\nIn particular, we will use a set of methods called stepwise selection, where were considering adding and/or removing one variable or a small subset of variables at a time (the steps).\n\n\n10.3.1 Forward selection\nThe first stepwise method is forward selection. In forward selection, we begin with the intercept-only model (also called the null model), and add variables one at a time until we can no longer find a model that is a better fit than the current one. We use the model comparison statistics from Section 10.2 to determine the variables to add to the model. The steps for forward selection are as follows:\nStart with the intercept-only model, then for each step:\n\nConsider all predictors that are not currently in the model one at a time. Compute the model fit statistic for each new candidate model.\nAdd the predictor that results in the best value of the statistic (minimum or maximum depending on the statistic).\nRepeat until adding new predictors no longer improves the model fit.\n\nLet’s look at an example of forward selection using AIC to compare the models. In the first step, the “current” model is the intercept-only model. We compute AIC for this model and all possible models with a single predictor. Table 10.6 shows the results in order of AIC.\n\n\n\n\nTable 10.6: Output from first step of forward selection. The model valence ~ 1 is the intercept-only model.\n\n\n\n\n\n\nCandidate model\nAIC\n\n\n\n\nvalence ~ danceability\n-427\n\n\nvalence ~ playlist_genre\n-343\n\n\nvalence ~ energy\n-179\n\n\nvalence ~ duration\n-146\n\n\nvalence ~ loudness\n-136\n\n\nvalence ~ liveness\n-127\n\n\nvalence ~ 1\n-127\n\n\nvalence ~ mode\n-125\n\n\nvalence ~ tempo\n-125\n\n\nvalence ~ acousticness\n-125\n\n\n\n\n\n\n\n\nBased on AIC, a model with danceability is the best model out of the current candidates. Therefore, as forward selection progresses, we limit the space of candidate models to those that include danceability as a predictor. Table 10.7 shows the next step of forward selection. We compute AIC for all two-predictor models that have danceability as one of the predictors, along with the current model that only has danceability as a predictor.\n\n\n\n\nTable 10.7: Output from second step of forward selection\n\n\n\n\n\n\nCandidate model\nAIC\n\n\n\n\nvalence ~ danceability + playlist_genre\n-709\n\n\nvalence ~ danceability + energy\n-508\n\n\nvalence ~ danceability + loudness\n-439\n\n\nvalence ~ danceability + duration\n-438\n\n\nvalence ~ danceability + tempo\n-432\n\n\nvalence ~ danceability\n-427\n\n\nvalence ~ danceability + mode\n-425\n\n\nvalence ~ danceability + liveness\n-425\n\n\nvalence ~ danceability + acousticness\n-425\n\n\n\n\n\n\n\n\nBased on Table 10.7, we choose the model that includes danceability and playlist_genre, because it is the model that minimizes AIC. Now, as we continue with forward selection, the space of potential models is narrowed to those that have danceability and playlist_genre as predictors. This process is continued until adding a new predictor to the model does not improve model fit based on AIC.\n\n\n\n\nTable 10.8: Model selected by forward selection using AIC as criteria\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.423\n0.046\n-9.16\n0.000\n\n\ndanceability\n0.639\n0.029\n21.88\n0.000\n\n\nplaylist_genrelatin\n0.203\n0.013\n15.90\n0.000\n\n\nplaylist_genrepop\n0.162\n0.013\n12.82\n0.000\n\n\nplaylist_genrer&b\n0.209\n0.014\n15.44\n0.000\n\n\nplaylist_genrerap\n0.135\n0.013\n10.57\n0.000\n\n\nplaylist_genrerock\n0.224\n0.014\n15.66\n0.000\n\n\nenergy\n0.439\n0.032\n13.85\n0.000\n\n\nacousticness\n0.101\n0.020\n5.12\n0.000\n\n\nduration_min\n-0.018\n0.004\n-4.80\n0.000\n\n\nloudness\n-0.007\n0.002\n-4.13\n0.000\n\n\ntempo\n0.000\n0.000\n3.29\n0.001\n\n\n\n\n\n\n\n\nTable 10.8 shows the model selected using forward selection with AIC as the selection criteria. Using this model selection approach, all predictors except liveness and mode were included in the final model. The AIC for this model is -954.989.\n\n\n10.3.2 Backward selection\nThe next stepwise method is backward selection. Backward selection begins with the full model that contains all predictors and removes variables one at a time until removing a predictor no longer improves the model fit. We use the model fit statistics from Section 10.2 to determine the variables to remove from the model. The steps for backward selection are as follows:\nStart with the full model, then for each step:\n\nRemove the predictors in the current model one at a time, and compute the model fit statistic of the resulting model.  Compute the model comparison statistic for each new candidate model.\nRemove the predictor that results in the best value of the statistic (minimum or maximum depending on the statistic).\nRepeat until removing a predictor no longer improves the model fit.\n\nLet’s look at an example of backward selection using AIC. In the first step, we fit all possible models in which a single predictor is removed and compute the AIC for these models, along with the AIC for the full model with all predictors . Table 10.9 shows the results in order of AIC.\n\n\n\n\nTable 10.9: Output from first step of backward selection (-var indicates which variable is removed)\n\n\n\n\n\n\nCandidate model\nAIC\n\n\n\n\n- mode\n-953\n\n\n- liveness\n-953\n\n\nFull model\n-951\n\n\n- tempo\n-942\n\n\n- loudness\n-936\n\n\n- duration\n-930\n\n\n- acousticness\n-927\n\n\n- energy\n-770\n\n\n- playlist_genre\n-577\n\n\n- danceability\n-512\n\n\n\n\n\n\n\n\nAfter the first step, we remove the predictor mode from the model. Thus, in the next step, the space of potential models is narrowed to those that do not include mode.\n\n\n\n\nTable 10.10: Output from second step of backward selection (-var indicates which variable is removed)\n\n\n\n\n\n\nCandidate model\nAIC\n\n\n\n\n- liveness\n-955\n\n\nRemove none\n-953\n\n\n- tempo\n-944\n\n\n- loudness\n-938\n\n\n- duration\n-932\n\n\n- acousticness\n-929\n\n\n- energy\n-772\n\n\n- playlist_genre\n-578\n\n\n- danceability\n-514\n\n\n\n\n\n\n\n\nIn the next step, we compute the AIC for the current model with only mode removed and the models with mode and one other predictor removed. After the second step in Table 10.10, we remove liveness and choose the model in which liveness is removed (along with mode that was removed in the previous step). We continue this process until removing a new predictor does not improve model fit. The final model selected by backward selection is in Table 10.11.\n\n\n\n\nTable 10.11: Model selected using backward selection with AIC as selection criteria\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.423\n0.046\n-9.16\n0.000\n\n\nplaylist_genrelatin\n0.203\n0.013\n15.90\n0.000\n\n\nplaylist_genrepop\n0.162\n0.013\n12.82\n0.000\n\n\nplaylist_genrer&b\n0.209\n0.014\n15.44\n0.000\n\n\nplaylist_genrerap\n0.135\n0.013\n10.57\n0.000\n\n\nplaylist_genrerock\n0.224\n0.014\n15.66\n0.000\n\n\ndanceability\n0.639\n0.029\n21.88\n0.000\n\n\nenergy\n0.439\n0.032\n13.85\n0.000\n\n\nloudness\n-0.007\n0.002\n-4.13\n0.000\n\n\nacousticness\n0.101\n0.020\n5.12\n0.000\n\n\ntempo\n0.000\n0.000\n3.29\n0.001\n\n\nduration_min\n-0.018\n0.004\n-4.80\n0.000",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#sec-training-testing",
    "href": "10-model-eval.html#sec-training-testing",
    "title": "10  Model selection",
    "section": "10.4 Training and testing sets",
    "text": "10.4 Training and testing sets\n\nNow let’s expand on the methods from the previous sections to develop a model building workflow. In Section 10.3, we used the entire sample data set to choose a model using stepwise selection methods. In fact, we have used the entire data set in all analyses done in the book thus far. It is appropriate statistically to use the entire data set for analysis and sometimes is the best choice given the analysis purpose. For example, using the entire sample may be a good analysis decision if the primary objective is explanation (particularly if \\(n\\) is small), because the model performance on new data is not necessarily important for the analysis goals. \nIn contrast, if the primary analysis objective is prediction, then we do not get all the information needed to evaluate model performance when we use all the data to build the model. Specifically, if all of the data is used in the model building process, then there is no data left to evaluate how well the model predicts for new observations. In general, we do not want to implement a model for widespread use without having some evaluation of how it performs on new data, particularly if there are serious implications for inaccurate model results, such as models used to inform business decisions or health diagnoses.\n\n10.4.1 Using training and testing sets\nOne way to evaluate the model performance on new data is to collect more data. This is often not a feasible solution in practice, because data collection is often time consuming and expensive. Another solution, then, is to split the sample data so that some observations are set aside to act as “new data”. Thus, we can split the sample data into two subsets, the training and testing set. The training set is the subset that is used to build the model, and the testing set is the subset that is used to evaluate how well the final model performs on new data. The testing set is only used to evaluate the final model. The final model is not changed based on the results from the testing set in order to prevent overfitting the model.\n\nThere are multiple ways to split the data into training and testing sets as shown in Figure 10.3.\n\n\n\n\n\n\n\n\n\n\n\n(a) Simple random sample\n\n\n\n\n\n\n\n\n\n\n\n(b) Stratified random sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Time ordered\n\n\n\n\n\n\n\nFigure 10.3: Splitting data into training and testing sets\n\n\n\nThe most common way to assign observations to the training or testing set is to take a simple random sample such that \\(Z\\%\\) of the observations are in the training set and \\((100 - Z)\\%\\) are in the testing set (Figure 10.3 (a)). A common split is 80% training and 20% testing; however, there is no magic number for the best split. The primary consideration for determining a split is the overall size of the sample data and ensuring there are enough observations in the training set to conduct inference and model evaluation. For example, if the data set is small (e.g., 100 observations), perhaps a split of 90% training and 10% testing may be preferred, so that as much data as possible is available for the model building process.\nAnother common approach is to take a stratified random sample (Figure 10.3 (b)). This may be the preferred approach is there are particular subgroups you want to ensure make up an exact proportion of the observations in the training and testing sets. For example, suppose \\(W\\%\\) of the population is part of a particular subgroup, and we want to ensure exactly \\(W\\%\\) of the observations in the training and testing sets are in that group. In this case, we stratify the data by group, then take \\(Z\\%\\) for training and \\((100 - Z)\\%\\) for testing within each subgroup. \nFinally, if time is an important component of the data, another option is to use the most recent time period as the testing set and all previous time periods for the training set (Figure 10.3 (c)). This is especially helpful if the model will be used for predicting future observations (called forecasting).\nOnce the data have been split into training and testing sets, we use the following workflow.\n\nConduct exploratory data analysis separately on the training and testing sets to evaluate how they compare to the full sample data and observe if they contain unusual observations, such as outliers, that may impact model results later on.\nPut the testing set aside. Using only the training set:\n\nFit and evaluate models using the statistics and model selection procedures, such as those introduced in Section 10.2 and Section 10.3 to select an initial main effects model.\nEvaluate model fit statistics, model conditions, and diagnostics for the selected model.\nConsider potential interaction terms, higher order terms, transformations, etc.\nRepeat steps ii - iii, as needed, until a final model is chosen.\n\nUse the testing set to compute predictions and evaluate how well the final model performs on new data using RMSE or \\(R^2\\). \nUse the entire sample data (training and testing set) to refit the model selected in Step 2 and to get the final estimated model coefficients and statistics. These estimated coefficients should be similar to the statistics from Step 2. We use the full sample data to estimate the coefficients for the final model at the end, so that the coefficients and associated inferential statistics are estimated using all available data.\nUse the model for inferential conclusions and/or prediction based on the analysis objective.\n\n\n\n10.4.2 Model building workflow example\nHere we apply this workflow to the Spotify data. First, we split the data into the commonly used 80% training and 20% testing sets. The original sample data has 3000 observations, so the training set has 2400 observations and the testing set has 600 observations. Figure 10.4 shows a comparison of the distribution of valence in the full sample data, training set, and testing set. The distributions have a similar shape and are all centered around 0.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Full sample data\n\n\n\n\n\n\n\n\n\n\n\n(b) Training set\n\n\n\n\n\n\n\n\n\n\n\n(c) Testing set\n\n\n\n\n\n\n\nFigure 10.4: Distribution of valence in full sample data, training set, and testing set\n\n\n\nFor now we put the testing set aside and use the training set to build a model. Because the primary analysis objective in Section 10.1 is explanation, we’ll use forward selection with AIC as the selection criteria. Table 10.12 is the model selected by this process.\n\n\n\n\nTable 10.12: Model selected by forward selection with AIC as selection criteria using the training set\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.424\n0.051\n-8.28\n0.000\n\n\ndanceability\n0.629\n0.033\n19.25\n0.000\n\n\nplaylist_genrelatin\n0.215\n0.014\n15.12\n0.000\n\n\nplaylist_genrepop\n0.172\n0.014\n12.17\n0.000\n\n\nplaylist_genrer&b\n0.218\n0.015\n14.61\n0.000\n\n\nplaylist_genrerap\n0.144\n0.014\n10.14\n0.000\n\n\nplaylist_genrerock\n0.230\n0.016\n14.65\n0.000\n\n\nenergy\n0.451\n0.036\n12.55\n0.000\n\n\nacousticness\n0.107\n0.022\n4.94\n0.000\n\n\nduration_min\n-0.019\n0.004\n-4.56\n0.000\n\n\nloudness\n-0.008\n0.002\n-4.10\n0.000\n\n\ntempo\n0.000\n0.000\n2.46\n0.014\n\n\n\n\n\n\n\n\nNext, we check the model conditions and diagnostics from Section 8.5 for the selected model.\n\nWhich data set is used to check model conditions - full sample data, training set, testing set?6\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Residuals vs. predicted\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n(c) Cook’s Distance\n\n\n\n\n\n\n\nFigure 10.5: Model conditions and diagnostics for model in Table 10.12\n\n\n\nFrom Figure 10.5, the linearity, equal variance, and normality conditions are satisfied and there are no influential observations based on Cook’s Distance. From the description in Section 10.1, we know that there is only one song for a given artist in the data, so we can reasonably conclude the independence condition is met as well. Table 10.13 shows the RMSE and \\(R^2\\) on the training set. We prioritize \\(R^2\\), because the primary analysis objective is explanation.\n\n\n\n\nTable 10.13: Model performance statistics for model in Table 10.12\n\n\n\n\n\n\nR-sq\nRMSE\n\n\n\n\n0.257\n0.204\n\n\n\n\n\n\n\n\n\nThis is a good point in the analysis to consider potential interaction terms, variable transformations, or higher order polynomial terms if there is indication from the exploratory data analysis that such terms would improve model fit. As we try new terms, we use RMSE, \\(R^2\\), and inferential statistics to evaluate whether to include them in the model. We will proceed with the model in Table 10.12 as the final model for the purposes of this example.\nNow that we have selected the final model, we use the model to compute RMSE, \\(R^2\\), and predictions on the testing set. RMSE and \\(R^2\\) are then used to evaluate how well the model performs on the testing set.\n\n\n\n\nTable 10.14: Model performance statistics for the testing and training sets\n\n\n\n\n\n\nSample,\nR-sq\nRMSE\n\n\n\n\nTraining\n0.257\n0.204\n\n\nTesting\n0.204\n0.211\n\n\n\n\n\n\n\n\nTable 10.14 shows the values of \\(R^2\\) and RMSE for the training and testing set. When we fit a model, the goal is to adequately capture the trends in the data without capturing the trends so specifically that the can’t be used on new data. In general, we expect the model performance to be better (lower RMSE, higher \\(R^2\\)) for the training set, because it was used to fit the model. When we compare these values, we want the results from the testing set to be similar, even if slightly worse, as the training set. When the results are similar, this means that the model can be generalized and reliably applied to new observations in the population. As we implement the model and use it to predict for new observations, we can expect a similar model performance as we have observed in the analysis. In contrast, if the model performs much better on the training set compared to the testing set, then it is indication of overfitting.\n\nThe goal when building a model is to capture the trends in the data while still making something that can applied to new data. When a model is overfit, a model will perform very well on the data used to fit it but perform very poorly on any new data. Overfitting generally occurs when there are complex interaction terms between three or more variables or polynomial terms that are cubic or high-order.\n\nAvoid overfitting by preferencing simpler models that adequately capture the trends in the data over those that fit very closely to the training set used to build them.\n\nRemember, model building is like making a suit that can generally fit everyone around a given size, not making a custom-tailored suit that can only fit one person perfectly. \n\nWe note in Table 10.14 the model performed better on the training data, but the performance statistics are similar for the training and testing sets. This means that we have not overfit the model to the training set and thus can feel some level of confidence that we understand how the model will perform on new data.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#cross-validation",
    "href": "10-model-eval.html#cross-validation",
    "title": "10  Model selection",
    "section": "10.5 Cross validation",
    "text": "10.5 Cross validation\nThe testing set gives us information about the model performance on new data; however, using a single testing set can make it difficult to know how much the results are impacted by nuances in the data, such as the presence of outliers. Additionally, because the testing set is only used to evaluate the model performance once the model building process is complete, the information about the model’s performance on new data does not inform the final model selection. Having multiple sets for testing during the model selection process would help address these issues.\n\n10.5.1 Resampling\nWe could collect more data to make additional testing sets, however, that is often not feasible in practice given time and cost constraints. Therefore, we will use resampling, the process of sampling from the data to generate “new” data sets.  We have discussed two resampling methods earlier in the book, bootstrapping and permutation tests in Section 8.3.\nBecause the goal is to generate multiple testing sets, we will resample in such a way that a different subset of the data is used to build the model and a different subset is used to evaluate the model’s performance on new data in each iteration. We only use the training set for resampling, so we can still reserve the testing set to evaluate the final model.  For each iteration of resampling from the training set, the subset of the training set used to build the model is called the analysis set, and the subset of the training set used to evaluate the model is the assessment set. We use these terms from Kuhn and Silge (2022) to distinguish them from the training and testing sets. Figure 10.6 shows the resampling process for \\(k\\) iterations.\n\n\n\n\n\n\nFigure 10.6: Illustration of resampling for k iterations. Solid blue = Analysis set, Striped green = Assessment set\n\n\n\n\n\n10.5.2 Cross validation\nCross validation (also called k-fold cross validation) is a resampling method that splits the data into \\(k\\) subsets called folds. Commonly used choices for the number of folds are \\(k = 5 \\text{ or }10\\). In each iteration, \\(k - 1\\) of the folds make up the analysis set, and the remaining fold makes up the assessment set. At the end of the \\(k\\) iterations, the model’s overall performance is computed as an average of the performance statistic across all iterations. For example, if we use RMSE to evaluate performance, the model’s overall performance is the average RMSE across the \\(k\\) iterations. Figure 10.7 illustrates the folds, analysis set, and assessment sets for a 5-fold cross validation.\n\n\n\n\n\n\n\nFigure 10.7: Illustration of 5-fold cross validation. Solid blue = Analysis set, Striped green = Assessment set\n\n\n\nBecause cross validation is only conducted on the training set, a measure of how the model performs on new data (the assessment set at each iteration) can be used to compare potential models and throughout the model building process. As before, the testing set is reserved for providing a final evaluation of the performance of the final model.\nBelow is the outline of a model building workflow workflow using cross-validation.\n\nSplit the data into training and testing sets. Set the testing set to the side!\nUse simple random sampling to split the training set into \\(k\\) folds.\nFor \\(k\\) iterations\n\nUse \\(k-1\\) folds in the analysis set to fit a candidate model. Compute model performance statistics such as \\(\\text{Adj. } R^2\\), AIC, and BIC.\nUse the remaining fold as the analysis set to evaluate the model’s ability to predict for new observations. Compute statistics such as \\(R^2\\) and RMSE to evaluate model performance.\nRepeat steps (i) and (ii) \\(k\\) times, rotating which fold is the assessment set at each iteration.\n\nCompute the average model fit or performance statistic computed in Step 3 across the \\(k\\) folds (e.g., average AIC or average RMSE) as the overall measure of the model performance.\nRepeat Steps 3 and 4 for each candidate model. Use the overall performance statistic from Step 4 to select a final model.\nOnce the final model is selected, use the entire training set to refit the selected model.\nUse the model fit in Step 6 to compute predictions on the testing set. Compute model performance statistics as a final measure of the model’s predictive performance.\nUse the entire data set (training and testing set) to refit the selected model and get the final model coefficients.\nUse the model from Step 8 for interpretation, inference and prediction.\n\nWhile doing cross validation, we are not necessarily concerned with the estimated regression coefficients or their interpretations. In fact, by the end of of Step 3, there will be \\(k\\) similar (but different!) estimated values for each model coefficient. The purpose of cross validation is to evaluate how well the model predicts for data it has not seen before. It is at the last step of the process that we look closely at the model coefficients, their interpretations, and conclusions about the relationship between the response and predictor variables.\n\nThe most commonly used values for \\(k\\), the number of folds, are 5 and 10. There are few points to consider when choosing \\(k\\).\nUsing a smaller value of \\(k\\)\n\nUse a smaller number of folds there are few observations in the training set. Only one fold is part of the assessment set used to evaluate the model performance at each iteration, so if \\(k\\) is large, there will be very few observations in the assessment set.\nSmall values of \\(k\\) tend to result in more biased estimates of the model performance statistic, for example RMSE. Biased estimates are far from the true (yet unknown!) population statistic, so they can result in an unreliable view of the model performance.\n\n\nUsing a larger value of \\(k\\)\n\nCross-validation can be computationally intensive. Larger \\(k\\) means more resampling and model refitting is required, which can result in long computational times, often for little added value.\nLarge values of \\(k\\) result in more variability in the estimated model performance statistic, for example RMSE. This means at each iteration may produce very different estimates of the model performance, resulting in an unreliable evaluation of the model performance.\n\n\nChoosing \\(k = 5\\) or \\(k = 10\\) produces model performance statistics that have little bias and variability, while also reducing the computational intensity. See Chapter 3 of Kuhn and Johnson (2019) and Chapter 5 of James et al. (2021) for more discussion about choosing \\(k\\).\n\n\n\n10.5.3 Example: Cross validation workflow\nHere we illustrate the cross validation workflow to fit the full model using all predictors in Section 10.1 to explain variability in the valence of the songs on Spotify. Recall from Section 10.4 the training set has 2400 observations. The training set is split into five folds, with each having 480 observations.\n\nBased on Figure 10.7, which fold(s) make up the analysis set and which make up the assessment set in the first iteration?\nFor the first iteration of cross validation on the Spotify training set, how many observations will be in the analysis set? How many in the assessment set?7\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Fold 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Fold 2\n\n\n\n\n\n\n\n\n\n\n\n(c) Fold 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Fold 4\n\n\n\n\n\n\n\n\n\n\n\n(e) Fold 5\n\n\n\n\n\n\n\nFigure 10.8: Distribution of valence for 5 folds\n\n\n\nFigure 10.8 shows the distribution of the the response variable valence for each fold. As we expect from random sampling, the distribution of valence is very similar across the folds.\nNext, we fit the full model using the analysis set in each of the five iterations. Recall from Figure 10.7, that in each iteration the analysis set consists of four folds and one fold is held out as the assessment set. For example, from Figure 10.7, in Iteration 5, Folds 1 - 4 are the analysis set and Fold 5 is the assessment set.\n\n\n\n\nTable 10.15: Model coefficients from 5-fold cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\n\n\n\n\n(Intercept)\n-0.432\n-0.408\n-0.484\n-0.407\n-0.420\n\n\nplaylist_genrelatin\n0.216\n0.229\n0.199\n0.216\n0.221\n\n\nplaylist_genrepop\n0.167\n0.178\n0.151\n0.181\n0.187\n\n\nplaylist_genrer&b\n0.215\n0.222\n0.205\n0.224\n0.228\n\n\nplaylist_genrerap\n0.150\n0.146\n0.130\n0.153\n0.146\n\n\nplaylist_genrerock\n0.227\n0.226\n0.220\n0.245\n0.236\n\n\ndanceability\n0.656\n0.596\n0.675\n0.608\n0.630\n\n\nenergy\n0.415\n0.435\n0.475\n0.456\n0.465\n\n\nloudness\n-0.007\n-0.008\n-0.009\n-0.008\n-0.008\n\n\nmodeMinor\n-0.009\n0.002\n-0.002\n-0.007\n0.001\n\n\nacousticness\n0.108\n0.102\n0.127\n0.112\n0.090\n\n\nliveness\n0.055\n0.020\n-0.008\n0.018\n0.042\n\n\ntempo\n0.001\n0.000\n0.000\n0.000\n0.000\n\n\nduration_min\n-0.018\n-0.016\n-0.013\n-0.021\n-0.025\n\n\n\n\n\n\n\n\nTable 10.15 shows the estimated model coefficients from each iteration of cross validation. The coefficients are similar but not exactly the same across each iteration, because the analysis set used to fit the models at each iteration are similar but not exactly the same. The coefficients are displayed here to provide a glimpse into the process, but in practice we are not concerned with the estimates of the model coefficients at this point in the analysis. Instead, we are most concerned with the model performance.\n\n\n\n\nTable 10.16: Model performance statistics on the assessment sets in 5-fold cross validation\n\n\n\n\n\n\nFold\nR2\nRMSE\n\n\n\n\nFold1\n0.202\n0.256\n\n\nFold2\n0.212\n0.262\n\n\nFold3\n0.209\n0.226\n\n\nFold4\n0.207\n0.245\n\n\nFold5\n0.201\n0.233\n\n\n\n\n\n\n\n\nTable 10.16 shows \\(R^2\\) and RMSE computed from the assessment set in each iteration, measuring the model’s performance on new data. As with the model coefficients, there is some variability in these statistics, but they show a similar model fit. In practice, we are most interested in the average values across all folds. The average \\(R^2\\) is 0.244 and the average RMSE is 0.206.\nIn practice, these values are most helpful when comparing multiple candidate models during the model selection process. Let’s compare the results of cross-validation for the full model to the results of cross-validation for the model selected by forward stepwise selection in Table 10.8. Note that we will use the same folds for both models, so that we are comparing the models’ performances on the same analysis and assessment sets.\n\n\n\n\nTable 10.17: 5-fold cross validation results for full model and model selected by forward selection in Section 10.3.1\n\n\n\n\n\n\nModel\nRMSE\nR-sq\n\n\n\n\nFull model\n0.206\n0.244\n\n\nForward selection\n0.206\n0.246\n\n\n\n\n\n\n\n\nFrom Table 10.17, the results from 5-fold cross validation are very similar for the full model and the model selected by forward selection. Thus, there is no clear preferred model based on these model performance results. At this point, we would rely on our judgment as data scientists and the original analysis objective to choose between these models. Given the aim to select the simplest model, we may choose the model selected by forward selection over the full model.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#model-evaluation-and-selection-in-r",
    "href": "10-model-eval.html#model-evaluation-and-selection-in-r",
    "title": "10  Model selection",
    "section": "10.6 Model evaluation and selection in R",
    "text": "10.6 Model evaluation and selection in R\n\n10.6.1 Model comparison statistics in R\nThe glance() function in the broom package (Robinson, Hayes, and Couch 2025) is used to compute Adjusted \\(R^2\\), AIC, and BIC. It also includes other model fit statistics, such as log-likelihood used to compute AIC and BIC.\n\nspotify_model &lt;- lm(valence ~ danceability + mode + liveness, \n                    data = spotify)\n\nglance(spotify_model) |&gt; \n  select(adj.r.squared, AIC, BIC)\n\n# A tibble: 1 × 3\n  adj.r.squared   AIC   BIC\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1        0.0951 -424. -394.\n\n\nThe Root Mean Square Error is obtained by first computing the predicted values, then using rmse() from the yardstick package (Kuhn, Vaughan, and Hvitfeldt 2025). The code below uses augment() to compute the predicted values.\n\nspotify_model_aug &lt;- augment(spotify_model)\n\nrmse(spotify_model_aug, truth = valence, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.225\n\n\n\n\n10.6.2 Stepwise model selection in R\nForward and backward selection are conducted using the step() in the stats base R package. For both selection methods, we define the intercept-only model (called the null_model) and the model with all candidate predictors (called the full_model). The code below shows how to specify each model for the Spotify data analysis using various features of a song to predict valence.\n\nnull_model &lt;- lm(valence ~ 1, data = spotify)\nfull_model &lt;- lm(valence ~  playlist_genre + danceability + energy +\n                   loudness + mode +  acousticness + liveness + \n                   tempo + duration_min, \n                         data = spotify)\n\nNext is the code to conduct forward selection. The final model chosen by forward selection is saved as forward_model.\n\nforward_model &lt;- step(null_model,\n                      scope = formula(full_model),\n                      direction = \"forward\", \n                      trace = 0, \n                      k = 2)\n\nThe argument trace = 0 will suppress all the intermediate results from the stepwise process. Use trace = 1 to display output from each step of the forward selection.\nThe step function uses AIC as the selection criteria by default, as shown by k = 2. Use the argument k = log(nrow(my_data)) to use BIC as the model selection criteria. For example, we use k = log(nrow(spotify)) to use BIC as the selection criteria for this analysis.\nThe code for backward selection is similar to the code for forward selection. Primary differences are how the scope are defined and the direction.\n\nbackward_selection &lt;- step(full_model,\n                           scope = list(lower = formula(null_model), \n                                        upper = formula(full_model)),\n                           direction = \"backward\",\n                           trace = 0, \n                           k = 2)\n\nThe step function can only use AIC or BIC as the model selection criteria. An alternative stepwise selection function is regsubsets() in the leaps R package (Miller 2024). It can use a variety of model selection criteria, such as Adjusted \\(R^2\\), and residual sum of squares (SSR, which is proportional to RMSE).\n\n\n\n10.6.3 Training and testing sets in R\nData can be split into training and testing sets using functions from the resample package (Frick et al. 2025).\nThe function initial_split() is used to randomly assign observations to the training and testing set. The argument prop = defines the proportion of observations that are assigned to the training set. Because this is a random process, use set.seed() for reproducibility to ensure the same assignment each time the code is run.\nThe code below randomly assigns 80% of the observations to the training set and 20% of the observations to the testing set.\n\nset.seed(12345)\n\nspotify_split &lt;- initial_split(spotify, prop = 0.8)\nspotify_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2400/600/3000&gt;\n\n\nThe data in the training and testing sets can be extracted using the functions training() and testing() respectively. We save the sets under the object names spotify_train and spotify_test and will call these objects in subsequent steps of the analysis.\n\n\n10.6.4 Cross validation in R\nWe conduct cross validation using functions from the resample and workflows (Vaughan and Couch 2025) packages. The folds are defined using the vfold_cv() in the resamples package. It uses the argument v = to define the number of folds. Below is the code to create the folds for 5-fold cross validation. Note that we use spotify_train, the training set.\n\nset.seed(12345)\nfolds &lt;- vfold_cv(spotify_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits             id   \n  &lt;list&gt;             &lt;chr&gt;\n1 &lt;split [1920/480]&gt; Fold1\n2 &lt;split [1920/480]&gt; Fold2\n3 &lt;split [1920/480]&gt; Fold3\n4 &lt;split [1920/480]&gt; Fold4\n5 &lt;split [1920/480]&gt; Fold5\n\n\nThe output indicates that for each fold, there are 1920 observations in the analysis set and 480 observations in the assessment set. If we wish to examine the data in each fold more closely (for example, as in Figure 10.8), we are able to extract the data contained in each fold. Below is code to extract the data in the analysis and assessment sets for Fold 1.\n\nfold1_analysis &lt;- folds$splits[[1]] |&gt; analysis()\n\nfold1_assessment &lt;- folds$splits[[1]] |&gt; assessment()\n\nOnce the folds are defined, we use the workflow(), add_model(), and add_formula() functions from the workflows package to specify the model that will be fit in each iteration of cross validation. The code below defines the workflow for fitting the full model from the Spotify data.\n\n1spotify_workflow &lt;- workflow() |&gt;\n2  add_model(linear_reg()) |&gt;\n  add_formula(valence ~ playlist_genre + danceability + energy + \n                loudness + mode + acousticness + liveness + \n3                tempo + duration_min)\n\n\n1\n\nInitiate a modeling workflow for cross validation and save it as spotify_workflow.\n\n2\n\nSpecify model is in the form of a multiple linear regression model (see Equation 8.1).\n\n3\n\nSpecify the response and predictor variables in the model.\n\n\n\n\nNext, we use fit_resamples() from the rsamples package and collect_metrics() from the tune package (Kuhn 2025) to fit the specified model to each of the 5 analysis sets and obtain the performance metrics.\n\n1spotify_cv &lt;- spotify_workflow |&gt;\n2  fit_resamples(resamples = folds)\n\n\n3collect_metrics(spotify_cv, summarize = TRUE)\n\n\n1\n\nIndicate we will use the model specification in spotify_workflow. Save the models fit in cross validation as spotify_cv.\n\n2\n\nFit the specified model to the analysis set in each fold saved in the object fold.\n\n3\n\nCompute the average \\(R^2\\) and RMSE across all iterations. These are the default performance metrics for linear models.\n\n\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   0.207     5 0.00104 pre0_mod0_post0\n2 rsq     standard   0.237     5 0.0113  pre0_mod0_post0\n\n\nThe default for collect_metrics() is to produce the average performance statistics, as indicated by the argument summarize = TRUE . We can use summarize = FALSE to print the performance statistics for each iteration.\n\ncollect_metrics(spotify_cv, summarize = FALSE)\n\n# A tibble: 10 × 5\n  id    .metric .estimator .estimate .config        \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Fold1 rmse    standard       0.207 pre0_mod0_post0\n2 Fold1 rsq     standard       0.230 pre0_mod0_post0\n3 Fold2 rmse    standard       0.206 pre0_mod0_post0\n4 Fold2 rsq     standard       0.274 pre0_mod0_post0\n5 Fold3 rmse    standard       0.210 pre0_mod0_post0\n6 Fold3 rsq     standard       0.204 pre0_mod0_post0\n# ℹ 4 more rows\n\n\nThe full code workflow for 5-fold cross validation is below.\n\nset.seed(12345)\n\n# define the folds\nfolds &lt;- vfold_cv(spotify_train, v = 5)\n\n# specify the model \nspotify_workflow &lt;- workflow() |&gt; \n  add_model(linear_reg()) |&gt; \n  add_formula(valence ~ playlist_genre + danceability + energy + \n                loudness + mode + acousticness + liveness + \n                tempo + duration_min) \n\n# fit the model to the analysis set in each fold\nspotify_cv &lt;- spotify_workflow |&gt; \n  fit_resamples(resamples = folds)\n\n# obtain performance statistics for each iteration\ncollect_metrics(spotify_cv, summarize = TRUE)\n\nThere are many ways to customize cross validation, including the sampling process used to define the folds and the performance statistics computed at each iteration. We point interested readers to Chapter 10 of Kuhn and Silge (2022) for more details on customizing cross validation in R.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#summary",
    "href": "10-model-eval.html#summary",
    "title": "10  Model selection",
    "section": "10.7 Summary",
    "text": "10.7 Summary\nIn this chapter, we reviewed and introduced statistics for measuring model fit and predictive performance. We discussed how to determine which statistics to prioritize based on the primary modeling objective and introduced stepwise methods for model selection. We introduced a model selection workflow using training and testing sets, and expanded this with cross validation. Lastly, we described how to implement a complete model selection workflow and showed how to do so in R.\nHere we introduced the basics of cross-validation, but there are many iterations of cross validation, including Leave One Out Cross Validation (LOOCV) and Monte Carlo cross validation. There are also extensions of the training and testing set, such as validation set that can be used alongside the training set in the model building process. We point readers to Chapters 10 and 11 of Kuhn and Silge (2022) and Chapter 3 of Kuhn and Johnson (2019) as starting points for more advanced discussion of these topics.\nSo far, we have discussed linear regression models for quantitative response variables. In Chapter 11, we look beyond quantitative response variables and introduce logistic regression, models for categorical response variables.\n\n\n\n\nAkaike, H. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nBurnham, Kenneth P, and David R Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. Springer.\n\n\nCampbell, Elizabeth, Elizaveta Berezina, and Gill Hew. 2020. “The Effects of Music Induction on Mood and Affect in an Asian Context.” Psychology of Music 49 (5): 1132–44. https://doi.org/10.1177/0305735620928578.\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and Hadley Wickham. 2025. “Rsample: General Resampling Infrastructure.” https://doi.org/10.32614/CRAN.package.rsample.\n\n\nHowell, Joel, Dianne Singer, Erica Solway, Nicholas Box, Scott Roberts, Lauren Hutchens, Emily Smith, and Jeffrey Kullgren. 2024. “The Sound of Music.” University of Michigan National Poll on Healthy Aging. https://doi.org/10.7302/22174.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer.\n\n\nKass, Robert E, and Adrian E Raftery. 1995. “Bayes Factors.” Journal of the American Statistical Association 90 (430): 773–95.\n\n\nKubicek, Lorrie. 2022. “Can Music Improve Our Health and Quality of Life?” July 25, 2022. https://www.health.harvard.edu/blog/can-music-improve-our-health-and-quality-of-life-202207252786.\n\n\nKuhn, Max. 2025. “Tune: Tidy Tuning Tools.” https://doi.org/10.32614/CRAN.package.tune.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman; Hall/CRC.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \" O’Reilly Media, Inc.\".\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2025. “Yardstick: Tidy Characterizations of Model Performance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\nMacGregor, Emily. 2025. “Music Can Lift Mood, Foster Community and Even Rewire Brains – but Does It Need to Have a Purpose?” April 14, 2025. https://www.theguardian.com/music/2025/apr/14/mood-music-radio-3-unwind-emily-macgregor.\n\n\nMiller, Thomas Lumley based on Fortran code by Alan. 2024. “Leaps: Regression Subset Selection.” https://doi.org/10.32614/CRAN.package.leaps.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using Spotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2025. “Broom: Convert Statistical Objects into Tidy Tibbles.” https://doi.org/10.32614/CRAN.package.broom.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a Model.” The Annals of Statistics, 461–64.\n\n\nSpotify for Developers. n.d. “Get Audio Features (Spotify Web API Reference).” Online Documentation. https://developer.spotify.com/documentation/web-api/reference/get-audio-features.\n\n\nTaruffi, Liila, Corinna Pehrs, Stavros Skouras, and Stefan Koelsch. 2017. “Effects of Sad and Happy Music on Mind-Wandering and the Default Mode Network.” Scientific Reports 7 (1): 14396. https://doi.org/10.1038/s41598-017-14849-0.\n\n\nVaughan, Davis, and Simon Couch. 2025. “Workflows: Modeling Workflows.” https://doi.org/10.32614/CRAN.package.workflows.",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "10-model-eval.html#footnotes",
    "href": "10-model-eval.html#footnotes",
    "title": "10  Model selection",
    "section": "",
    "text": "Answers may vary. Danceability and Energy appear to be the most strongly associated with valence based the bivariate graphs. Both show a positive relationship - more danceable songs tend to be more positive and more high energy songs to be more positive.↩︎\nWe generally select the model with lower RMSE, because it means the model is producing more accurate predictions, on average.↩︎\nBased on (Kass and Raftery 1995)↩︎\nBased on (Burnham and Anderson 2002, pg. 70)↩︎\nWe would choose the model with the single predictor danceability based on AIC, because it has the smaller value. The same is true for BIC. Based on the guidelines in Table 10.3, AIC gives weak evidence and BIC gives very strong evidence in favor of this model.↩︎\nThe training set. The training set is used for the entire model building process.↩︎\nBased on Figure 10.7, Folds 2 - 5 are the analysis set and Fold 1 is the assessment set in Iteration 1. There will be 1920 observations in the analysis set (4 folds) and 480 observations in the assessment set (1 fold).↩︎",
    "crumbs": [
      "Part 3: Multiple linear regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model selection</span>"
    ]
  },
  {
    "objectID": "11-logistic.html",
    "href": "11-logistic.html",
    "title": "11  Logistic regression",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#learning-goals",
    "href": "11-logistic.html#learning-goals",
    "title": "11  Logistic regression",
    "section": "",
    "text": "Identify whether linear or logistic regression is appropriate for a given analysis\nCalculate and interpret probabilities, odds, and odds ratios\nInterpret the coefficients of a logistic regression model\nConduct simulation-based and theory-based inference for the coefficients of a logistic regression model\nEvaluate model conditions for logistic regression\nConduct logistic regression using R",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-intro-gss",
    "href": "11-logistic.html#sec-logistic-intro-gss",
    "title": "11  Logistic regression",
    "section": "11.1 Introduction: Comfort with driverless cars",
    "text": "11.1 Introduction: Comfort with driverless cars\nArtificial Intelligence (AI) has quickly become ubiquitous, with generative AI tools such as ChatGPT being widely adopted in the workplace, school, and throughout every day life. One area in particular where AI is continuing to grow is in self-driving cars. The notion of riding in car that drives around as an individual reads a book has increasingly become a reality, with some cities already having self-driving taxis on the road. Waymo, a company that deploys a fleet self-driving taxis (called “robotaxis”), describes its robotaxi as “the world’s most experienced driver.” and is operating in five United States cities with plans to expand to two more at the time of this writing. (waymo.com). Additionally, Amazon has launched a robotaxi service in Las Vegas called Zoox (Liedtke 2025).\nDespite the growth of self-driving cars, there are still complex moral questions Vemoori (2024) and the general readiness individuals have with this new technology and driving experience. David LI, co-founder of Hesai, a remote-sensing technology company, said that the adoption of self-driving cars are not just a question about technology development, but “it is a social question, it is a regulatory question, it is a political question” (White 2025). Therefore, it is important to understand how people, some of who may already will one day purchase a self-driving car, view this technological phenomenon.\n\nThe goal of the analysis in this chapter is to use data from the 2024 General Social Survey (GSS) to explore individual characteristics associated with one’s comfort with self-driving cars.\n\nThe GSS, administered by the National Science Foundation and is administered by National Opinion Research Center (NORC) at the University of Chicago, began in 1972 and is conducted about every two years to measure “trends in opinions, attitudes, and behaviors” among adults age 18 and older in the United States (Davern et al. 2025). Households are randomly selected using multistage cluster sample based on data from the United States Census.\nRespondents in the 2024 survey were part of the second year of an experiment studying survey administration and randomly split into two groups. The first group had the opportunity to complete the survey through an in-person interview (format traditionally used) and non-respondents were offered the online survey. The second group had the opportunity to take the online survey first, then non-respondents were offered the in-person interview. \nThe data are in gss24-ai.csv. The data were obtained through the gssr R package (Healy 2023). Questions about opinions on special topics such as technology are not given to every respondent, so the data in this analysis includes responses from 1521 adults who were asked about comfort with self-driving cars and who provided their age and sex in the survey. We will focus on the variables below (out of the over 600 variables in the full General Social Survey). The variable definitions are based on survey prompts and variable definitions in the General Social Survey Documentation and Public Use File Codebook (Davern et al. 2025).\nFor the remainder of the chapter, we will refer to self-driving cars as “driverless cars” to align with the language used in the 2024 GSS.\n\naidrive_comfort: Indicator variable for respondent’s comfort with driverless (self-driving) cars. 0: Not comfortable at all; 1: At least some comfort.\n\nThis variable was derived from responses to the original survey prompt: “Comfort with driverless cars”. Scores ranged from 0 to 10 with 0 representing “totally uncomfortable with this situation” and 10 representing “totally comfortable with this situation”. Responses of 0 on the original survey were coded as aidrive_comfort = 0. All other responses coded as 1.\n\ntech_easy : Response to the question, “Does technology make our lives easier?” Categories are Neutral, Can't choose (Respondent doesn’t know / is unable to provide an answer), Agree, Disagree.\ntech_harm: Response to the question, “Does technology do more harm than good?”. Categories are Neutral, Can't choose (Respondent doesn’t know / is unable to provide an answer), Agree, Disagree.\nage: Respondent’s age in years\nsex: Respondent’s self-reported sex. Categories are Male, Female, the options provided on the original survey. \nincome: Response to the question “In which of these groups did your total family income, from all sources, fall last year? That is, before taxes.” Categories are Less than $20k, $20-50k, $50-110k, $110k or more, Not reported .\n\nNote: These categories were defined based on the 27 categories in income16 from the original survey.\n\npolviews: Response to the question, “I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal–point 1–to extremely conservative–point 7. Where would you place yourself?” Categories are Moderate, Liberal, Conservative, No reported.\n\nNote: These categories were defined based on the original 7 point scale.\n\n\n\nThe General Social Survey includes sample weights, so that the analysis data is representative of the population of adults in the United States.\nThese sample weights are not used in the analysis in this chapter or Chapter 12 which uses the same data. Therefore, the analysis and conclusions we draw are only for educational purposes. To use the General Social Survey for research, see (Davern et al. 2025) for more information about incorporating the sample weights.\n\n\n11.1.1 Exploratory data analysis\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of original resposnes from 2024 GSS\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of aidrive_comfort\n\n\n\n\n\n\n\nFigure 11.1: Distribution of original and aggregated variable on comfort with driverless cars\n\n\n\nFigure 11.1 shows the distribution of the original responses to the survey question about comfort with driverless cars and aidrive_comfort, the binary variable we will use in the analysis. In practice, categorical variables may have many categories, so we can aggregate categories to simplify the variable based on the analysis question. We saw an example of this in Section 3.4.1 when we looked at the variable season created from month. By simplifying the variable, we change the scope of the variable from a rating of the level of comfort an individual has with driverless cars to whether or not an individual has comfort with driverless cars. The latter aligns with the analysis objective and the modeling introduced in this chapter. Models for categorical response variables with three or more levels, such as the original survey responses, are introduced in Section 13.1.\nIn Figure 11.1 (b), we see about 45% of the respondents said they were “totally uncomfortable” with driverless cars and 55% reported some level of comfort with these cars. In the distribution of the original response variable in Figure 11.1 (a), among those who had at least some comfort, most had comfort levels of 5 or less. This detail could be important as we interpret the practical implications of the conclusions drawn from the analysis.\n\n\n\n\n\n\n\n\n\n\n\n(a) sex\n\n\n\n\n\n\n\n\n\n\n\n(b) age\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) tech_easy\n\n\n\n\n\n\n\nFigure 11.2: Univariate exploratory data analysis of predictor variables\n\n\n\n\n\n\n\nTable 11.1: Summary statistics for age\n\n\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\n\n\nage\n50.4\n17.6\n18\n36\n49\n65\n89\n0\n\n\n\n\n\n\n\n\nFigure 11.2 and Table 11.1 shows the univariate distributions of the predictor variables that will be used in this chapter. The exploratory data analysis for the other predictors is in Chapter 12  . The mean age in the data is about 50.4 and the median is 49 years old. The youngest respondents in the data set are 18 and the oldest are 89 years old. The standard deviation is about 17.645 years, so the data spans the general age range for adults in the United States.\nWe also observe from Figure 11.2 that the majority of the respondents (about 57 %) reported their sex as female, and a vast majority of respondents (about 79%) agree that technology is making life easier.\n\n\n\n\n\n\n\n\n\n\n\n(a) aidrive_comfort vs. sex\n\n\n\n\n\n\n\n\n\n\n\n(b) aidrive_comfort vs. age\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) aidrive_comfort vs. tech_easy\n\n\n\n\n\n\n\nFigure 11.3: Bivariate exploratory data analysis. Blue: adrive_comfort = 0, Red: aidrive_comfort = 1\n\n\n\nFigure 11.3 shows the relationship between the response and each of the predictor variables. Here we use a segmented bar plot  to visualize the relationship between the categorical predictors, sex and tech_easy and the categorical response, aidrive_comfort. As we look at the segmented bar plot, we are evaluating distribution of the response variable is approximately equal for each category of the predictor (Section 3.5.3). Unequal proportions suggests some relationship between the response and predictor variable. In Figure 11.3, a higher proportion of males have comfort with driverless cars compared to the proportion of females. This indicates a potential relationship between sex and comfort with driverless cars. Figure 11.3 shows overlap in the distribution of age among those who have comfort with driverless cars versus those who do not, but the distribution among those who have comfort with driverless cars tends to skew younger. As with linear regression, however, we will use statistical inference Section 11.5 to determine if the data provide evidence that such relationships exist.\n\nBased on Figure 11.3 , does there appear to be a relationship between opinions about whether technology makes life easier and comfort with driverless cars? Explain.1\n\nNow we use multivariate exploratory data analysis to explore potential interaction effects. Recall from Section 7.7 that an interaction effect occurs when the relationship between the response variable and a predictor differs based on values of the another predictor. We are often interested in interactions that include at least one categorical predictor, because we are interested how relationships might differ for subgroups in the sample.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) aidrive versus age faceted by tech_easy\n\n\n\n\n\n\n\n\n\n\n\n(b) aidrive versus tech_easy faceted by sex\n\n\n\n\n\n\n\nFigure 11.4: Mutlivariate EDA to explore potential interaction effects. Blue: adrive_comfort = 0, Red: aidrive_comfort = 1\n\n\n\nFigure 11.4 (a) is the relationship between age and aidrive_comfort faceted by the categories of tech_easy. As we look at the four sets of boxplots, we observe whether the relationship of the boxplots is the same for each level of tech_easy. If so, this indicates the relationship between age and aidrive_comfort is the same regardless of the value of tech_easy. In Figure 11.4 (a), the relationship in the distributions of age between those who have comfort with driverless cars versus those who don’t looks similar for the categories “Neutral”, “Agree”, and “Disagree”. The boxes largely overlap and the median age is approximately the same between the two groups of aidrive_comfort. The relationship does appear to be different among those in the “Can’t choose” category, as the median age among those who are not comfortable with driverless cars is higher than those who are comfortable.\nFigure 11.4 (b) is a plot to explore the interaction between two categorical predictors sex and tech_easy. Similar as before, we are examining whether the relationship between tech_easy and aidrive_comfort differs based on sex. We are not looking to see whether the proportions are equal between the two groups but are instead looking to see whether the proportions are approximately equal relative to one another within each category of sex.\nUsing this as the criteria, the relationship between tech_easy and aidrive_comfort looks approximately equal for the categories of sex. Therefore, the data do not show evidence of an interaction between tech_easy and sex. We do note that in general, the proportion of males comfortable with driverless cars within each group of tech_easy is higher than the corresponding groups for females. This is expected, given what we observed in the bivariate EDA in Figure 11.3.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-prob-odds",
    "href": "11-logistic.html#sec-prob-odds",
    "title": "11  Logistic regression",
    "section": "11.2 Probability / odds/ odds ratio",
    "text": "11.2 Probability / odds/ odds ratio\n\n11.2.1 Probability and odds\n\nBefore moving to modeling, let’s take some time to focus on the response variable. The response variable \\(Y\\) is categorical and takes values “0” or “1”. The response \\(Y = 1\\) indicates an observation takes the outcome of interest, and \\(Y = 0\\) indicates an observation does not take that outcome. Sometimes \\(Y = 1\\) and \\(Y = 0\\) are called “success” and “failure”, respectively. Note, however, that a “success” just means it is the outcome we are interested in studying, not necessarily what would be considered a “success” in practice.\nThe probability \\(Y=1\\), denoted \\(Pr(Y=1)\\), is a measure of the chance that the event \\(Y = 1\\) occurs. Probabilities take values between 0 and 1, with 0 indicating an event is impossible and never occurs, and 1 indicating that an event always occurs. The odds \\(Y = 1\\) are the ratio of the probability \\(Y = 1\\) occurs versus the probability \\(Y = 0\\) occurs. This is computed as\n\\[\n\\text{odds(Y = 1)} = \\frac{Pr(Y = 1)}{Pr(Y = 0)} = \\frac{Pr(Y = 1)}{1 - Pr(Y = 1)}\n\\tag{11.1}\\]\nNote that the odds that \\(Y = 0\\) are equal to \\(1/ \\text{odds}(Y = 1)\\) .\n\n\n\n\nTable 11.2: Frequency, probability, and odds of aidrive_comfort\n\n\n\n\n\n\naidrive_comfort\nn\nprobability\nodds\n\n\n\n\n0\n692\n0.455\n0.835\n\n\n1\n829\n0.545\n1.198\n\n\n\n\n\n\n\n\nTable 11.2 shows the probabilities an odds for the response variable aidrive_comfort. In the data from 1521 respondents, the probability (chance) a randomly selected respondent is comfortable with driverless cars is 0.545. This may also be phrased as there is a 54.5 % chance a randomly selected respondent is comfortable with driverless cars. The odds a randomly selected respondent is comfortable with driverless cars are 1.198. This means that a randomly selected respondent is 1.198 times more likely to be comfortable with driverless cars than not be.\n\nShow how the following values are computed:\n\n\\(Pr(\\text{aidrive\\_comfort} = 1) =\\) 0.545\n\\(\\text{odds}(\\text{aidrive\\_comfort}=1)=\\) 1.198 2\n\n\n\n\n11.2.2 Odds ratio\nWe are most interested in understanding the relationship between comfort with driverless cars and other factors about the respondents. Table 11.3 is a two-way table of the relationship between aidrive_comfort and tech_easy. Now we can not only answer questions regarding opinions about comfort with driverless cars overall but can also see how these opinions on comfort may differ based on respondent’s opinion about whether technology makes life easier.\n\n\n\n\n\nTable 11.3: Two-way table of aidrive_comfort (columns) versus tech_easy (rows)\n\n\n\n\n\n\nTech Easy\n0\n1\n\n\n\n\nNeutral\n127\n90\n\n\nCan’t choose\n18\n12\n\n\nAgree\n495\n706\n\n\nDisagree\n52\n21\n\n\n\n\n\n\n\n\nEach cell in the table is the number of observations that take the values indicated by the row and column. For example, there are 90 respondents in the data whose observed data are tech_easy = \"Neutral\" and aidrive_comfort = \"1\". We can use this table to ask how the probability of being comfortable with driverless cars differs based on opinions about whether technology makes life easier. For example, we can compute the probability and the corresponding odds a randomly selected respondent is comfortable with driverless cars given they are neutral about whether technology makes life easier.\n\\[\n\\begin{aligned}\nPr(Y = 1| \\text{tech\\_easy = Neutral}) = \\frac{90}{127 + 90}  \\approx 0.415 \\\\[5pt]\n\\text{odds}(Y = 1| \\text{tech\\_easy = Neutral}) = \\frac{90}{127} = \\frac{0.415}{1 - 0.415}\\approx{0.709}\n\\end{aligned}\n\\tag{11.2}\\]\n\\(Pr(Y=1 | \\text{tech\\_easy = Neutral})\\) is called a conditional probability, because it is the probability that a randomly selected respondent is comfortable with driverless cars conditioned on (given) they are neutral about whether technology makes life easier. Similarly, \\(\\text{odds}(Y=1 | \\text{tech\\_easy = Neutral})\\) are conditional odds.\n\nIt may sound awkward to communicate results where the odds are less than 1. Because the odds are reciprocal, they are typically reported in terms of the odds being greater than 1. For example, an alternative way to present the odds from Equation 11.2 is\n\nA randomly selected respondent who is neutral about technology making life easier is 1.41 times more likely to not be comfortable with driverless cars than be comfortable.\n\n\n\nCompute the probability and corresponding odds of being comfortable with driverless cars among those who agree with the statement that technology makes life easier.3\n\nFrom Table 11.3, the odds a respondent who is neutral about technology making life easier is comfortable with driverless cars are 0.709, and the odds a respondent who agrees technology makes life easier are 1.43. We quantify how the odds for these two groups compare using an odds ratio. An odds ratio is computed as the odds for one group divided the odds for another, as shown in Equation 11.3\n\\[\n\\text{odds ratio} = \\frac{\\text{odds}_{group 1}}{\\text{odds}_{group 2}}\n\\tag{11.3}\\]\nLet’s compute the odds ratio for those who agree that technology makes life easier versus those who are neutral.\n\\[\n\\text{odds ratio} = \\frac{\\text{odds}_{\\text{Agree}}}{\\text{odds}_{\\text{Neutral}}} = \\frac{1.43}{0.709} = 2.02\n\\]\n\nThis means that the odds a respondent who agrees technology makes life easier is comfortable with driverless cars are 2.02 times the odds a respondent who is neutral about technology is comfortable with driverless cars.\n\n\n\nCompute the odds ratio of being comfortable with driverless cars for tech_easy = Agree versus teach_easy = Disagree. Interpret this value in the context of the data.4",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-model",
    "href": "11-logistic.html#sec-logistic-model",
    "title": "11  Logistic regression",
    "section": "11.3 From linear to logistic regression",
    "text": "11.3 From linear to logistic regression\n\nThe two-way table and odds ratios from the previous section helped us being to explore the relationship between comfort with driverless cars and opinion about whether technology makes life easier. Let’s build on this by fitting a model that can (1) help us quantify the relationship between variables, (2) explore the relationship between the response and multiple predictors variables, and (3) be used to make predictors and draw rigorous conclusions. Before diving into the details of this model’s we’ll take a moment to understand why we are introducing a new model for the relationship between aidrive_comfort versus the predictors rather than using the linear regression model we’ve seen up to this point.\n\n\n\nSuppose we want to fit a model of the relationship between age and aidrive_comfort. Our first thought may be to use the linear regression model from Chapter 4 to model this relationship. In this case, the estimated model would be of the form\n\\[\n\\widehat{\\text{aidrive\\_comfort}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{age}\n\\tag{11.4}\\]\n\n\n\n\n\n\n\n\nFigure 11.5: Linear regression model for aidrive_comfort versus age\n\n\n\n\n\n\nFigure 11.5 is a visualization of the linear regression model for the relationship between age and aidrive_comfort shown in Equation 11.4. Here we easily see the linear regression model represented by the red line is not a good fit for the data. In fact, it (almost) never produces predictions of 0 or 1, the observed values of the response. Recall that the linear regression model is estimated by minimizing the sum of squared residuals, \\(0 - \\hat{y}_i\\) or \\(1 - \\hat{y}_i\\) in this scenario. These residuals can be minimized by finding a model that produces estimates between 0 and 1, as shown in Figure 11.5, rather than trying to predict the actual observed values, 0 or 1.\nNext, we might consider fitting a linear model such that the response variable is the \\(\\pi = Pr(Y = 1)\\), the probability of being comfortable with driverless cars. This estimated model takes the form\n\\[\n\\hat{\\pi} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{age}\n\\tag{11.5}\\]\n\n\n\n\n\n\n\n\nFigure 11.6: Linear model for Pr(aidrive_comfort = 1) versus age\n\n\n\n\n\nFigure 11.6 shows the relationship between age and the probability of being comfortable with driverless cars. This model seems to be a better fit for the data, as it generally captures the trend that younger respondents are more likely to be comfortable with driverless cars compared to older respondents. The primary issue with using the probability as the response variable is that probabilities are bounded between 0 and 1. Therefore, it is impossible to get values less than 0 or greater than 1 in practice, so we would want the same bounded constraints on our model particularly if we wish to use the model for prediction. We saw a similar issue in Chapter 10 with the values of valence in the Spotify data. We have the same issue when using the odds as the response variable, because the odds cannot be less than 0. Therefore, we need a model that not only captures the relationship between the response and predictor but also addresses the boundary constraint.\nWhen there is a binary categorical response variable, we use a logistic regression model to model the relationship between the binary response and one or more predictor variables as in Equation B.1.\n\\[\n\\log\\Big(\\frac{\\pi}{1 - \\pi}\\Big) = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p\n\\tag{11.6}\\]\nwhere \\(\\pi = Pr(Y = 1)\\) and \\(\\log\\big(\\frac{\\pi}{1 - \\pi}\\big)\\) is the logit transformation, also called the “log odds”. The log odds can take any value \\(-\\infty\\) to \\(\\infty\\), so it does not have the boundary constraints of the probability and odds. The log odds output from Equation B.1 can be transformed back to the odds and probabilities, and we will the probabilities to assign each observation to a predicted class aidrive_comfort = 0 or aidrive_comfort = 1. We’ll discuss this more in Chapter 12.\n\n\n\n\n\n\n\n\nFigure 11.7: Output from logistic regression model of aidrive_comfort versus age\n\n\n\n\n\nFigure 11.7 shows the output of the logistic regression model of aidrive_comfort versus age. Here, we see the logit is not bounded as the probability or odds.\n\n\nEquation B.1 is the statistical model for logistic regression. The statistical model does not have the error term \\(\\epsilon\\) as we’ve seen in the statistical model for linear regression in Equation 7.3. Recall from Section 7.2 that the error term \\(\\epsilon\\) is the difference between the observed response \\(Y\\) and the expected value produced by the model \\(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p\\). Thus, we produce predictions for the response variable directly when doing linear regression.\n\nIn contrast, the output of the logistic regression model are the log odds that \\(y = 1\\), not the actual observed values \\(y_i = 0\\) or \\(y_i  = 1\\). Thus, there is no error term in the statistical model for logistic regression, because we do not predict the value of the response directly and thus do not have the same notion of the difference between the observed and predicted response.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-interpret-coef",
    "href": "11-logistic.html#sec-logistic-interpret-coef",
    "title": "11  Logistic regression",
    "section": "11.4 Interpreting model coefficients",
    "text": "11.4 Interpreting model coefficients\n\nIn Section 7.3, we showed that the model coefficients for linear regression are estimated by the least-squares method, finding the values of the coefficients that minimizes the sum of squared residuals. There is no error term in the logistic regression model (see ?eq-logistic-model-2), so we cannot use least-squares in this case. The model coefficients for logistic regression are estimated using maximum likelihood estimation. Recall from Section 10.2 that the likelihood is a function that quantifies how likely the observed data are to have occurred given a combination of coefficients.  Thus, maximum likelihood estimation is used to find the combination of coefficients that makes the observed data the most likely to have occurred, i.e., that maximizes the value of the likelihood function. The mathematical details for estimating logistic regression coefficients using maximum likelihood estimation are available in Section B.2.\n\n11.4.1 Predictor in simple logistic regression\nIn Section 11.2, we computed values from a two-way table to understand the relationship between opinions about whether technology makes life easier and comfort with driverless cars. Now we will use a logistic regression model to quantify the relationship.\n\n\n\n\nTable 11.4: Logistic regression model for aidrive_comfort versus tech_easy\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.344\n0.138\n-2.499\n0.012\n\n\ntech_easyCan’t choose\n-0.061\n0.397\n-0.154\n0.878\n\n\ntech_easyAgree\n0.699\n0.150\n4.671\n0.000\n\n\ntech_easyDisagree\n-0.562\n0.293\n-1.919\n0.055\n\n\n\n\n\n\n\n\nEquation 11.7 is the equation from the model output in Table 11.4.\n\\[\n\\begin{aligned}\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.344 + -0.061 \\times \\text{Can't choose}  + 0.699 \\times \\text{Agree}   - 0.562 \\times \\text{Disagree}\n\\end{aligned}\n\\tag{11.7}\\]\nwhere \\(\\hat{\\pi}\\) is the predicted probability a respondent is comfortable with driverless cars.\n\nWhat is the baseline level for tech_easy? 5\n\nThe intercept, -0.344, is the estimated log odds for respondents in the baseline group, Neutral. Similar to interpretations in Section 9.2, though we do calculations using the logit, we write interpretations in terms of the odds, so they are more clearly understood by the reader. To transform from logit to odds, we exponentiate both sides of the model equation. Equation 11.8 shows the equation from Table 11.4 in terms of the odds of being comfortable with driverless cars.\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}} = e^{-0.344} \\times e^{-0.061 \\times \\text{Can't choose}} \\times e^{0.699 \\times \\text{Agree}}  \\times e^{-0.562 \\times \\text{Disagree}}\n\\end{aligned}\n\\tag{11.8}\\]\nFrom Equation 11.8, we see that the estimated odds a respondent who is neutral about whether technology makes life easier is comfortable with driverless cars is 0.709 ( \\(e^{-0.344}\\)). This equals the odds that was computed from the two-way table in Equation 11.2.\n\nThe following two rules were used to go from Equation 11.7 to Equation 11.8\n\n\\(e^{\\log(a)} = a\\)\n\\(e^{a+b}  = e^a e^b\\)\n\n\nNow let’s look at the coefficient for Agree, 0.699. Putting together what know about the response variable with what we’ve learned about interpreting coefficients for categorical predictors in Section 7.4.2, this estimated coefficient means the following:\n\nThe log odds a respondent who agrees technology makes life easier is comfortable with driverless cars are expected to be 0.699 higher than the log odds of a respondent who is neutral about whether technology makes life easier.\n\nFrom Equation 11.8, the coefficient for Agree is \\(e^{0.699} \\approx 2.012\\). This means the following:\n\nThe odds a respondent who agrees technology makes life easier is comfortable with driverless cars are expected to be 2.012 ( \\(e^{0.699}\\)) times the odds of a respondent who is neutral about whether technology makes life easier.\n\nThe value 2.012 is equal to the odds ratio of aidrive_comfort = 1 for tech_easy = Agree versus tech_easy = Neutral. It is the same (give or take rounding) as the odds ratio we computed from Table 11.3. This tells us something important about the relationship between the odds ratio and model coefficients in a logistic regression model. When we fit a simple logistic regression model with one categorical predictor \\(X\\) such that \\(\\beta_k\\) is the coefficient corresponding to the \\(k^{th}\\) level of \\(X\\), then \\(e^{\\beta_k}\\) is the odds ratio between the \\(k^{th}\\) level and the baseline level.\n\n\n11.4.2 Categorical predictors\nNow, we will fit a multiple logistic regression model and use age along with tech_easy as predictors aidrive_comfort.\n\n\n\n\nTable 11.5: Logistic regression model for aidrive_comfort versus age and tech_easy\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.420\n0.208\n2.020\n0.043\n\n\ntech_easyCan’t choose\n-0.118\n0.402\n-0.294\n0.769\n\n\ntech_easyAgree\n0.678\n0.151\n4.487\n0.000\n\n\ntech_easyDisagree\n-0.499\n0.295\n-1.690\n0.091\n\n\nage\n-0.015\n0.003\n-4.902\n0.000\n\n\n\n\n\n\n\n\nNow that we have multiple predictors in Table 11.5, the intercept is the predicted log odds of being comfortable with driverless cars for those who are neutral about technology making life easier and who are 0 years old. It would be impossible for a respondent to be 0 years old, so while the intercept is necessary to obtain the best fit model, its interpretation is not meaningful in practice. As with linear regression, we could use the centered values of age in the model if we wish to have an interpretable intercept.\nThe coefficient for Agree in this model is 0.678. It is but not equal to the coefficient for Agree in the previous model, because now the coefficient is computed after adjusting for age. The interpretation is similar as before, taking into account the fact that age is also in the model.\n\nThe log odds a respondent who agrees technology makes life easier is comfortable with driverless cars are expected to be 0.678 higher than the log odds of a respondent who is neutral about whether technology makes life easier, holding age constant.\n\nTo write the interpretation in terms of the odds, we need to exponentiate both sides of the model equation as in the previous section. Equation 11.9 shows the relationship between opinions about technology, age, and the odds of being comfortable with driverless cars.\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}} = e^{0.420} \\times e^{-0.118 \\times \\text{Can't choose}} \\times e^{0.678 \\times \\text{Agree}}  \\times e^{-0.499 \\times \\text{Disagree}} \\times e^{-0.015 \\times \\text{age}}\n\\end{aligned}\n\\tag{11.9}\\]\nwhere \\(\\hat{\\pi}\\) is the predicted probability a respondent is comfortable with driverless cars.\nBased on Equation 11.9, the interpretation of the Agree in terms of the odds is the following:\n\nThe odds a respondent who agrees technology makes life easier is comfortable with driverless cars are expected to be 1.970 ( \\(e^{0.678}\\)) times the odds of a respondent who is neutral about whether technology makes life easier, holding age constant.\n\nThe value 1.970 (\\(e^{0.678}\\)) is the odds ratio of comfort with driverless cars between those who agree technology makes life easier and those who are neutral, after adjusting for age. When there are multiple predictor variables in the logistic regression model, \\(e^{\\beta_j}\\) where \\(\\beta_j\\) is the coefficient for the predictor is called the adjusted odds ratio (AOR).\n\nUse the model from Table 11.5. Assume age is held constant.\n\nInterpret the coefficient of Disagree in terms of the log odds of being comfortable with driverless cars.\nInterpret the coefficient of Disagree in terms of the odds of being comfortable with driverless cars.6\n\n\n\n\n11.4.3 Quantitative predictors\nSimilar to categorical predictors, we can use what we learned about interpreting quantitative predictors in linear regression models in Section 7.4.1 as a starting point for interpretations for logistic regression. Let’s look at the interpretation of the coefficient for age from the model in Table 11.5. The coefficient is the expected change in the response when the predictor increases by one unit . Given this, the interpretation for age in terms of the log odds is the following:\n\nFor each additional year increase in age, the log odds a respondent is comfortable with driverless cars are expected to decrease by 0.015, holding opinions about technology constant.\n\nLet’s take a moment to show this interpretation mathematically by comparing the log odds between age and age + 1, where age+1 represents the one unit increase in age. The interpretations are written holding opinions about technology constant, so we can ignore this predictor when doing these calculations. We’ll also ignore the intercept, because it is also the same regardless of the value of age. Let \\(\\text{log odds (comfort | age)}\\) be the log odds given some value age and \\(\\text{log odds (comfort | age + 1)}\\) be the log odds given the value age + 1.\nThen, the change in the log odds is\n\\[\n\\begin{aligned}\n\\text{log odds (comfort | age + 1)}  - \\text{log odds (comfort | age)} &= -0.015 \\times ( \\text{age} + 1) - (-  0.015 \\times \\text{age}) \\\\\n& = -0.015 ( \\text{age} + 1 - \\text{age}) \\\\\n& = -0.015\n\\end{aligned}\n\\tag{11.10}\\]\nFrom Equation 11.10, \\(\\text{log odds (comfort | age + 1)} = \\text{log odds (comfort | age)} - 0.015\\).\nNow we’ll interpret the coefficient of age in terms of the odds. Based on Equation 11.9, the interpretation of age in terms of the odds of being comfortable with driverless cars is the following:\n\nFor each additional year increase in age, the odds a respondent is comfortable with driverless cars are expected to multiply by 0.985 ( \\(e^{-0.015}\\)), holding opinions about technology constant.\n\n\n\n\\(\\log(a) + \\log(b) = \\log(ab)\\)\n\n\n\n\\(\\log(a) - \\log(b) = \\log(\\frac{a}{b})\\)\n\n\nWe can show the interpretation mathematically starting with the result from Equation 11.10.\n\\[\n\\begin{aligned}\n\\text{log odds (comfort | age + 1)}  - \\text{log odds (comfort | age)} &= -0.015 \\\\[8pt]\n\\Rightarrow \\log\\Bigg(\\frac{\\text{odds (comfort | age + 1)}}{\\text{odds (comfort | age)}}\\Bigg) & = -0.015 \\\\[8pt]\n*\\text{exponetiate both sides}* \\\\[8pt]\n\\Rightarrow \\frac{\\text{odds (comfort | age + 1)}}{\\text{odds (comfort | age)}} & = e^{-0.015}\n\\end{aligned}\n\\tag{11.11}\\]\nFrom Equation 11.11, \\(\\text{odds (comfort | age + 1)} = e^{-0.015} \\times \\text{odds (comfort | age)}\\). The value \\(\\frac{\\text{odds (comfort | age + 1)}}{\\text{odds (comfort | age)}}\\) is the odds ratio of being comfortable with driverless cars between age versus age + 1. Thus, given \\(\\beta_j\\) is the coefficient for a quantitative predictor \\(X_j\\), the value \\(e^{\\beta_j}\\) is the (adjusted) odds ratio between \\(X_j + 1\\) and \\(X_j\\).\n\nUse the model in Table 11.5. Assume opinions about technology are held constant.\n\nHow are the log odds of being comfortable with driverless cars expected to change when age increases by 5 years?\nHow are the odds of being comfortable with driverless cars expected to change when age increases by 5 years? 7",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-inf",
    "href": "11-logistic.html#sec-logistic-inf",
    "title": "11  Logistic regression",
    "section": "11.5 Inference for a coefficient \\(\\beta_j\\)",
    "text": "11.5 Inference for a coefficient \\(\\beta_j\\)\nInference for coefficients in logistic regression is very similar to inference in linear regression, introduced in Chapter 5 and Chapter 8. We will use these chapters as the foundation, discuss how they apply to logistic regression, and show ways in which inference for logistic regression differs. We refer the reader to the chapters on inference for linear regression for an introduction to statistical inference more generally.\n\n11.5.1 Simulation-based inference\nWe can use simulation-based inference to draw conclusions about the model coefficients in logistic regression. The process for constructing bootstrap confidence intervals and using permutation sampling for hypothesis tests are the same as in linear regression, with the difference being the type of model that is fit. Here we will show how they apply to logistic regression.\n\nPermutation tests\nHypothesis tests is used to test a claim about a population parameter. In the case of logistic regression, we use hypothesis tests to evaluate whether there is evidence of a relationship between a given predictor and the log odds of the response variable. The hypotheses to test whether there is a relationship between predictor \\(X_j\\) and the response, after adjusting for the other predictors are in Equation 11.12.\n\\[\n\\begin{aligned}\n&H_0: \\beta_j = 0 - \\text{There is no relationship between }X_j\\text{ and the response}\\\\\n&H_a: \\beta_j \\neq 0 - \\text{There is a relationship between }X_j\\text{ and the response}\n\\end{aligned}\n\\tag{11.12}\\]\nThe hypothesis test is conducted assuming the null hypothesis is true. When doing simulation-based inference, we use permutation sampling to generate the null distribution under this assumption. The process for permutation sampling is the same for logistic regression as with linear regression in Section 5.6. The values of the predictor of interest \\(X_j\\) are permuted, such that the values are randomly assigned to each observation. There is no relationship between the permuted values of \\(X_j\\) and the response variable. The logistic model is fit to each permuted sample, and the estimated coefficients \\(\\hat{\\beta}_j\\) from the permuted samples make up the null distribution. The null distribution is used to compute the p-value and draw a conclusion about the hypotheses.\nWe’ll use a permutation test to evaluate whether there is evidence of a relationship between age and aidrive_comfort , after adjusting for tech_easy. \n\\[\n\\begin{aligned}\nH_0: \\beta_{\\text{age}} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} \\beta_{\\text{age}} \\neq 0\n\\end{aligned}\n\\]\nThe original value and five permutations of age for the first respondent are shown Table 11.6.\n\n\n\n\nTable 11.6: Five permutations of age for Respondent 1\n\n\n\n\n\n\n\naidrive_comfort\ntech_easy\nage\n\n\n\n\nOriginal Sample\n1\nAgree\n33\n\n\nPermutation 1\n1\nAgree\n31\n\n\nPermutation 2\n1\nAgree\n34\n\n\nPermutation 3\n1\nAgree\n69\n\n\nPermutation 4\n1\nAgree\n48\n\n\nPermutation 5\n1\nAgree\n80\n\n\n\n\n\n\n\n\nTable 11.6 illustrates permutation sampling for one respondent. For each permutation, the value of aidrive_comfort and tech_easy are the same, and the values of age are randomly shuffled. \nFigure 11.8 is the null distribution produced from the 1000 permutations.\n\n\n\n\n\n\n\n\n\nFigure 11.8: Null distribution produced by permutation sampling. The solid line is the observed value of age.\n\n\n\n\n\nFigure 11.8 shows that the estimated coefficient (solid vertical line) is far away from the center of the null distribution (the null hypothesized value 0), suggesting the evidence is not consistent with the null hypothesis. The p-value quantifies the strength of evidence against the null hypothesis. Based on the hypotheses in Equation 11.12, it is computed as the number of observations in the null distribution that have magnitude greater than \\(|\\beta_{\\text{age}}|\\).\nThe p-value in this example is \\(\\approx\\) 0. The p-value is very small, suggesting sufficient evidence against the null hypothesis. We reject the null hypothesis and conclude there is evidence of a relationship between age and comfort with driverless cars, after adjusting for opinions on technology.\n\n\nBootstrap confidence intervals\nA confidence interval is a range of values in which a population parameter may reasonably take. This range of values is computed based on the sampling distribution of the estimated statistic. When conducting simulation-based inference, we use bootstrap sampling to construct this sampling distribution.\nThe process for constructing the sampling distribution using bootstrapping is the same for logistic regression as the process for linear regression in Section 5.4. Each bootstrap sample is constructed by sampling with replacement \\(n\\) times, where \\(n\\) is the number of observations in the sample data. The model is fit to each bootstrap sample, and the estimated coefficients make up the sampling distribution. The \\(C\\%\\) confidence interval is computed as the bounds marking the middle \\(C\\%\\) of the bootstrapped sampling distribution.\nFigure 11.9 is the bootstrap sampling distribution for \\(\\hat{\\beta}_{\\text{age}}\\) for 1000 bootstrap samples.\n\n\n\n\n\n\n\n\nFigure 11.9: Bootstrap distribution for the coefficient of age. The vertical lines are the bounds for the 95% confidence interval.\n\n\n\n\n\nThe 95% bootstrap confidence is -0.021 to -0.009. These values are marked by vertical lines in Figure 11.9. As with the model output, these are in terms of the relationship between age and the log odds of being comfortable with driverless cars. Thus, the direct interpretation of the 95% confidence interval is as follows:\n\nWe are 95% confident that for each additional year in age, the log odds of being comfortable with driverless cars decrease between 0.009 to 0.021, holding opinions on technology constant.\n\nThe interpretation in terms of the odds is the following:\n\nWe are 95% confident that for each additional year in age, the odds of being comfortable with driverless cars multiply by a factor of 0.98 ( \\(e^{-0.021}\\)) to 0.991 ( \\(e^{-0.009}\\) ), holding opinions on technology constant.\n\n\n\n\n11.5.2 Theory-based inference\nThe general process for theory-based inference for a single coefficient in logistic regression is similar as linear regression. The primary difference is in the distribution of the estimated coefficient \\(\\hat{\\beta}_j\\).\nWhen fitting a linear regression model, we have an formula for the estimated coefficients (see Section 4.4 and Section 7.3). Thus we have an exact formula for the distribution of a given coefficient \\(\\hat{\\beta}_j\\) that applies for any sample size. Recall that we account for the sample size in how we define degrees of freedom of the \\(t\\) distribution for the coefficients (see Section 8.4.1).\nIn logistic regression, the coefficients are estimated using maximum likelihood estimation (see Section 11.4), such that numerical optimization methods are used to find the combination of coefficients that maximize the likelihood function. There is no closed-form equation for the estimated coefficients as with linear regression. Because the estimated coefficients are approximated using optimization, the distribution of the coefficients are also approximated.\nWhen the sample size is large , the distribution of the individual estimated coefficient \\(\\hat{\\beta}_j\\) is \\(N(\\beta_j, SE_{\\hat{\\beta}_j})\\), approximately normal with an expected value of \\(\\beta_j\\), the true value of the coefficient and standard error \\(SE_{\\hat{\\beta}_j}\\). In practice, we will get the estimated standard error from the software output. Mathematical details about computing \\(SE_{\\hat{\\beta}_j}\\) are described in Section B.3.\n\nHypothesis test\nThe steps for the hypotheses test are the same as those outlined in Section 8.4.2.\n\nState the null and alternative hypotheses.\nCalculate a test statistic.\nCalculate a p-value.\nDraw a conclusion.\n\nThe hypotheses for a single coefficient are the same as in Equation 11.12:\n\\[\nH_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs }\\hspace{2mm} H_a: \\beta_j \\neq 0\n\\]\nThe test statistic, in Equation 11.13, is called the Wald test statistic (Wald 1943), because it is based on the approximation of the mean and standard error as \\(n\\) is large.\n\\[\nz = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\n\\tag{11.13}\\]\nThe test statistic, denoted by \\(z\\), is the number of standard errors the estimated coefficient \\(\\hat{\\beta}_j\\) is from 0, the hypothesized mean of the distribution. It follows a standard normal distribution, \\(N(0, 1)\\), because it is based on asymptotic results (compared to the \\(t\\) test statistic in linear regression which applies even for small \\(n\\)). Because the alternative hypothesis is two-sided, the p-value is computed as \\(P(|Z| \\geq |z|)\\) , where \\(Z \\sim N(0, 1)\\).  The p-value is interpreted as before, with small p-values providing stronger evidence against the null hypothesis.\nLet’s take a look at the hypothesis test for the coefficient of age using the theory-based methods. The components for the theory-based hypothesis test are produced in the model output. The output of the model including tech_easy and age is reproduced in Table 11.7 along with the 95% confidence intervals for the model coefficients.\n\n\n\n\nTable 11.7: Model output for aidrive_comfort versus tech_easy and age with 95% confidence intervals for coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.420\n0.208\n2.020\n0.043\n0.012\n0.828\n\n\ntech_easyCan’t choose\n-0.118\n0.402\n-0.294\n0.769\n-0.928\n0.661\n\n\ntech_easyAgree\n0.678\n0.151\n4.487\n0.000\n0.383\n0.976\n\n\ntech_easyDisagree\n-0.499\n0.295\n-1.690\n0.091\n-1.094\n0.068\n\n\nage\n-0.015\n0.003\n-4.902\n0.000\n-0.021\n-0.009\n\n\n\n\n\n\n\n\nThe hypotheses are the same as with simulation-based inference\n\\[\nH_0:\\beta_{\\text{age}} = 0 \\hspace{2mm} \\text{ vs }\\hspace{2mm} \\beta_{\\text{age}} \\neq 0\n\\]\nThe null hypothesis is that there is no relationship between age and comfort with driverless cars after accounting for opinions about technology. The alternative hypothesis is that there is a relationship.\nThe test statistic8 is computed as\n\\[\nz = \\frac{-0.015 - 0}{0.003} \\approx - 5\n\\]\nThe p-value is computed as \\(P(|Z| \\geq |-4.902|) \\approx\\) 4.83^{-6}. This value is so small it rounds to 0 when displaying the results to 3 digits. This is consistent with the p-value we observed from the permutation test in Section 11.5.1. Because the p-value is small, we reject the null hypothesis and conclude there is evidence of a relationship between age and comfort with driverless cars after adjusting for opinions about technology.\n\nWe can use a decision-making threshold \\(\\alpha\\) when using the p-value to draw conclusions from a hypothesis test. The potential for Type I and Type II errors is the same for logistic regression as in linear regression. See Section 5.6.4 for more on choosing \\(\\alpha\\) and potential errors.\n\n\n\nConfidence interval\nThe equation to compute the confidence interval for coefficients in logistic regression is very similar to the formula in linear regression from Section 5.8. The difference is the critical value \\(z^*\\) is computed from the \\(N(0, 1)\\) distribution\n\\[\n\\hat{\\beta}_j \\pm z^* \\times SE_{\\hat{\\beta}_j}\n\\tag{11.14}\\]\nIn practice, the confidence interval is produced in the model output. Let’s show how the 95% confidence interval for age is computed in Table 11.7.\nThe values for \\(\\hat{\\beta}_j\\) and \\(SE_{\\hat{\\beta}_j}\\) can be read directly from table, as before. The critical value \\(z^*\\) is the point on the \\(N(0, 1)\\) distribution such that the middle 95% (or \\(C\\%\\) in general) of the distribution is between \\(-z^*\\) and \\(z^*\\). We can use statistical software to get this value as shown in Section 11.7. The \\(z^*\\) for the 95% confidence interval is 1.96.\nThe 95% confidence interval for the coefficient of age is\n\\[\n\\begin{aligned}\n&-0.015 \\pm 1.96 \\times 0.003 \\\\[5pt]\n\\Rightarrow &-0.015 \\pm  0.00588 \\\\[5pt]\n\\Rightarrow &\\mathbf{[ -0.021 , -0.009]}  \n\\end{aligned}\n\\]\n\n\nInterpret this interval in the context of the data in terms of the odds of being comfortable with driverless cars.\nHow does this interval compare to the bootstrap confidence interval computed in Section 11.5.1? 9\n\n\n\n\nConnection between hypothesis test and confidence intervals\nThe same connection between two-sided hypothesis tests and confidence intervals applies in the context of logistic regression as we saw in linear regression in Section 5.7 . A two-sided hypothesis test with decision-making threshold of \\(\\alpha\\) directly corresponds to the \\((1 - \\alpha) \\times 100 \\%\\) confidence interval. For example, a hypothesis test with decision-making threshold of \\(\\alpha = 0.05\\) directly corresponds to the 95% (\\((1 - 0.05) \\times 100\\)) confidence interval.\nThis means we can use a confidence interval to evaluate a claim about a coefficient and get a range of values the population coefficient may take. The following are the relationship between the confidence interval and hypothesis test:\n\nConfidence interval for \\(\\beta_j\\) : If 0 is in the interval, fail to reject the null hypothesis. If 0 is not in the interval, reject the null hypothesis.\nConfidence interval for \\(e^{\\beta_j}\\): If 1 is in the interval, fail to reject the null hypothesis. If 1 is not in the interval, reject the null hypothesis.\n\n\nUse the output in Table 11.7.\n\nShow how the test statistic is computed for tech_easyDisagree. Interpret this value in the context of the data.\nDo the data provide evidence that the odds are different among those who disagree technology makes life easier compared to those who are neutral (the baseline), after adjusting for age? Use \\(\\alpha = 0.05\\) Explain.\nShow how the 95% confidence interval for tech_easyDisagree is computed. Is it consistent with your conclusion in the previous question?10",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-conditions",
    "href": "11-logistic.html#sec-logistic-conditions",
    "title": "11  Logistic regression",
    "section": "11.6 Model conditions",
    "text": "11.6 Model conditions\n\nWe fit logistic regression model under a set of assumptions. These are similar to the assumptions for linear regression in Section 5.3, but there is no assumption related to normality or equal variance.\n\nThere is a linear relationship between the predictors and logit of the response variable\nThe observations are independent of one another \n\nSimilar to linear regression, we check model conditions to evaluate whether these assumptions hold for the data.\n\n11.6.1 Linearity\nThe linearity assumption for logistic regression states that there is a linear relationship between the predictors and the logit of the response variable. We check this assumption by looking at plots of the quantitative predictors versus the empirical logit of the response. The empirical logit is the log-odds of a binary variable (“logit”) calculated from the data (“empirical”). For example, Equation 11.15 is the empirical logit for the comfort with driverless cars in the sample data set.\n\\[\n\\text{empirical logit} = \\log\\Big(\\frac{\\pi}{1 - \\pi}\\Big) = \\log\\Big(\\frac{0.545}{1 - 0.545}\\Big) = 0.180\n\\tag{11.15}\\]\nWe can also look at the empirical logit by subgroup of a categorical variable. For example, Table 11.8 shows the empirical logit by opinion on whether technology makes life easier.\n\n\n\n\nTable 11.8: Probability and empirical logit of aidrive_comfort by tech_easy\n\n\n\n\n\n\ntech_easy\nn\nProbability\nEmpirical Logit\n\n\n\n\nNeutral\n217\n0.415\n-0.344\n\n\nCan’t choose\n30\n0.400\n-0.405\n\n\nAgree\n1201\n0.588\n0.355\n\n\nDisagree\n73\n0.288\n-0.907\n\n\n\n\n\n\n\n\nComputing the empirical logit across values of a quantitative predictor is similar as the process for categorical predictors. We divide the values of the quantitative predictor into bins and compute the empirical logit of the response for the observations within each bin. There are multiple ways to divide the quantitative predictor into bins. Some software creates the bins, such that there are an equal number of observations in each bin. Another option is to create bins by dividing by some incremental amount (e.g., dividing age into 5 or 10 year increments). Dividing the variable by the same incremental amount can be more easily interpreted; however, there may be large variation in the number of observations in each bin that needs to be taken into account when comparing the empirical logit across bins. The former method may be less interpretable, but we know the empirical logit is computed using the same observations for each bin.\nTable 11.9 shows the number of observations \\(n\\), probability, and empirical logit across bins of age using the two approaches for dividing age into bins. In Table (a) of Table 11.9, the bins have been created such that the observations are more evenly distributed across bins. Often, the number of observations will be equal or differ by a small amount. In this case, because age is a discrete variable, there are many observations with the exact same value of age. For example, there are 38 observations with the age of 39 years old. Because all observations with the same age are in the same bin, the number of observations in bins that include ages that occur frequently is higher than less common ages in the data, such as those over 70 years old. Note, however, that the number of observations in each bin are still more even than when the bins are created using equal age increments.\n\n\n\nTable 11.9: Probability and empirical logit of aidrive_comfort by age\n\n\n\n\n\n\n\n(a) Evenly distribute observations\n\n\n\n\n\nage_bins\nn\nProbability\nEmpirical Logit\n\n\n\n\n[18,27)\n158\n0.677\n0.741\n\n\n[27,34)\n154\n0.636\n0.560\n\n\n[34,40)\n178\n0.607\n0.434\n\n\n[40,45)\n141\n0.582\n0.329\n\n\n[45,50)\n134\n0.575\n0.301\n\n\n[50,57)\n157\n0.490\n-0.038\n\n\n[57,63)\n153\n0.438\n-0.250\n\n\n[63,69)\n163\n0.466\n-0.135\n\n\n[69,75)\n143\n0.462\n-0.154\n\n\n[75,89]\n140\n0.507\n0.029\n\n\n\n\n\n\n\n\n\n\n\n(b) Equal age increments\n\n\n\n\n\nage_bins\nn\nProbability\nEmpirical Logit\n\n\n\n\n(18,25]\n132\n0.667\n0.693\n\n\n(25,32]\n151\n0.642\n0.586\n\n\n(32,39]\n207\n0.618\n0.483\n\n\n(39,46]\n193\n0.549\n0.198\n\n\n(46,54]\n159\n0.572\n0.291\n\n\n(54,61]\n188\n0.463\n-0.149\n\n\n(61,68]\n181\n0.448\n-0.211\n\n\n(68,75]\n170\n0.471\n-0.118\n\n\n(75,82]\n90\n0.567\n0.268\n\n\n(82,89]\n50\n0.400\n-0.405\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we can compute the empirical logit, let’s use it to check the linearity assumption. As with linear regression, we only check linearity for quantitative predictors. To check linearity, we will make a plot of the empirical logit of the response versus the bins of the quantitative predictor. We use the mean value within each bin of the predictor to represent the bin on the graph.\nFigure 11.10 shows a plot of the empirical logit of being comfortable with driverless cars versus the mean value of age in each bin. We use the bins from Table (a) in Table 11.9, so that we know there is a similar number of observations represented by each point on the graph.\n\n\n\n\n\n\n\n\nFigure 11.10: Empirical logit of aidrive_comfort versus age\n\n\n\n\n\n\nWhen examining the plot of empirical logit versus the binned quantitative predictor, we are asking whether a line would reasonably describe the relationship between the predictor and empirical logit. The condition is satisfied if the trend generally looks linear overall. As with checking the linearity condition for linear regression, we are looking for obvious violations, not exact adherence to linearity.\nFrom Figure 11.10, it appears there is a potential quadratic relationship between age and comfort with driverless cars, and thus the linearity condition is not satisfied.  The empirical logit is lowest for respondents around 60 years old, then it appears to increase again. This prompts us to consider a quadratic term for the model.\n\n\n\n\nTable 11.10: Logistic regression model including quadratic term for age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.2106\n0.4545\n2.664\n0.0077\n0.3246\n2.1077\n\n\ntech_easyCan’t choose\n-0.1600\n0.4040\n-0.396\n0.6922\n-0.9745\n0.6227\n\n\ntech_easyAgree\n0.6832\n0.1512\n4.519\n0.0000\n0.3884\n0.9816\n\n\ntech_easyDisagree\n-0.5118\n0.2956\n-1.731\n0.0834\n-1.1070\n0.0562\n\n\nage\n-0.0498\n0.0181\n-2.755\n0.0059\n-0.0854\n-0.0145\n\n\nI(age^2)\n0.0003\n0.0002\n1.967\n0.0492\n0.0000\n0.0007\n\n\n\n\n\n\n\n\n\nTable 11.10 is the output of the model that includes the quadratic term for age. The p-value for the quadratic term is around 0.05, so based on this alone, it is unclear whether or not to keep the squared term in the model. Thus, let’s look at the confidence interval and some model comparison statistics introduced in Section 10.2 to have more information to take into account regarding the quadratic term.\nThe 95% confidence interval for the coefficient of \\(\\text{age}^2\\) is 1.955^{-6} to 6.802^{-4}. This shows the quadratic term has a very small adjustment on the relationship between age and comfort with driverless cars. Even if the quadratic term is statistically significant, it may not be practically significant.\n\n\n\n\nTable 11.11: AIC and BIC for models with and without quadratic term for age\n\n\n\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nAge\n2036\n2062\n\n\nAge + Age^2\n2034\n2066\n\n\n\n\n\n\n\n\nIn Table 11.11, we look at the AIC and BIC for the model with and without the quadratic term for age. The model that includes \\(\\text{age}^2\\) has a lower AIC but a higher BIC. The difference in the AIC values is small, indicating that that one model is not preferenced over the other in terms of model fit. The same is true when comparing the values of BIC. Therefore, with a mind towards choosing the most parsimonious (simplest) model without hindering model performance, we choose to remove the quadratic term and move forward including only the main effect for age.\n\n\n11.6.2 Independence\nThe independence assumption for logistic regression is similar to the assumption for linear regression, that there are no dependencies between observations. As with linear regression, this assumption is important, because we conduct inference assuming we have \\(n\\) pieces of independent information produced by the \\(n\\) observations. If observations there is dependency between observations, then we are effectively working with less than \\(n\\) pieces of independent information, as knowing something about one observation tells a lot about all the others that are correlated with it.\nWe typically evaluate independence based on a description of the observations and the data collection process. If there are potential dependency issues based on the order in which the data were collected, spatial correlation, or other sub group dependencies, we can use visualizations to plot the odds or empirical logit of the response by time, space, or subgroup, respectively. We can also add predictors in the model to account for those potential dependencies.\nFor this analysis on comfort with driverless cars, the independence condition is satisfied. Based on the description of the sample and data collection process in Section 11.1, we can reasonably conclude there are no dependencies between respondents.\n\nWe conduct logistic regression assuming the data are a representative sample from the population of interest. Ideally, it would be a random sample, but that is not always feasible in practice. Therefore, even if the sample is not completely random, it should be representative of the population.\nIf there are biases in the sample (e.g., a particular subgroup is over or under represented in the data), they will influence the interpretations, conclusions, and predictions from the model. Therefore, we can narrow the scope of the population for the analysis, or clearly disclose these biases when presenting the results.\nThis issue often occurs in survey data, like the data we’re analyzing in this chapter. Data scientists analyzing survey data will often include weights in their analysis so that the sample looks more representative of the population in the analysis calculations even if the original data has some bias.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#sec-logistic-r",
    "href": "11-logistic.html#sec-logistic-r",
    "title": "11  Logistic regression",
    "section": "11.7 Logistic regression in R",
    "text": "11.7 Logistic regression in R\n\n11.7.1 Fitting a logistic model\nThe logistic regression model is fit using glm() in the stats package in base R. This function is used to fit a variety of models that are part of the wider class of models called generalized linear models, so we must also specify which generalized linear model is being fit when using this function. The argument family = binomial is specifies that the model fit is a logistic regression model.\n\ncomfort_tech_age_fit &lt;- glm(aidrive_comfort ~ tech_easy + age,\n                          data = gss24_ai, \n                          family = binomial)\n\nWe input the observed categorical response variable and R does all computations behind the scenes to produce the model in terms of the logit. The response variable must be coded as a character (&lt;chr&gt;) or factor (&lt;fct&gt;) type.\nThe tidy function produces the model output in a tidy form and kable() can be used to neatly format the results to a specific number of digits. The argument conf.int = TRUE shows the confidence interval, and conf.level = is used to set the confidence level.\n\ntidy(comfort_tech_age_fit, conf.int = TRUE ,conf.level = 0.95) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.420\n0.208\n2.020\n0.043\n0.012\n0.828\n\n\ntech_easyCan’t choose\n-0.118\n0.402\n-0.294\n0.769\n-0.928\n0.661\n\n\ntech_easyAgree\n0.678\n0.151\n4.487\n0.000\n0.383\n0.976\n\n\ntech_easyDisagree\n-0.499\n0.295\n-1.690\n0.091\n-1.094\n0.068\n\n\nage\n-0.015\n0.003\n-4.902\n0.000\n-0.021\n-0.009\n\n\n\n\n\nBy default, the tidy function will show the model output for the response in terms of the logit. The argument exponentiate = TRUE will produce the output for the model in terms of the odds with the exponentiated coefficients. The argument, exponentiate is set to FALSE by default.\n\ntidy(comfort_tech_age_fit, conf.int = TRUE ,conf.level = 0.95, \n     exponentiate = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.522\n0.208\n2.020\n0.043\n1.012\n2.289\n\n\ntech_easyCan’t choose\n0.889\n0.402\n-0.294\n0.769\n0.395\n1.936\n\n\ntech_easyAgree\n1.969\n0.151\n4.487\n0.000\n1.467\n2.653\n\n\ntech_easyDisagree\n0.607\n0.295\n-1.690\n0.091\n0.335\n1.071\n\n\nage\n0.985\n0.003\n-4.902\n0.000\n0.979\n0.991\n\n\n\n\n\n\n\n11.7.2 Simulation-based inference\nThe code for simulation-based inference is the same for logistic regression as the code for linear regression introduced in Section 4.8. Behind the scenes, the functions in the infer R package use the response variable to determine whether to fit a linear or logistic regression model.\nBelow is the code for the bootstrap confidence intervals and permutation test conducted in Section 11.5.1. We refer the reader to Section 8.7.1 for more a more detailed explanation about the code.\nPermutation test\n\n# set seed\nset.seed(12345)\n\n# construct null distribution using permutation sampling\nnull_dist &lt;- gss24_ai |&gt; \n  specify(aidrive_comfort ~ tech_easy + age) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\", variables = age) |&gt;\n  fit()\n\n# compute observed coefficient\nobs_fit &lt;- gss24_ai |&gt; \n  specify(aidrive_comfort ~ tech_easy + age) |&gt; \n  fit()\n\n# compute p-value\npval &lt;- get_p_value(null_dist, obs_stat = obs_fit, direction = \"two-sided\")\n\nBootstrap confidence interval\n\n# set seed\nset.seed(12345)\n\n# construct bootstrap distribution \nboot_dist &lt;- gss24_ai |&gt;\n  specify(aidrive_comfort ~ tech_easy + age) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\n# compute confidence interval\nci &lt;- get_confidence_interval(\n    boot_dist, \n    level = 0.95, \n    point_estimate = obs_fit\n  )\n\n\n\n11.7.3 Empirical logit plots\nThe code to compute the empirical logit primarily utilizes data wrangling functions in the dplyr R package (Wickham et al. 2023a).\nEmpirical logit for groups of a categorical variable\nThe code below produces the empirical logit for being comfortable with driverless cars for each level of tech_easy, opinion on whether technology makes life easier.\n\n1gss24_ai |&gt;\n2  group_by(tech_easy) |&gt;\n3  count(aidrive_comfort) |&gt;\n4  mutate(probability = n / sum(n),\n         empirical_logit = log(probability/(1 - probability)))\n\n\n1\n\nUse the gss24_ai data frame.\n\n2\n\nCompute all subsequent calculations separately for each level of tech_easy.\n\n3\n\nCount the number of observations such that aidrive_comfort == 0, and the number of observations such that aidrive_comfort == 1.\n\n4\n\nCompute the probability that aidrive_comfort == 0 and aidrive_comfort == 1 .\n\n\n\n\n# A tibble: 8 × 5\n# Groups:   tech_easy [4]\n  tech_easy    aidrive_comfort     n probability empirical_logit\n  &lt;fct&gt;        &lt;fct&gt;           &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1 Neutral      0                 127       0.585           0.344\n2 Neutral      1                  90       0.415          -0.344\n3 Can't choose 0                  18       0.6             0.405\n4 Can't choose 1                  12       0.4            -0.405\n5 Agree        0                 495       0.412          -0.355\n6 Agree        1                 706       0.588           0.355\n# ℹ 2 more rows\n\n\nThe code to compute the empirical logit based on a quantitative predictor is the similar, with the additional step of splitting the quantitative predictor into bins. There are many ways to do this in R; here we will show the functions used for the tables in Section 11.6.\nThe first is using the cut() function that is part of base R. This function transforms numeric variable types into factor variable types by dividing them into bins. The argument breaks = is used to either specify the number of bins or explicitly define the bins. If the number of bins is specified, cut() will divide the observations into bins of equal length, as shown in the code below.\nOnce the bins are defined, the remainder of the code is the same as before, where the probability and empirical logit are computed for each bin. Here is the code to compute the empirical logit of comfort with driverless cars by age, where age is divided into 10 bins. Here, the cut() function divided the age into bins of length 7.\nNote that the option dig.lab = 0 in the cut() function means to display the bin thresholds as integers, i.e., with 0 digits. This only changes how the lower and upper bounds are displayed; it does not change any computations.\n\ngss24_ai |&gt; \n  mutate(age_bins = cut(age, breaks = 10, dig.lab = 0)) |&gt;\n  group_by(age_bins) |&gt;\n  count(aidrive_comfort) |&gt;\n  mutate(probability = n / sum(n)) |&gt;\n  mutate(empirical_logit = log(probability/(1 - probability))) \n\n# A tibble: 20 × 5\n# Groups:   age_bins [10]\n  age_bins aidrive_comfort     n probability empirical_logit\n  &lt;fct&gt;    &lt;fct&gt;           &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1 (18,25]  0                  44       0.333          -0.693\n2 (18,25]  1                  88       0.667           0.693\n3 (25,32]  0                  54       0.358          -0.586\n4 (25,32]  1                  97       0.642           0.586\n5 (32,39]  0                  79       0.382          -0.483\n6 (32,39]  1                 128       0.618           0.483\n# ℹ 14 more rows\n\n\nAnother option for splitting the quantitative variable into bins is to do, such that each bin has an approximately equal number of observations. To do so, we can use the cut2() function from them Hmisc R package (Harrell Jr 2025). The number of bins is specified in the g = argument, and the function will divide the observations into the number of specified bins, such that each bin has an approximately equal number of observations.\n\ngss24_ai |&gt; \n  mutate(age_bins = cut2(age, g = 10)) |&gt;\n  group_by(age_bins) |&gt;\n  count(aidrive_comfort) |&gt;\n  mutate(probability = n / sum(n)) |&gt;\n  mutate(empirical_logit = log(probability/(1 - probability))) |&gt;\n  filter(aidrive_comfort == 1) \n\n# A tibble: 10 × 5\n# Groups:   age_bins [10]\n  age_bins aidrive_comfort     n probability empirical_logit\n  &lt;fct&gt;    &lt;fct&gt;           &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1 [18,27)  1                 107       0.677          0.741 \n2 [27,34)  1                  98       0.636          0.560 \n3 [34,40)  1                 108       0.607          0.434 \n4 [40,45)  1                  82       0.582          0.329 \n5 [45,50)  1                  77       0.575          0.301 \n6 [50,57)  1                  77       0.490         -0.0382\n# ℹ 4 more rows\n\n\nWe see the bins are more evenly distributed than using the previous method, but there is not an equal number in each bin. This is because the data set is imbalanced, as there are many more respondents in the data who are 18 - 27 years old versus 75 years and older. We can try different values for g = (minimum of 5), if we wish to make the bins more evenly distributed.\nThe plot of the empirical logit versus a quantitative predictor has the mean value within each bin on the \\(x\\)-axis and the empirical logit for the bin on the \\(y\\)-axis. In the code below, we use the summarise() function in the dplyr R package (Wickham et al. 2023b) to compute the mean age within each bin. We then join the mean ages to the empirical logit data computed above  (saved as the object emplogit_age), and use the combined data to create the scatterplot.\n\n# compute mean age for each bin\nmean_age  &lt;- gss24_ai |&gt; \n  mutate(age_bins = cut(age, breaks = 10)) |&gt; \n  group_by(age_bins) |&gt;\n  summarise(mean_age = mean(age))\n\n# join the mean ages and create scatterplot\nemplogit_age |&gt; \n  left_join(mean_age, by = \"age_bins\") |&gt;\n  ggplot(aes(x = mean_age, y = empirical_logit)) +\n  geom_point() +\n  labs(x = \"Mean age\", \n       y = \"Empirical logit of (aidrive_comfort = 1)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe Stat2Data R package (Cannon et al. 2019) has built-in functions for making empirical logit plot utilizing the base R plotting functions (instead of ggplot2).\nThe emplogitplot1() function is used to create the empirical logit plot for the response versus a quantitative predictor variable. The argument ngroups = specifies the number of bins. The bins can be explicitly defined using the breaks= argument instead of ngroups.\nThe code for the empirical logit plot versus age using 10 bins is below.\n\nemplogitplot1(aidrive_comfort ~ age, data = gss24_ai, ngroups = 10)\n\n\n\n\n\n\n\n\nWe can include the argument out = TRUE save the underlying data used to make the plot. Additionally, the argument showplot = FALSE will suppress the plot, if we are only interested in the underlying data.\n\nemplogit_age_data &lt;- emplogitplot1(aidrive_comfort ~ age, data = gss24_ai, \n                                   ngroups = 10, out = TRUE, showplot = FALSE)\n\nemplogit_age_data\n\n   Group Cases XMin XMax XMean NumYes  Prop AdjProp  Logit\n1      1   158   18   26  22.8    107 0.677   0.676  0.735\n2      2   154   27   33  30.2     98 0.636   0.635  0.554\n3      3   178   34   39  36.6    108 0.607   0.606  0.431\n4      4   141   40   44  42.2     82 0.582   0.581  0.327\n5      5   134   45   49  47.1     77 0.575   0.574  0.298\n6      6   157   50   56  53.3     77 0.490   0.491 -0.036\n7      7   153   57   62  59.5     67 0.438   0.438 -0.249\n8      8   163   63   68  65.5     76 0.466   0.466 -0.136\n9      9   143   69   74  71.3     66 0.462   0.462 -0.152\n10    10   140   75   89  80.2     71 0.507   0.507  0.028\n\n\nAs we see from the output of underlying data, emplogitplot1() divides the quantitative variable into bins, such that the observations are approximately evenly distributed across bins. The result is very similar to the result from creating bins using cut2(). The underlying data in emplogit_age_data can be used to make the graph using the ggplot2 functions, as shown below.\n\nggplot(emplogit_age_data, aes(x = XMean, y = Logit )) + \n  geom_point() + \n    labs(x = \"Mean age\", \n       y = \"Empirical logit of (aidrive_comfort = 1)\") +\n  theme_bw()",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#summary",
    "href": "11-logistic.html#summary",
    "title": "11  Logistic regression",
    "section": "11.8 Summary",
    "text": "11.8 Summary\nIn this chapter we introduced logistic regression for data with a binary categorical response variable. We used probabilities and odds to describe the distribution of the response variable, and we used odds ratios to understand the relationship between the response and predictor variables. We introduced the form of the logistic regression model and interpreted the model coefficients in terms of the logit and odds. We used simulation-based and theory-based methods to draw conclusions about individual coefficients, and used model conditions to evaluate whether the assumptions for logistic regression hold in the data. We concluded by showing these elements of logistic regression analysis in R.\nIn Chapter 12, we will continue with logistic regression models and show how these models are used for prediction and classification. We will also present statistics to evaluate model fit and conduct model selection.\n\n\n\n\nCannon, Ann, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman, and Jeffrey Witmer. 2019. “Stat2Data: Datasets for Stat2.” https://doi.org/10.32614/CRAN.package.Stat2Data.\n\n\nDavern, Michael, Rene Bautista, Jeremy Freese, Pamela Herd, and Stephen L. Morgan. 2025. “General Social Survey 1972–2024.” NORC at the University of Chicago; [Machine-readable data file]. https://gss.norc.org/content/dam/gss/get-documentation/pdf/codebook/GSS%202024%20Codebook.pdf.\n\n\nHarrell Jr, Frank E. 2025. “Hmisc: Harrell Miscellaneous.” https://doi.org/10.32614/CRAN.package.Hmisc.\n\n\nHealy, Kieran. 2023. “Gssr: General Social Survey Data for Use in r.” http://kjhealy.github.io/gssr.\n\n\nLiedtke, Michael. 2025. “Amazon’s Zoox Launches Its Robotaxi Service in Las Vegas.” AP News. https://apnews.com/article/amazon-zoox-robotaxis-las-vegas-bd5cb24602fb16243efcba05c7fe518f.\n\n\nMaxmen, Amy. 2018. “Self-Driving Car Dilemmas Reveal That Moral Choices Are Not Universal.” Nature 562: 469–70. https://doi.org/10.1038/d41586-018-07135-0.\n\n\nVemoori, Vamsi. 2024. “Navigating the Ethical Dilemmas of Self-Driving Cars: Who Decides When Safety Is at Risk?” Forbes, October. https://www.forbes.com/councils/forbestechcouncil/2024/10/23/navigating-the-ethical-dilemmas-of-self-driving-cars-who-decides-when-safety-is-at-risk/.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.” Transactions of the American Mathematical Society 54 (3): 426–82.\n\n\nWhite, Edward. 2025. “Top Sensor Maker Hesai Warns World Not Ready for Fully Driverless Cars.” Financial Times, September. https://www.ft.com/content/1cea9526-17a8-4554-a660-1c1e6d69676b.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023a. “Dplyr: A Grammar of Data Manipulation.” https://doi.org/10.32614/CRAN.package.dplyr.\n\n\n———. 2023b. “Dplyr: A Grammar of Data Manipulation.” https://doi.org/10.32614/CRAN.package.dplyr.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "11-logistic.html#footnotes",
    "href": "11-logistic.html#footnotes",
    "title": "11  Logistic regression",
    "section": "",
    "text": "There does appear to be some relationship. A higher proportion of those who agree technology makes life easier have comfort with driverless cars.↩︎\n\\(Pr(Y = 1) = 829 / (692 + 829) = 0.545\\)\n\\(\\text{odds}(Y=1) = 0.545 / (1 - 0.545)  \\approx 1.2\\)\nNote results differ slightly due to rounding.↩︎\n\\(Pr(Y=1 | \\text{tech\\_easy = Agree}) = 706/ (495 + 706) = 0.588\\) and \\(\\text{odds}(Y=1 | \\text{tech\\_easy = Agree)} = 0.588/(1 - 0.588) = 706/495 = 1.43\\)↩︎\nThe odds ratio is \\(\\frac{706/495}{21/52} = 3.53\\) . This means that the odds a respondent who agrees technology makes life easier is comfortable with driverless cars are 3.53 times the odds a respondent who is disagrees that technology is making life easier is comfortable with driverless cars.↩︎\nThe baseline level is Neutral . It is the only level that is not in the model output.↩︎\nThe log odds a respondent who disagrees technology makes life easier is comfortable with driverless cars are expected to be 0.499 less than the log odds of a respondent who is neutral about whether technology makes life easier, holding age constant.\nThe odds a respondent who disagrees technology makes life easier is comfortable with driverless cars are expected to be 0.607 (\\(e^{-0.499}\\)) times the odds of a respondent who is neutral about whether technology makes life easier, holding age constant.\n\nAlternatively, we could write this in terms of an odds ratio greater than 1: The odds a respondent is neutral about whether technology makes life easier is comfortable with driverless cars are expected to be 1.647 (\\(1/e^{-0.499}\\)) times the odds of a respondent who disagrees that technology makes life easier, holding age constant.↩︎\nWhen age increases by 5, the log odds are expected to decrease by 0.075 (\\(-0.015 \\times 5\\) ) . The odds are expected to multiply by 0.928 ( \\(e^{-0.015 \\times 5}\\)).↩︎\nNote the difference the result here and the model output is because the model output is computed using exact values of \\(\\hat{\\beta}_j\\) and \\(SE_{\\hat{\\beta}_j}\\).↩︎\nWe are 95% confident that for each additional year in age, the odds of being comfortable with driverless cars are expected to multiply by 0.979 ( \\(e^{-0.021}\\) ) to 0.991 ( \\(e^{-0.009}\\)), holding opinions about technology constant. This interval is equal to the interval obtained using bootstrapping.↩︎\n\nThe test statistic is \\(\\frac{-0.499 - 0}{0.295} \\approx - 1.69\\). The estimated coefficient of -0.499 is 1.69 standard errors below the hypothesized mean of 0. The p-value 0.091 &gt; 0.05, so the data do not provide sufficient evidence the odds are different for those who disagree that technology makes life easier versus those who are neutral, after adjusting for age. The 95% confidence interval is computed as \\(-0.499 \\pm 1.96 \\times 0.295\\). It is consistent with the conclusion, because 0 is in the interval.\n\n\n↩︎",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html",
    "href": "12-logistic-prediction.html",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#learning-goals",
    "href": "12-logistic-prediction.html#learning-goals",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "",
    "text": "Compute predictions from the logistic regression model\nUse model predictions to classify observations\nConstruct and interpret a confusion matrix\nUse the ROC curve to evaluate model performance and select classification threshold\nEvaluate model performance using AUC, AIC, and BIC\nImplement a model building workflow for logistic regression using R",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#introduction-predicting-comfort-with-driverless-cars",
    "href": "12-logistic-prediction.html#introduction-predicting-comfort-with-driverless-cars",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.1 Introduction: Predicting comfort with driverless cars",
    "text": "12.1 Introduction: Predicting comfort with driverless cars\nIn Chapter 11, we introduced data from the 2024 General Social Survey in which adults in the United States their opinions were asked on a variety of issues, including their comfort with driverless cars. In the previous chapter, we fit a logistic regression model used the model to describe the characteristics associated with the odds an adult is comfortable with driverless cars. We continue with the analysis in this chapter, with a focus on using the model for prediction. We will also evaluate the model performance and show an example workflow for comparing two logistic regression models.\nWe use the variables below in this chapter. The variable definitions are based on survey prompts and variable definitions in the General Social Survey Documentation and Public Use File Codebook (Davern et al. 2025).\n\n\naidrive_comfort: Indicator variable for respondent’s comfort with driverless (self-driving) cars. 0: Not comfortable at all; 1: At least some comfort.\n\nThis variable was derived from responses to the original survey prompt: “Comfort with driverless cars”. Scores ranged from 0 to 10 with 0 representing “totally uncomfortable with this situation” and 10 representing “totally comfortable with this situation”. Responses of 0 on the original survey were coded as aidrive_comfort = 0. All other responses coded as 1.\n\n\ntech_easy : Response to the question, “Does technology make our lives easier?” Categories are Neutral, Can't choose (Respondent doesn’t know / is unable to provide an answer), Agree, Disagree.\nage: Respondent’s age in years\n\nincome: Response to the question “In which of these groups did your total family income, from all sources, fall last year? That is, before taxes.” Categories are Less than $20k, $20-50k, $50-110k, $110k or more, Not reported .\n\nNote: These categories were defined based on the 27 categories in income16 from the original survey.\n\n\ntech_harm: Response to the question, “Does technology do more harm than good?”. Categories are Neutral, Can't choose (Respondent doesn’t know / is unable to provide an answer), Agree, Disagree.\n\npolviews: Response to the question, “I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal–point 1–to extremely conservative–point 7. Where would you place yourself?” Categories are Moderate, Liberal, Conservative, No reported.\n\nNote: These categories were defined based on the original 7 point scale.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#exploratory-data-analysis",
    "href": "12-logistic-prediction.html#exploratory-data-analysis",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.2 Exploratory data analysis",
    "text": "12.2 Exploratory data analysis\n\n12.2.1 Univariate EDA\nWe conducted exploratory data analysis for the response variable aidrive_comfort and the predictors age and tech_easy in Section 11.1. Here we focus on EDA for the new predictors and their relationship with the response variable.\n\n\n\n\n\n\n\n\n\n(a) income\n\n\n\n\n\n\n\n\n\n(b) polviews\n\n\n\n\n\n\n\n\n\n\n\n(c) tech_harm\n\n\n\n\n\n\nFigure 12.1: Univariate exploratory data analysis\n\n\nFigure 12.1 shows the distributions of the predictors that are new in this chapter. From the distribution of income in Figure 12.1 (a) , we see that the most common response for income is between $50-110K. There is a sizable proportion of the respondents who did not report an income. Studies have shown that failure to report income in surveys is not random (e.g., Jabkowski and Piekut 2024), so it will be worth noting if the Not Reported indicator has a statistically significant relationship with the response variable as we continue the analysis.\nThe distribution of polviews_fct in Figure 12.1 (b) shows a relatively even distribution along the range of political views. A few respondents chose not to report their political views. Lastly, the distribution of tech_harm in Figure 12.1 (c) shows that most people either disagree are have neutral feelings about the statement that technology causes more harm than good.\n\n12.2.2 Bivariate EDA\n\n\n\n\n\n\n\n\n\n(a) aidrive_comfort vs. income\n\n\n\n\n\n\n\n\n\n(b) aidrive_comfort vs. polviews\n\n\n\n\n\n\n\n\n\n\n\n(c) aidrive_comfort vs. tech_harm\n\n\n\n\n\n\nFigure 12.2: Bivariate exploratory data analysis. Blue: adrive_comfort = 0, Red: aidrive_comfort = 1\n\n\nThe visualizations in Figure 12.2 show the relationships between the response variable and each new predictor variable. Figure 12.2 (a) shows the relationship between income and aidrive_comfort. The graph shows that a higher proportion of respondents in the higher income categories are comfortable with driverless cars compared to respondents in lower income categories or who did not report income. This suggests an individual’s income may be useful in understanding the chance they are comfortable with driverless cars.\nThe relationship between polviews and aidrive_comfort is in Figure 12.2 (b). Those who identify as “liberal” on the political spectrum are the most likely to be comfortable with driverless cars, and those who did not report a political affiliation are the least likely. Those who identify as “moderate” or “conservative” have about the same odds of being comfortable with driverless cars.\nLastly, Figure 12.2 (c) is the relationship between tech_harm and aidrive_comfort. Those who disagree that technology causes more harm than good are the most likely to be comfortable with driverless cars. Those who did not provide a response or agree that technology causes more harm than good are the least likely to be comfortable with driverless cars.\n\n12.2.3 Initial model\nWe begin by fitting a model using the predictors from Chapter 11, age and tech_easy, along with a new predictor income to predict whether an individual is comfortable with driverless cars. We’ll use this model for the majority of the chapter as we introduce prediction, classification, and model assessment for logistic regression. In Section 12.7, we’ll use cross validation and the model selection workflow from Chapter 10 to compare this model to another one that also includes polviews and tech_harm.\n\n\n\nTable 12.1: Model of aidrive_comfort versus age, tech_easy, and income with 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.032\n0.237\n0.136\n0.892\n-0.433\n0.496\n\n\nage\n-0.016\n0.003\n-5.165\n0.000\n-0.022\n-0.010\n\n\ntech_easyCan’t choose\n0.075\n0.408\n0.184\n0.854\n-0.746\n0.866\n\n\ntech_easyAgree\n0.576\n0.155\n3.730\n0.000\n0.275\n0.881\n\n\ntech_easyDisagree\n-0.436\n0.300\n-1.456\n0.145\n-1.039\n0.140\n\n\nincome_fct$20-50k\n0.264\n0.177\n1.491\n0.136\n-0.082\n0.612\n\n\nincome_fct$50-110k\n0.526\n0.169\n3.111\n0.002\n0.196\n0.860\n\n\nincome_fct$110k or more\n1.223\n0.182\n6.704\n0.000\n0.868\n1.584\n\n\nincome_fctNot reported\n0.265\n0.219\n1.212\n0.225\n-0.164\n0.694\n\n\n\n\n\n\n\n\n\nConsider the coefficients for the indicators of income_fct. Are they consistent with the observations from the EDA? Why or why not. 1",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#prediction-and-classification",
    "href": "12-logistic-prediction.html#prediction-and-classification",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.3 Prediction and classification",
    "text": "12.3 Prediction and classification\nIn Chapter 11, we introduced the logistic regression, where we used the model to describe and draw conclusions about the relationship between the response and predictor variables. In practice, logistic regression models are widely used for classification, particularly in data science and machine learning. They are part of a branch of machine learning models called supervised learning, in which the model is built and evaluated using data that contains observed outcomes. Therefore, we will now focus on using the model to predict whether an individual is comfortable with self-driving cars given particular characteristics.\n\n12.3.1 Prediction\nRecall from Section 11.3 that the response variable in the logistic regression model is the logit (log-odds). When we input values of age, tech_easy, and income into the model in Table 12.1, the model will output the log odds an individual with those characteristics is comfortable with driverless cars. Once we have the predicted log odds, we can use the relationships in Section 11.2 to compute the predicted odds and the predicted probability.  Table 12.2 shows the predicted log odds, predicted odds, and predicted probability for 10 observations in the data set.\n\n\n\nTable 12.2: Predictions from model in Table 12.1 for 10 respondents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naidrive_comfort\nage\ntech_easy\nincome\nPred. log odds\nPred. odds\nPred. probability\n\n\n\n1\n1\n33\nAgree\n$110k or more\n1.306\n3.692\n0.787\n\n\n2\n1\n19\nNeutral\n$110k or more\n0.953\n2.593\n0.722\n\n\n3\n0\n25\nCan’t choose\nNot reported\n-0.026\n0.974\n0.494\n\n\n4\n0\n68\nNeutral\nLess than $20k\n-1.052\n0.349\n0.259\n\n\n5\n0\n63\nCan’t choose\nLess than $20k\n-0.897\n0.408\n0.290\n\n\n6\n0\n31\nAgree\n$50-110k\n0.641\n1.899\n0.655\n\n\n7\n0\n63\nAgree\n$110k or more\n0.828\n2.288\n0.696\n\n\n8\n1\n41\nAgree\n$50-110k\n0.482\n1.619\n0.618\n\n\n9\n1\n75\nAgree\n$20-50k\n-0.323\n0.724\n0.420\n\n\n10\n1\n51\nAgree\n$110k or more\n1.019\n2.771\n0.735\n\n\n\n\n\n\n\n\nLet’s show how the predicted values for the first observation are computed. The predicted log odds the first individual who is 33 years old, agrees that technology makes life better, and has an annual income of $110k or more are\n\\[\n\\begin{aligned}\n\\log \\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) &=  0.032 - 0.016  \\times 33 + 0.075   \\times 0 +  0.576 \\times 1 \\\\\n&- 0.436  \\times 0 +  0.264 \\times 0 +  0.526  \\times 0 \\\\\n& + 1.223  \\times 1 + 0.265 \\times 0 \\\\\n& = 1.3\n\\end{aligned}\n\\]\nwhere \\(\\hat{\\pi}\\) is the predicted probability of being comfortable with driverless cars.\nUsing the predicted log odds from Table 12.2, the predicted odds for this individual are\n\\[\\widehat{\\text{odds}} = e^{\\log \\big( \\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\big)} = e^{1.306} = 3.69 \\]\nLastly, we use the odds from Table 12.2 to compute the predicted probability:\n\\[\n\\hat{\\pi} = \\frac{\\widehat{\\text{odds}}}{1 + \\widehat{\\text{odds}}} = \\frac{3.692}{1 + 3.692} = 0.787\n\\]\nNote: These values may differ slightly from the values in the the table, because we are computing predictions using rounded coefficient.\n\nShow how to compute the predicted log odds, odds, and probability for individual #2 in Table 12.2.2\n\n\n12.3.2 Classification\nKnowing the predicted odds and probabilities can be useful for understanding how likely individuals with various characteristics will be comfortable with driverless cars. In many contexts, however, we would like to group individuals based on whether or not the model predicts they are comfortable with driverless cars. For example, the marketing team for a robotaxi company may want to use targeted marketing strategies and offer discounts to potential new customers. To have a successful marketing campaign, they want to direct the marketing to those who are comfortable with driverless cars.\nAs we’ve seen thus far, the logistic regression model does not directly produce predicted values of the binary response variable. Therefore, we can group observations based on the predicted probabilities computed from the model output. This process of grouping observations based on the predictions is called classification. The groups the observations are put into are the predicted classes. In the case of our analysis, we will use the model to classify observations into the class of those not comfortable with driverless cars (aidrive_comfort = 0) or the class of those comfortable with driverless cars (aidrive_comfort = 1) .\n\nWe showed how to compute the predicted probabilities from the logistic regression output in Section 12.3.1, and we will use those probabilities to classify observations. The question, then, is how large does the probability need to be to classify an observation as having the response \\(Y = 1\\) ? In terms of our analysis, how large does the probability of being comfortable with driverless cars need to be to classify an individual as being comfortable with driverless cars, aidrive_comfort = 1?\nWhen using the logistic regression model for classification, we define a threshold, such an observation is classified as \\(\\hat{Y} = 1\\) if the predicted probability is greater than the threshold. Otherwise, the observation is classified as \\(\\hat{Y} = 0\\). If we’re unsure what threshold to set, we can start with a threshold of 0.5, the default threshold typically used in statistical software. This means if the model predicts an observation is more likely than not to have response \\(Y = 1\\), even if just by a small amount, then the observation is classified as having response \\(\\hat{Y} = 1\\).\nFor now, let’s use the threshold equal to 0.5 to assign the predicted classes of aidrive_comfort for the respondents in the sample data based on the predicted probabilities produced from the model in Table 12.1.\n\n\n\nTable 12.3: Predicted class based on model in Table 12.1 and threshold of 0.5 for 10 respondents\n\n\n\n\n\naidrive_comfort\nPred. probability\nPred. class\n\n\n\n1\n1\n0.787\n1\n\n\n2\n1\n0.722\n1\n\n\n3\n0\n0.494\n0\n\n\n4\n0\n0.259\n0\n\n\n5\n0\n0.290\n0\n\n\n6\n0\n0.655\n1\n\n\n7\n0\n0.696\n1\n\n\n8\n1\n0.618\n1\n\n\n9\n1\n0.420\n0\n\n\n10\n1\n0.735\n1\n\n\n\n\n\n\n\n\nTable 12.3 shows the observed value of the response (aidrive_comfort), predicted probability, and predicted class for ten respondents. For many of these respondents, the observed and predicted classes are equal. There are some, such as Observation 6, in which the predicted classes differs from the observed. In this instance, the respondent had a combination of age, income, and tech_easy, that are associated with a higher probability of being comfortable with driverless cars; however, the individual responded they are not comfortable with driverless cars in the General Social Survey.\nWe need a way to more holistically evaluate how well the predicted classes align with the observed classes. To do so, we use a confusion matrix, a \\(2 \\times 2\\) table of the observed classes versus the predicted classes.\n\n\n\nTable 12.4: Confusion matrix for model in Table 12.1 and threshold of 0.5. Observed class (columns), Predicted class (rows)\n\n\n\n\n\n\n\n\n\n\n\n\nObserved class\n\n\n\nPred. class\n0\n1\n\n\n\n\n0\n349\n228\n\n\n1\n343\n601\n\n\n\n\n\n\n\n\n\nTable 12.4 shows the confusion matrix for the model in Table 12.1 and a threshold of 0.5. In this table, the rows define the predicted classes and the columns define the observed classes. Let’s break down what each cell is in the table:\n\nThere are 349 observations with the predicted class of 0 and observed class of 0.\nThere are 228 observations with the predicted class of 0 and observed class of 1.\nThere are 343 observations with the predicted class of 1 and observed class of 0.\nThere are 601 observations with the predicted class of 1 and observed class of 1.\n\nWe compute various statistics from the confusion matrix to evaluate how well the observations are classified. The first statistics we can calculate are the accuracy and misclassification rate. The accuracy is the proportion  of observations that are correctly classified (the observed and predicted classes are the same). The accuracy based on Table 12.4 is\n\\[\n\\text{accuracy} = \\frac{349 + 601}{349 + 228 + 343 + 601} = 0.625\n\\tag{12.1}\\]\nUsing the model in Table 12.1 and the threshold of 0.5, 62.5% of the observations are correctly classified.\nThe misclassification rate is the proportion of observations that are not correctly classified (the observed and predicted classes differ). The misclassification rate based on Table 12.4 is\n\\[\n\\text{misclassification} = \\frac{228 + 343}{349 + 228 + 343 + 601} = 0.375\n\\tag{12.2}\\]\nUsing the model in Table 12.1 and the threshold of 0.5, 37.5% of the observations are incorrectly classified. Note that the misclassification rate is equal to \\(1 - \\text{accuracy}\\) ,and vice versa.\n\nWhen the distribution of the response variable is largely imbalanced, the accuracy can be a misleading measure of how well the observations are classified. For example, suppose there are 100 observations, such that 5% of the observations have an observed response \\(Y = 1\\), and 95% of the observations have an observed response of \\(Y = 0\\). We may observe an imbalanced distribution like this when building a model to detect the presence of a rare disease, for example.\n\nLet’s suppose based on the model and threshold, all observations have a predicted class of 0, and the confusion matrix looks like the following:\n\n\n\n\n\n\n\n\n\n\n\n\nObserved class\n\n\n\nPred. class\n0\n1\n\n\n\n\n0\n95\n5\n\n\n1\n0\n0\n\n\n\n\n\n\nThe accuracy for this model will be (95 + 0 ) / (95 + 5) = 0.95. Based on this value, it appears the classification has performed very well, even though we did not accurately classify any of the observations in which the observed response is \\(Y = 1\\).\n\nThe accuracy and misclassification rate provide a nice initial indication of the how well the model classifies observations, but they do not give a complete picture. For example, suppose we want to know how many of the people who are actually comfortable with driverless cars are predicted to be comfortable based on the model predictions and threshold. Or suppose we want to know how many people who are actually not comfortable with driverless cars were incorrectly classified as being comfortable. To answer these and similar questions, let’s take a more detailed look at the confusion matrix.\n\n\nTable 12.5: Detailed confusion matrix\n\n\n\n\n\n\n\n\n\n\nNot comfortable with driverless cars \\((y_i = 0)\\)\n\n\nComfortable with driverless cars \\((y_i = 1)\\)\n\n\n\n\n\nClassified not comfortable\\((\\hat{y}_i = 0)\\)\n\nTrue negative (TN)\nFalse negative (FN)\n\n\n\nClassified comfortable\\((\\hat{y}_i = 1)\\)\n\nFalse positive (FP)\nTrue positive (TP)\n\n\n\n\n\n\n\nTable 12.5 shows in greater detail what is being quantified in each cell of the confusion matrix. We will use these values to compute more granular values about the classification. As in Table 12.4, the rows define the predicted classes and the columns define the observed classes. The values in the cell indicate the following:\n\nTrue negative (TN): The number of observations that are predicted to be not comfortable with driverless cars ( \\(\\hat{y}_i = 0\\)) and have observed response of not comfortable (\\(y_i = 0\\)) .\nFalse negative (FN): The number of observations that are predicted to be not comfortable with driverless cars ( \\(\\hat{y}_i = 0\\)) and have observed response of comfortable ( \\(y_i = 1\\)) .\nFalse positive (FP): The number of observations that are predicted to be comfortable with driverless cars ( \\(\\hat{y}_i = 1\\)) and have observed response of not comfortable ( \\(y_i = 0\\)) .\nTrue positive (TP): The number of observations that are predicted to be comfortable with driverless cars ( \\(\\hat{y}_i = 1\\)) and have observed response of comfortable ( \\(y_i = 1\\)) .\n\nUsing these definitions, the general form of accuracy computed in Equation 12.1 is\n\\[\n\\text{accuracy} = \\frac{\\text{True negative} + \\text{True positive}}{\\text{True negative} + \\text{False negative} + \\text{False positive} + \\text{True positive}}\n\\tag{12.3}\\]\n\nWrite the general equation for the misclassification computed in Equation 12.2 using the terms in Table 12.5.3\n\nNow let’s take a look at additional statistics that help us quantify how well the observations are classified. First, we’ll focus on the column containing those who have observed values \\(y_i = 1\\).\nThe sensitivity (true positive rate) is the proportion of those with observed \\(y_i = 1\\) that were correctly classified as \\(\\hat{y}_i = 1\\). In machine learning contexts, this value is also called recall or probability of detection.\n\\[\n\\text{Sensitivity} = \\frac{\\text{True positive}}{\\text{False negative} + \\text{True positive}}\n\\tag{12.4}\\]\nThe false negative rate is the proportion of those with observed \\(y_i = 1\\) that were incorrectly classified as \\(\\hat{y}_i = 0\\).\n\\[\n\\text{False negative rate} = \\frac{\\text{False negative}}{\\text{False negative} + \\text{True positive}}\n\\tag{12.5}\\]\nThe false negative rate is equal to \\(1 - \\text{Sensitivity}\\) and vice versa. The denominators for the false negative rate and the sensitivity are the total number of observations with observed response \\(y_i = 1\\). In terms of our analysis, this is the total number of people who responded they are comfortable with driverless cars in the General Social Survey.\nNext, we look at the column of containing those who have observed values of \\(y_i =0\\).\nThe specificity (true negative rate) is the proportion of those with observed \\(y_i = 0\\) who were correctly classified as \\(\\hat{y}_i = 0\\).\n\\[\n\\text{Specificity} = \\frac{\\text{True negative}}{\\text{True negative} + \\text{False positive}}\n\\tag{12.6}\\]\nThe false positive rate is the proportion of those with observed \\(y_i = 0\\) who were incorrectly classified as \\(\\hat{y}_i = 1\\). In machine learning contexts, this value is also called the probability of false alarm.\n\\[\n\\text{False positive rate} = \\frac{\\text{False positive}}{\\text{True negative} + \\text{False positive}}\n\\tag{12.7}\\]\nThe false positive rate is equal to \\(1 - \\text{specificity}\\). The denominators for the specificity and false positive rate are the total number of observations with observed response \\(y_i = 0\\). In terms of our analysis, this is the total number of people who responded they are not comfortable with driverless cars in the General Social Survey.\nThe values shown thus far quantify how well observations are classified based on the observed response. Another question that is often of interest is among those with predicted class of \\(\\hat{y}_i = 1\\), how many actually have observed values of \\(y_i = 1\\)? This value is called the precision.\n\\[\n\\text{Precision} = \\frac{\\text{True positive}}{\\text{False positive} + \\text{True positive}}\n\\tag{12.8}\\]\nNow the denominator is the number of observations that have a predicted class \\(\\hat{y}_i = 1\\), the second row in the Table 12.5. In the context of our analysis, the precision is how many of the individuals who are predicted to be comfortable with driverless cars actually responded on the General Social Survey that they are comfortable with driverless cars. If we’re using the model to identify individuals for a targeted marketing campaign, the precision can be useful in quantifying whether the marketing will generally capture those who are actually comfortable with driverless cars or if a large proportion would be aimed at those who actually aren’t comfortable with driverless cars and likely won’t become robotaxi customers.\n\nUse Table 12.4 to compute the following:\n\nSensitivity\nSpecificity\nPrecision4\n\n\nWe have shown how we can derive a lot of useful information from the confusion matrix. As we use the confusion matrix to evaluate how well observations are classified; however, we need to keep in mind that the confusion matrix is determined based on the model predictions and the threshold for classification. For example, in Table 12.6, we see a confusion matrix for the same model in (aidrive-comfort-fit?) using the threshold of 0.3.\n\n\n\nTable 12.6: Confusion matrix using model in Table 12.1 and threshold of 0.3\n\n\n\n\n\n\n\n\n\n\n\n\nObserved class\n\n\n\nPred. class\n0\n1\n\n\n\n\n0\n70\n15\n\n\n1\n622\n814\n\n\n\n\n\n\n\n\nDue to the low threshold, there are many more observations classified as \\(\\hat{y}_i = 1\\) compared to Table 12.4. The accuracy rate is now 58.1% and the misclassification rate is 41.9% even though the model hasn’t changed. From this example, we see that even if the model is unchanged, the metrics computed from the confusion matrix will differ based on the threshold. Therefore, we would like a way to evaluate the model performance independent from the choice of threshold.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#sec-roc",
    "href": "12-logistic-prediction.html#sec-roc",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.4 ROC Curve",
    "text": "12.4 ROC Curve\nIdeally, we want a way to evaluate the the model performance regardless of threshold and then choose a threshold that results in the “best” classification as determined by the statistics from the previous section. We could make confusion matrices across a range of thresholds, but this process could be cumbersome and time consuming to make so many confusion matrices. Rather than make individual confusion matrices, we will use the receiving operator characteristic (ROC) curve shown in Figure 12.3. The ROC curve is a single visualization to holistically evaluate the model fit and see how well the model classifies at different thresholds. We can use the data from the ROC curve to choose a classification threshold. \n\n\n\n\n\n\n\nFigure 12.3: ROC curve for model in Table 12.1 with point marked at threshold = 0.4754\n\n\n\n\n\nFigure 12.3 is the ROC curve for the model in Table 12.1. The \\(x\\)-axis on the ROC curve is \\(1 - \\text{Specificity}\\), the false positive rate, and the \\(y\\)-axis is \\(\\text{Sensitivity}\\), the true positive rate. Thus, the ROC curve is a visualization of the true positive rate versus the false positive rate at classification thresholds ranging from 0 to 1 (equivalent to the log odds ranging from \\(-\\infty\\) to \\(\\infty\\)). The diagonal line represents a model fit in which the true positive rate and false positives are equal regardless of the threshold. This means the model is unable to distinguish the observations that actually have an observed response of \\(y_i = 1\\) versus those that do not, so it is essentially the same as using a coin flip to classify observations (not a good model!). In contrast, ROC curves that hit closer to the top-left corner indicate a model that is good at distinguishing true positives and false positives.\n\n\n\n\n\n\n\n\n\n(a) Poor distinction\n\n\n\n\n\n\n\n\n\n(b) Good distinction\n\n\n\n\n\n\n\n\n\n(c) Nearly perfect distinction\n\n\n\n\n\n\nFigure 12.4: Example ROC curves for different model performs\n\n\n\nFigure 12.4 shows example ROC curves for different model fits. Figure 12.4 (a) is the ROC curve for a model that does a poor job distinguishing between the true positives and false positives (close to the diagonal line) and Figure 12.4 (c) is the ROC curve for a model that almost perfectly distinguishes between the true and false positives (close to the top-left corner). Generally, we expect to see ROC curves somewhere in the middle, similar to Figure 12.4 (b). Here, the model generally does a good job distinguishing between the true and false positives, but we will expect to get some false positives if we want a high true positive rate (sensitivity).\nEach point in the ROC curve is \\((1 - \\text{Specificity, } \\text{Sensitivity})\\) at a given threshold and can be thought of as representing an individual confusion matrix for a given threshold. For example, the point marked in red on the ROC curve in Figure 12.3 shows corresponds to \\(1 - \\text{Specificity} = 0.525\\) and \\(\\text{Sensitivity} = 0.75\\). This point corresponds to the classification threshold 0.4888088. The corresponding confusion matrix is shown in Table 12.7. Observations with predicted probability \\(\\hat{\\pi}_i &gt; 0.4888088\\) are classified as being comfortable with driverless cars, and those with \\(\\hat{\\pi}_i \\leq 0.4888088\\) are classified as not being comfortable with driverless cars.\n\n\n\nTable 12.7\n\n\n\n\n\n\n\n\n\n\n\n\nObserved class\n\n\n\npred_class3\n0\n1\n\n\n\n\n0\n329\n207\n\n\n1\n363\n622\n\n\n\n\n\n\n\n\n\n\nCompute the sensitivity and specificity from Table 12.7 and compare these to the values observed on the curve in ?fig-aidrive-roc.\n\nIn the next section, we will talk more about using the ROC curve to evaluate the model fit beyond a visual assessment. For now, let’s discuss how to use the ROC curve to determine a probability threshold for classification. When we use a model for classification, we want a high true positive rate and a low false positive rate. Therefore, one of the most straightforward ways of using the ROC curve to identify a classification threshold is to choose the threshold that corresponds to the point on the curve that is closest to the top-left corner.  . There are many ways to identify this point mathematically. For example, we can find the combination of \\(\\text{Sensitivity}\\) and \\(1 - \\text{Specificity}\\) that minimizes the following:\n\\[\n\\sqrt{(1 - \\text{Specificity})^2 + (\\text{Sensitivity} - 1)^2}\n\\]\n\nIn terms of the ROC curve in ?fig-aidrive-roc, this point is at a false positive rate of and true positive rate of 0.612, corresponding to a threshold of 0.552.\nWhile this may be a reasonable approach for identifying a threshold in many contexts, we often need to consider the practical implications when making analysis decisions in practice. For example, suppose we build a logistic regression model to diagnosis a medical illness. A classification of “1” means the patient has the illness and undergoes a treatment plan. A classification of “0” means the patient does not have the illness and does not undergo a treatment plan.\n\nWhat is a “true positive” in this scenario? What is a “false positive”?5\n\nWhen determining the probability threshold for classification, we need to think carefully about the implications of the analysis decision. More specifically, one thing we need to consider is the severity of the treatment the patient will undergo. If the treatment is minimal, then perhaps we might be willing to set a threshold that results in more false positives. If the treatment is very invasive, however, we want to minimize false positives. Otherwise, there will be many patients who do not need the treatment who will undergo an invasive treatment. Similarly, we want to consider the implications of lower sensitivity and not diagnosing individuals who actually have the illness.\n\n\nIf the main objective is high sensitivity, do we set a probability threshold closer to 0 or to 1?\nIf the main objective is high specificity, do we set a probability threshold closer to 0 or to 1?6",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#sec-logistic-model-eval",
    "href": "12-logistic-prediction.html#sec-logistic-model-eval",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.5 Model evaluation and comparison",
    "text": "12.5 Model evaluation and comparison\nIn Section 12.3.2, we discussed methods for evaluating how well a model classifies observations at a given threshold. As in linear regression, we want to quantify the overall model fit, independent of the threshold, so we can evaluate how well the model fits the data and compare multiple models.\n\n12.5.1 Area Under the Curve (AUC)\nThe ROC curve visualizes how well the model differentiates between true positives and false positives for the full range of classification thresholds 0 to 1. Therefore, in addition to helping us identify a classification threshold, it can be used to evaluate the model performance. The Area Under the Curve (AUC) is a measure of the model performance and is computed as the area under the ROC curve. The values of AUC range from 0.5 to 1. An \\(AUC = 0.5\\) corresponds to an ROC curve on the diagonal line, indicating the model is unable to distinguish between true and false positives. An \\(AUC = 1\\) corresponds to a curve that meets the top-left corner, indicating the model is able to perfectly distinguish between true and false positives.\n\n\n\n\n\n\n\n\n\n(a) AUC = 0.57\n\n\n\n\n\n\n\n\n\n(b) AUC = 0.79\n\n\n\n\n\n\n\n\n\n(c) AUC = 0.99\n\n\n\n\n\n\nFigure 12.5: Examle ROC curves with corresponding AUC\n\n\nFigure 12.5 shows the ROC curves from Figure 12.4 along with the AUC for each curve. Similar to \\(R^2\\) for linear regression (Section 4.7.2), we prefer models with AUC close to 1. However, there is no single threshold that defines “good” AUC. What is considered a “good” AUC depends on the subject matter context and complexity of the modeling task.\nThe AUC for the model in Table 12.1 is 0.664. Predicting individuals’ opinions is a complex modeling task, so the model is a reasonably good fit for the data. We are only using three predictors, however, so we will consider other predictors in Section 12.7 to potentially improve the model performance.\n\nThough we prefer models with high values of AUC (close to 1), we do not want a model in which \\(AUC = 1\\) exactly. When the \\(AUC  =1\\), it means the model has perfect separation, the ability to perfectly distinguish between the true positives and false positives. Though this may seem like ideal model performance, it is often a sign that the model is overfit  and will not effectively classify observations in new data.\n\n\n12.5.2 Comparing models using AIC and BIC\nSimilar to linear regression, we can use the Akaike’s Information Criterion (AIC) (Akaike 1974) and the Bayesian Schwarz’s Criterion (BIC) (Schwarz 1978) as relative measures for comparing logistic regression models. The equations for AIC and BIC in logistic regression are the same as these in linear regression Section 10.2.3.\n\\[\n\\begin{aligned}\n&\\text{AIC} = -2 \\log L + 2(p+1) \\\\[5pt]\n&\\text{BIC} = -2 \\log L + \\log(n)(p+1)\n\\end{aligned}\n\\]\nwhere \\(\\log L\\) is the log-likelihood of the model, \\(n\\) is the number of observations, and \\(p + 1\\) is the number of terms in the model. The penalty applied by BIC for the number of predictors in the model is greater than the penalty applied in AIC, as \\(\\log(n) &gt; 2\\) (when \\(n &gt; 8\\)). Therefore, BIC will tend to preference more parsimonious models, those with a fewer number of predictors. The values of AIC and BIC are not meaningful to interpret for individual models. These values are most useful for comparing models in the model selection process. When using AIC and BIC to compare models, we can use the guidelines in Table 10.3, the same as with linear regression.\nLet’s use AIC and BIC to compare the model from Table 12.1 to a model that includes the same predictors along with polviews (how individuals rate their political views) and tech_harm (whether an individual thinks technology generally does more harm than good).\n\n\n\nTable 12.8: AIC and BIC for two candidate models\n\n\n\n\nModel\nAIC\nBIC\n\n\n\nage, income, tech_easy\n1984\n2032\n\n\nage, income, tech_easy, polviews, tech_harm\n1730\n1782\n\n\n\n\n\n\n\n\nTable 12.8 shows AIC and BIC for both models. Based on these measures, there is very strong evidence in favor of the model that includes the additional predictors polviews and tech_harm.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#sec-logistic-prediction-R",
    "href": "12-logistic-prediction.html#sec-logistic-prediction-R",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.6 Prediction and evaluation in R",
    "text": "12.6 Prediction and evaluation in R\n\n12.6.1 Prediction and classification\nMany of the functions for prediction and evaluation for logistic regression are the same as those used in linear regression. The predict() function is used to compute predictions from the logistic regression model. The predicted logit can also be obtained from the .fitted column in the data produced by the augment() function.\nBelow is the code to fit the model in Table 12.1 and produce the predicted log odds. The predictions for the first 10 observations are shown.\n\naidrive_comfort_fit &lt;- glm(aidrive_comfort ~ age + tech_easy + income_fct, \n                           data = gss24_ai, \n                           family = \"binomial\")\n\npredict(aidrive_comfort_fit)\n\n\n\n      1       2       3       4       5       6       7       8       9      10 \n 1.3061  0.9528 -0.0258 -1.0516 -0.8967  0.6411  0.8279  0.4817 -0.3228  1.0192 \n\n\nThe predicted probabilities can be computed directly using the argument type = \"response\" in predict().\n\npredict(aidrive_comfort_fit, type = \"response\")\n\n\n\n    1     2     3     4     5     6     7     8     9    10 \n0.787 0.722 0.494 0.259 0.290 0.655 0.696 0.618 0.420 0.735 \n\n\nThe predicted classes can be computed “manually” using the predicted probabilities and dplyr functions. Below is code to compute the predicted probabilities, predict the classes based on a threshold of 0.5, and add these as columns to the original gss24_ai data frame. The predicted probability and class for the first 10 observations are shown below.\n\ngss24_ai &lt;- gss24_ai |&gt;\n  mutate(pred_prob = predict(aidrive_comfort_fit, type = \"response\"),\n         pred_class = factor(if_else(pred_prob &gt; 0.5, \"1\", \"0\")))\n\n\n\n# A tibble: 10 × 2\n  pred_prob pred_class\n      &lt;dbl&gt; &lt;fct&gt;     \n1     0.787 1         \n2     0.722 1         \n3     0.494 0         \n4     0.259 0         \n5     0.290 0         \n6     0.655 1         \n# ℹ 4 more rows\n\n\n\n12.6.2 Confusion matrix and ROC curve\nWe use the predicted probabilities and predicted classes to make the confusion matrix is using the conf_mat() function from the yardstick package (Kuhn, Vaughan, and Hvitfeldt 2025).\n\naidrive_conf_mat &lt;- gss24_ai |&gt;\nconf_mat(aidrive_comfort, pred_class)\n\naidrive_conf_mat\n\n          Truth\nPrediction   0   1\n         0 349 228\n         1 343 601\n\n\nThe autoplot() function produces the same confusion matrix with some additional formatting applied. For example, the argument type = \"heatmap\" produces a confusion matrix in which the cells are shaded in based on the number of observations.\n\nautoplot(aidrive_conf_mat, type = \"heatmap\")\n\n\n\n\n\n\n\nThe roc_curve function is used to compute the data for the ROC curve. Ten data points from the ROC curve data are shown below.\n\naidrive_roc_data &lt;- gss24_ai |&gt;\nroc_curve(aidrive_comfort, pred_prob, event_level = \"second\")\n\n\n\n# A tibble: 10 × 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      0.367      0.188        0.935\n2      0.283      0.0679       0.984\n3      0.432      0.308        0.852\n4      0.525      0.571        0.665\n5      0.430      0.305        0.856\n6      0.319      0.121        0.972\n# ℹ 4 more rows\n\n\nThe argument event_level = \"second\" is needed to specify that the predicted probability is associated with the probability that \\(y_i = 1\\) (rather than the probability \\(y_i = 0\\)). The .threshold column contains the classification thresholds.\nThe ROC curve data can be plotted using the autoplot() function. The resulting graph is ggplot object, so additional ggplot2 layers such as labs and annotate can be applied to the ROC curve.\n\nautoplot(aidrive_roc_data) + \nlabs(title = \"ROC Curve\")\n\n\n\n\n\n\n\n\n12.6.3 Model evaluation\nThe roc_auc() function is used to compute the Area Under the Curve using the predicted probabilities and predicted classes. It follows a similar syntax as roc_curve().\n\ngss24_ai |&gt; \nroc_auc(aidrive_comfort, pred_prob, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.664\n\n\nThe last model comparison statistics, AIC and BIC, are computed from the glance() function.\n\nglance(aidrive_comfort_fit)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1         2096.    1520  -983. 1984. 2032.    1966.        1512  1521\n\n\nNote that the output from glance() does not include \\(R^2\\) and Adj. \\(R^2\\) as it does in linear regression, because we do not use the ANOVA-based statistics in logistic regression.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#sec-logistic-workflow",
    "href": "12-logistic-prediction.html#sec-logistic-workflow",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.7 Modeling building workflow in R",
    "text": "12.7 Modeling building workflow in R\nLet’s put together what we learned about model selection in Chapter 10 and evaluating logistic regression models in Section 12.5 to illustrate an example model building workflow for logistic regression. Here, we will split the data into training and testing sets, and use cross validation with AUC as the criteria for choosing between two candidate models.\nThe first model is the one we’ve analyzed in this chapter that includes the predictors age, tech_easy and income. The second model will use these predictors along with polviews and tech_harm.\nFirst, we define the training and testing sets. We’ll use simple random sampling to assign 80% of the observations to the training set and 20% of the observations to the testing set.\n\nset.seed(12345)\naidrive_split &lt;- initial_split(gss24_ai, prop = 0.8)\naidrive_train &lt;- training(aidrive_split)\naidrive_test &lt;- testing(aidrive_split)\n\nWe can use the training data to evaluate the conditions for logistic regression, linearity and independence. We evaluated these conditions in Section 11.6 and determined they were satisfied. Therefore, we proceed with modeling and split the data into 5 folds for cross validation.\n\nset.seed(12345)\n\nfolds &lt;- vfold_cv(aidrive_train, v = 5)\n\nNext, we conduct 5-fold cross validation for Model 1 with the predictors age, tech_easy, and income. We collect a summary of the metrics in cross validation in the objective aidrive_cv_1_metrics. Because we are fitting logistic regression models, collect_metrics() uses AUC to measure the model performance.\n\n# cross validation workflow for Model 1\naidrive_workflow_1 &lt;- workflow() |&gt;\n  add_model(logistic_reg()) |&gt;\n  add_formula(aidrive_comfort ~ age + tech_easy + income_fct) \n  \naidrive_cv_1 &lt;- aidrive_workflow_1 |&gt; \n  fit_resamples(resamples = folds) \n\naidrive_cv_1_metrics &lt;- collect_metrics(aidrive_cv_1, summarize = TRUE) \n\nWe repeat the process of cross validation for Mode 2 that includes the predictors age, tech_easy, income_fct, polviews_fct, and tech_harm. The performance metrics from cross validation are stored in the object aidrive_cv_2_metrics.\n\n# cross validation workflow for Model 2\naidrive_workflow_2 &lt;- workflow() |&gt;\n  add_model(logistic_reg()) |&gt;\n  add_formula(aidrive_comfort ~ age + tech_easy + income_fct + \n                polviews_fct + tech_harm)\n  \naidrive_cv_2 &lt;- aidrive_workflow_2 |&gt; \n  fit_resamples(resamples = folds) \n\naidrive_cv_2_metrics &lt;- collect_metrics(aidrive_cv_2, summarize = TRUE) \n\nNow let’s look at the average AUC across the 5-fold cross validation for each model. When we do cross validation, the AUC is computed based on a ROC curve fit on the assessment data in each fold. This gives us a view of how well each model performs on new data.\nModel 1\n\naidrive_cv_1_metrics |&gt; filter(.metric == \"roc_auc\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 roc_auc binary     0.649     5 0.00737 pre0_mod0_post0\n\n\nModel 2\n\naidrive_cv_2_metrics |&gt; filter(.metric == \"roc_auc\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 roc_auc binary     0.674     5 0.00895 pre0_mod0_post0\n\n\nThe average AUC is 0.649 for Model 1 and 0.674 for Model 2. Therefore, we select Model 2 that includes the additional predictors tech_harm and polviews_fct, because it has the higher average AUC across the five folds. This is consistent with the conclusion from AIC and BIC in Section 12.5.2. In practice, we will likely consider more than two models, for example, we might consider models with interaction terms or transformations. We will conduct cross validation and compare average AUC for each candidate model. For simplicity, we will only compare two models here and proceed with Model 2 as the final model.\nNow that we have selected the final model we refit this model on the entire training set, and we use the testing set to compute AUC as a final evaluation of how well the model performs on new data. We compute the predicted probabilities and construct the ROC curve on the testing data.\n\n# refit model on full training set\naidrive_comfort_final &lt;- glm(aidrive_comfort ~ age + tech_easy + \n                               income_fct + tech_harm + polviews_fct, \n                             data = aidrive_train, \n                             family = \"binomial\")\n\n# compute predicted probabilities \naidrive_test &lt;- aidrive_test |&gt;\n  mutate(pred_prob = predict(aidrive_comfort_final, newdata = aidrive_test, type = \"response\"))\n\n# make roc curve \naidrive_test |&gt; \n    roc_curve(aidrive_comfort, \n              pred_prob, \n            event_level = \"second\")  |&gt;\n  autoplot()\n\n\n\n\n\n\n\nThe AUC for the testing data is 0.692. Given the analysis task of modeling individuals’ opinions, this model performs reasonably well in classifying individuals who are comfortable with driverless cars versus those who are not.\nAs a final step, we refit the model using all observations in the data. At this point, we are ready to use the model for interpretation, drawing inferential conclusions, and to put into production for prediction and classification.\n\nglm(aidrive_comfort ~ age + tech_easy + \n                               income_fct + tech_harm + polviews_fct, \n                             data = gss24_ai, \n                             family = \"binomial\") |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n0.203\n0.254\n0.798\n0.425\n\n\nage\n-0.017\n0.003\n-5.334\n0.000\n\n\ntech_easyCan’t choose\n0.226\n0.484\n0.467\n0.640\n\n\ntech_easyAgree\n0.381\n0.163\n2.339\n0.019\n\n\ntech_easyDisagree\n-0.355\n0.309\n-1.149\n0.251\n\n\nincome_fct$20-50k\n0.244\n0.181\n1.349\n0.177\n\n\nincome_fct$50-110k\n0.481\n0.174\n2.771\n0.006\n\n\nincome_fct$110k or more\n1.133\n0.187\n6.062\n0.000\n\n\nincome_fctNot reported\n0.302\n0.224\n1.350\n0.177\n\n\ntech_harmCan’t choose\n-0.402\n0.423\n-0.949\n0.343\n\n\ntech_harmAgree\n-0.378\n0.148\n-2.555\n0.011\n\n\ntech_harmDisagree\n0.394\n0.134\n2.934\n0.003\n\n\npolviews_fctLiberal\n0.286\n0.141\n2.024\n0.043\n\n\npolviews_fctConservative\n-0.167\n0.135\n-1.239\n0.215\n\n\npolviews_fctNot reported\n-0.443\n0.295\n-1.502\n0.133",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#summary",
    "href": "12-logistic-prediction.html#summary",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "\n12.8 Summary",
    "text": "12.8 Summary\nIn this chapter, we expanded on the introduction to logistic regression in Chapter 11, as we used the logistic regression model to compute predicted log odds, odds, and probabilities. We then used the predicted values to classify observations into \\(\\hat{Y} = 0\\) or \\(\\hat{Y} = 1\\) . We constructed confusion matrices to evaluate the model performance at individual thresholds, and evaluated classification results using statistics such as sensitivity and specificity, among others. We used the ROC curve to select a classification threshold and computed the area under the curve (AUC) to more holistically evaluate the model fit. We also computed AIC and BIC for model comparison. We applied the model building practices from Chapter 10 to compare two models using a cross validation workflow.\nIn the Chapter 13, we conclude with some advanced modeling techniques and special topics for analysis in practice.\n\n\n\n\nAkaike, H. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nDavern, Michael, Rene Bautista, Jeremy Freese, Pamela Herd, and Stephen L. Morgan. 2025. “General Social Survey 1972–2024.” NORC at the University of Chicago; [Machine-readable data file]. https://gss.norc.org/content/dam/gss/get-documentation/pdf/codebook/GSS%202024%20Codebook.pdf.\n\n\nJabkowski, Piotr, and Aneta Piekut. 2024. “Not Random and Not Ignorable. An Examination of Nonresponse to Income Question in the European Social Survey, 2008–2018.” Field Methods 36 (3): 213–28.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2025. “Yardstick: Tidy Characterizations of Model Performance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a Model.” The Annals of Statistics, 461–64.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "12-logistic-prediction.html#footnotes",
    "href": "12-logistic-prediction.html#footnotes",
    "title": "12  Logistic regression: Prediction and evaluation",
    "section": "",
    "text": "Yes, the coefficients and confidence intervals for the indicators for income_fct support the observations from the EDA. The indicators for higher income, $50-110K, and $110k or more are positive. Thus, those with higher income are more likely to be comfortable with driverless cars compared to individuals in the lowest level, after adjusting for age and tech_easy.↩︎\n\nLog odds: \\(\\log \\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) =  0.032 - 0.016  \\times 19 + 0.075   \\times 0 +  0.576 \\times 0\n- 0.436  \\times 0 +  0.264 \\times 0 +  0.526  \\times 0\n+ 1.223  \\times 1 + 0.265 \\times 0 = 0.951\\)\nOdds: \\(e^{\\log(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}})} = e^{0.953} = 2.59\\)\nProbability: \\(\\frac{\\widehat{\\text{odds}}}{1 + \\widehat{\\text{odds}}} = \\frac{2.593}{1 + 2.593} = 0.722\\)↩︎\n\n\\[\\text{misclassification} = \\frac{\\text{False negative} + \\text{False positive}}{\\text{True negative} + \\text{False negative} + \\text{False positive} + \\text{True positive}}\n\\]↩︎\n\\[\\begin{aligned}\\text{Sensitivity} &= 601/ (228 + 601) = 0.725 \\\\ \\text{Specificity} &= 349 / (349 + 343) = 0.504 \\\\\\text{Precision} &= 601 / (343 + 601) = 0.637\\end{aligned}\\]↩︎\nA true positive is correctly classifying a patient with the illness as a “1”, having the illness. A “false positive” is incorrectly classifying a patient that doesn’t have the illness as a “1”, having the illness.↩︎\nIf the main objective is high sensitivity, we set a low threshold close to 0. If the main objective is high specificity, we set a high threshold close to 1.↩︎",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic regression: Prediction and evaluation</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html",
    "href": "13-special-topics.html",
    "title": "13  Special topics",
    "section": "",
    "text": "Learning goals\nThis chapter introduces readers to a collection to models that are extensions of linear and logistic regression. The goal is to provide an introduction to the topic along with resources for those interested in learning more. The following topics are introduced:",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#learning-goals",
    "href": "13-special-topics.html#learning-goals",
    "title": "13  Special topics",
    "section": "",
    "text": "Multinomial logistic regression\nRandom intercepts models\nDecision trees for regression and classification\nCausal inference using propensity score models",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#sec-multinomial",
    "href": "13-special-topics.html#sec-multinomial",
    "title": "13  Special topics",
    "section": "\n13.1 Multinomial logistic regression",
    "text": "13.1 Multinomial logistic regression\nIn Chapter 11, we introduced logistic regression models for binary response variables. In this section, we introduce multinomial logistic regression, models for categorical response variables with three or more levels.\n\n13.1.1 Data: Voting frequency\nIn the 2020 article “Why Many Americans Don’t Vote” (Thomson-DeVeaux, Mithani, and Bronner 2020) on the now defunct data journalism website FiveThirtyEight, journalists explored the voting behavior of eligible voters in the United States.  As indicated by the article’s title, their primary objective was to understand why many eligible voters choose not to vote at all or only vote sporadically.  They considered numerous factors in their analysis, but we will focus on a few, age and political affiliation, for this analysis. \nThe data were obtained from the FiveThirtyEight data website (https://data.fivethirtyeight.com) and is available in fivethirtyeight-voters-data.csv. It contains survey responses  from 5836 respondents. All survey participants had been eligible to vote in at least four prior elections. The variables used in this analysis are below. The definitions are adapted from the data documentation (https://github.com/fivethirtyeight/data/tree/master/non-voters)\n\n\nvoter_category: An respondent’s past voting behavior, categorized as\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nage: Respondent’s age in years (called ppage in original data set)\n\nparty_id: Respondent’s political affiliation categorized as Democrat, Republican , No affiliation. This variable was derived from the variable Q30 in the original data set, defined as the following:\n\n\nResponse to the question “Generally speaking, do you think of yourself as a…”\n\n1:Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\n\nResponses 3, 4, 5,-1 are classified as “No affiliation” in this analysis.\n\n\nincome_cat: Respondent’s annual income (Less than $40k, $40-75k, $75-125k, $125k or more)\n\nThe goal of this analysis is to use age and party_id to predict voting_id and describe factors associated with variability in voting behavior. The univariate exploratory data analysis is in Figure 13.1 and Table 13.1.\n\n\n\n\n\n\n\n\n\n\n(a) voter_category\n\n\n\n\n\n\n\n\n\n\n\n(b) age\n\n\n\n\n\n\n\n\n\n(c) party_id\n\n\n\n\n\n\nFigure 13.1: Univariate EDA for voting frequency data\n\n\n\n\n\n\nTable 13.1: Summary statistics for age\n\n\n\n\nVariable\nMean\nSD\nMin\nQ1\nMedian (Q2)\nQ3\nMax\nMissing\n\n\nage\n51.7\n17.1\n22\n36\n54\n65\n94\n0\n\n\n\n\n\n\n\nThe most common voting frequency among respondents in sporadic, with 44.1% of the voters being in this category. About 24.9% of the respondents vote rarely or never, and 31% of the respondents voted always. The distribution of age is bimodal and skewed right. The median age in the data set is 54 years old, and the IQR is 29 years. The most common political party identification is “No affiliation” with about 38.4% of the respondents. About 34.3% of the respondents identified as “Democrat” and about 27.3% as “Republican”.\n\n\n\n\n\n\n\n\n\n(a) voter_category versus age\n\n\n\n\n\n\n\n\n\n(b) voter_category versus party_id\n\n\n\n\n\n\nFigure 13.2: Bivariate EDA for voting frequency data. Blue: rarely/never, Red: sporadic, Yellow: always\n\n\nThe bivariate exploratory data analysis is in Figure 13.2. From Figure 13.2 (a), the distribution of age among voters categorized as “always” is bimodal with a peak around 25 years old and another around 60 years old. In contrast, the distribution of age among voters categorized as “rarely/never” is unimodal and skewed towards younger voters. This suggest there may be a relationship between age and voting frequency.\nFrom Figure 13.2 (b), the voters who have no political party affiliation are the most likely to vote “rarely/never” and the least likely to vote “always”. The distribution of voting frequency appears to be similar between Democrats and Republicans. This indicates there may be some relationship between political party identification (or even whether or not someone identifies with a political party) and voting frequency.\nWe have some initial insights from the data about overall voting frequency and potential relationships with age and political party affiliation, so we want to fit a model to summarize the relationship between these variables that we can use to draw robust conclusions and make predictions.\n\n13.1.2 Multinomial logistic model\nThe response variable in this analysis has three levels, so we will use an extension of the logistic regression model. Multinomial logistic regression is part of the wider class of models called generalized linear models. It is a model for data with a categorical response variable that has three or more levels. Such response variables follow a multinomial distribution.\n\nMultinomial distribution\nLet \\(Y\\) be a categorical random variable with \\(K &gt; 2\\) levels. Then,\n\\[P(Y = 1) = \\pi_1, P(Y = 2) = \\pi_2, \\ldots, P(Y = K) = \\pi_K\\] such that \\(\\sum_{k=1}^K \\pi_K = 1\\)\n\nLet’s look at the general form of the multinomial logistic regression model, then apply it to model voting frequency. Suppose we have a categorical response variable that takes values \\(A\\), \\(B\\), and \\(C\\). Let \\(A\\) be the baseline level. Then, the general form of the multinomial logistic regression model is\n\\[\\begin{aligned}\n\\log\\Big(\\frac{\\pi_B}{\\pi_A}\\Big) = \\beta_{0\nB} + \\beta_{1B}X_1 +\\beta_{2B}X_2 + \\dots + \\beta_{pB}X_p \\\\\n\\log\\Big(\\frac{\\pi_C}{\\pi_A}\\Big) = \\beta_{0C} + \\beta_{1C}X_1 +\\beta_{2C}X_2 + \\dots + \\beta_{pC}X_p\n\\end{aligned}\n\\tag{13.1}\\]\nFrom Equation 13.1, we see a few features of the multinomial logistic model. We start by choosing a baseline level for the response variable. Each equation in the fitted model quantifies the log odds of being at another level of the response variable versus the baseline1. WE saw this implicitly in logistic regression, where the response variable, \\(\\log(\\frac{\\pi}{1-\\pi})\\), is the “log odds of \\(Y = 1\\) versus \\(Y = 0\\).”\nThe next feature of multinomial logistic regression is the multiple equations that comprise the model. More specifically, if the response variable has \\(K\\) levels, then there will be \\(K - 1\\) equations in the model. We will ultimately use the model to compute the predicted probabilities an observation takes each level of the response variable, so we need all \\(K-1\\) equations in order to compute the predicted probabilities for every level of the response variable.\nThe last feature is that each of the \\(K -1\\) equations have the same form but different values of the coefficients. This is shown in Equation 13.1 as the \\(\\beta's\\) have unique indices in each equation. As with logistic regression, the coefficients for multinomial logistic regression are estimated using maximum likelihood estimation (see 11.4).\nTable 13.2 is the multinomial logistic regression model using age and party_id to predict voter_category.\n\n\n\nTable 13.2: Model of voter_category versus age and party_id with 95% confidence intervals for the coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\nsporadic\n(Intercept)\n-1.046\n0.116\n-9.031\n0.000\n-1.273\n-0.819\n\n\nsporadic\nage\n0.041\n0.002\n18.834\n0.000\n0.036\n0.045\n\n\nsporadic\nparty_idNo affiliation\n-0.677\n0.081\n-8.389\n0.000\n-0.836\n-0.519\n\n\nsporadic\nparty_idRepublican\n-0.106\n0.095\n-1.118\n0.264\n-0.292\n0.080\n\n\nalways\n(Intercept)\n-1.983\n0.131\n-15.157\n0.000\n-2.240\n-1.727\n\n\nalways\nage\n0.052\n0.002\n22.144\n0.000\n0.048\n0.057\n\n\nalways\nparty_idNo affiliation\n-0.872\n0.089\n-9.824\n0.000\n-1.046\n-0.698\n\n\nalways\nparty_idRepublican\n-0.091\n0.101\n-0.901\n0.368\n-0.288\n0.106\n\n\n\n\n\n\n\n\nThe column y.level indicates the corresponding level for the given equation. The level of the response variable that is not listed is the baseline. In this case, the baseline level is rarely/never. Therefore, this model produces the log odds of being in the other categories versus rarely/ never.\nWe can write the model in Table 13.2 as the following:\n\\[\n\\begin{aligned}\n\\log\\Big(\\frac{\\pi_{sporadic}}{\\pi_{rarely/never}}\\Big)  = -1.046 + 0.041 \\times \\text{age} -0.677 \\times \\text{No affiliation} -0.106 \\times \\text{Republican} \\\\\n\\log\\Big(\\frac{\\pi_{always}}{\\pi_{rarely/never}}\\Big)  = -1.983 + 0.052 \\times \\text{age} -0.872 \\times \\text{No affiliation}  -0.091 \\times \\text{Republican}\n\\end{aligned}\n\\]\n\n13.1.3 Interpretation\nThe interpretation for the coefficients of the multinomial logistic model are very similar to the interpretations for the logistic model in Section 11.4. Like logistic regression, the model coefficients represent the change in the log odds, but in practice want to write interpretations in terms of the odds. The primary difference in the interpretation is the explicit statement of the baseline level of the response variable.\nLet’s interpret the coefficients in terms of having the voter frequency of sporadic versus rarely/never. The interpretation of age in terms of the log odds is as follows:\n\nFor each additional year older, the log odds of an individual voting sporadically versus rarely/never are expected to increase by 0.041, holding political affiliation constant.\n\nThe interpretation in terms of the odds is the following:\n\nFor each additional year older, the odds of an individual voting sporadically versus rarely/never are expected to multiply by 1.042 \\((e^{0.041})\\), holding political affiliation constant.\n\nNext, we interpret the effect of political affiliation on the voting frequency. As with other models, we write the interpretation in reference to the baseline level of the categorical predictor. The interpretation of the coefficients for party_id in terms of the log odds are the following:\n\nThe log odds an individual who has no political affiliation votes sporadically versus rarely/never are 0.677 less than the log odds of an individual whose party affiliation is Democrat, holding age constant.\n\n\nThe log odds an individual whose political affiliation is Republican votes sporadically versus rarely/never are 0.106 less than the log odds of an individual whose party affiliation is Democrat, holding age constant.\n\nThe interpretations in terms of the odds are as follows:\n\nThe odds an individual who has no political affiliation votes sporadically versus rarely/never are 0.508 \\((e^{-0.872})\\) times the odds of an individual whose party affiliation is Democrat, holding age constant.\n\n\nThe odds an individual whose political affiliation is Republican votes sporadically versus rarely/never are 0.899 \\((e^{-0.106})\\) times the odds of an individual whose party affiliation is Democrat, holding age constant.\n\nAlong with the coefficients in Table 13.2 are 95% confidence intervals for the population coefficients. These confidence intervals are estimated in a similar way as those for logistic regression in Section 11.5. As with previous models, we look to see whether the confidence interval contains 0 to determine whether it is a useful predictor in explaining variability in voting frequency after adjusting for the other predictors. In both equations in Table 13.2, none of the confidence intervals for age and party_idNoaffiliation include 0. Thus, both of these indicators are helpful in understanding voting frequency. In contrast, the confidence intervals for party_idRepublican do contain 0, so the voting behavior for Republicans does not differ significantly from Democrats, after adjusting for age. This is consistent with the observations from Figure 13.2 (b).\n\nIn Table 13.2, the inferential conclusions for each predictor are the same for both equations in the multinomial logistic model. This is not a requirement for multinomial logistic regression and is not always the case. It could be possible for a predictor to be useful in differentiating always versus rarely/never, but not useful in differentiating sporadic versus rarely/never.\n\n\n13.1.4 Prediction and classification\nWe often want to use multinomial logistic regression models for prediction, and more specifically classifying observations into the groups defined by the response variable. We can use the log odds produced by the multinomial logistic model to compute probabilities, then use those probabilities for classification.\nLet’s go back to the general form of the model in Equation 13.1. The predicted probability an observation is in category \\(B\\) is computed as\n\\[\n\\hat{\\pi}_B = \\frac{e^{\\beta_{0B} + \\beta_{1B}X_1 + \\beta_{2B}X_2 + \\dots + \\beta_{pB}X_p}}{1 + \\sum_{k \\in (B,C)}e^{\\beta_{0k} + \\beta_{1k}X_1 + \\beta_{2k}X_2 + \\dots + \\beta_{pk}X_p}}\n\\tag{13.2}\\]\n\nThe numerator of Equation 13.2 are the odds of an observation being in category \\(B\\) versus the baseline \\(A\\). The denominator is the sum of the odds for every possible level. In this example, it is the sum of the odds an observation is in category \\(A\\) versus \\(A\\) (that’s the “1” in the denominator), the odds an observation is in category \\(B\\) versus \\(A\\), and the odds an observation is in category \\(C\\) versus \\(A\\). A similar formula is used to compute \\(\\hat{p}_c\\), the predicted probability of being in category \\(C\\). The difference is the numerator, which would be the odds of being in category \\(C\\) versus \\(A\\).\nThe predicted probability an observation is in the baseline category \\(A\\) is computed as\n\\[\n\\hat{\\pi}_A = 1 - (\\hat{\\pi}_B + \\hat{\\pi}_C)\n\\tag{13.3}\\]\nEquation 13.3 utilizes the fact that the sum of the probabilities across all possible categories equals 1. This is is also why we need all \\(K - 1\\) equations as part of the multinomial logistic when there are \\(K\\) categories of the response variable.\nTable 13.3 shows the predicted probabilities based on Table 13.2 for ten observations in the data.\n\n\n\nTable 13.3: Predictions from voter frequency model for ten observations\n\n\n\n\nvoter_category\nrarely/never\nsporadic\nalways\npred_class\n\n\n\nalways\n0.071\n0.484\n0.446\nsporadic\n\n\nalways\n0.069\n0.483\n0.447\nsporadic\n\n\nsporadic\n0.160\n0.486\n0.354\nsporadic\n\n\nsporadic\n0.132\n0.490\n0.379\nsporadic\n\n\nalways\n0.055\n0.466\n0.479\nalways\n\n\nrarely/never\n0.220\n0.470\n0.310\nsporadic\n\n\nalways\n0.057\n0.468\n0.475\nalways\n\n\nalways\n0.087\n0.488\n0.424\nsporadic\n\n\nalways\n0.088\n0.479\n0.433\nsporadic\n\n\nalways\n0.093\n0.489\n0.417\nsporadic\n\n\n\n\n\n\n\n\nThe predicted probabilities are then used to classify the observations based on the model. For each observation, the predicted class is the one with the highest predicted probability. For example, for the first observation in Table 13.3, \\(\\hat{\\pi}_{rarely/never} = 0.071\\), \\(\\hat{\\pi}_{sporadic} = 0.484\\), and \\(\\hat{\\pi}_{always} = 0.446\\). Therefore, the predicted class for this observation is “sporadic”.\n\n13.1.5 Confusion matrix\nFrom Table 13.3, we see the predicted class is not always equal to the observed class. Therefore, we want to quantify the model performance, specifically how well the model classifies observations and how well it distinguishes observations from a specific class. A confusion matrix, introduced in Section 12.3.2, is used to compare the observed versus predicted classes. Figure 13.3 is the confusion matrix for the model in Table 13.2. Because the predicted class is always the category with the highest predicted probability, we do not need to specify a threshold as in logistic regression.\n\n\n\n\n\n\n\nFigure 13.3: Confusion matrix for voters model\n\n\n\n\n\nUse Figure 13.3 to compute\n\nthe accuracy.\nthe misclassification rate2\n\nThere are 5836 observations in the data.\n\nFigure 13.3 can also be used to compute more specific statistics about how well the model identifies observations from a particular class. For example, among those who are actually classified voting rarely/never, the model correctly identified 38.801 % of them.\nConversely, among those who were classified by the model as rarely/never, 49.343% of them actually vote rarely/never.\n\n13.1.6 ROC Curves and AUC\nReceiver Operating Characteristic (ROC) curves, introduced in Section 12.4, can be used to evaluate how well the model distinguishes observations from a particular category. To do so, we create multiclass ROC curves using a “one versus all” approach that evaluates how well the model distinguishes one category versus all the others. For example, one curve represents how well the model distinguishes “always” voters versus all the others (“rarely/never” and “sporadic”). Essentially, we compute a logistic regression ROC curve for each possible class of the response.\n\n\n\n\n\n\n\nFigure 13.4: Multiclass ROC curves for the voter frequency model in Table 13.2\n\n\n\n\nFigure 13.4 shows the multiclass ROC curve for the model in Table 13.2. We interpret this curve similarly to the ROC curve for logistic regression. Curves close to the upper-left portion of the graph indicate good model distinguishing, but curves close to the diagonal line indicate poor model distinguishing. Based on this, the model performs best in identifying “rarely/never” voters versus the others and does the worst in identifying sporadic voters.\nThe overall model performance is quantified using the average area under the curve (AUC). The average AUC is computed in two ways. The first by computing the AUC for each class, then taking the average across all values of AUC. The average AUC for Figure 13.4 is 0.656 . The second approach is to weight the AUC by the number of observations actually in each class. Therefore, larger classes have more influence on the measure of model performance. Using this approach, the average AUC for Figure 13.4 is 0.627.\n\n13.1.7 Multinomial logistic regression in R\nMultinomial logistic models are fit using multinom() from the nnet R package (Venables and Ripley 2002).\n\n# library(nnet) \n\nvoter_fit &lt;- multinom(voter_category ~ age + party_id,\n                      data = voters)\n\n# weights:  15 (8 variable)\ninitial  value 6411.501317 \niter  10 value 5881.210811\nfinal  value 5858.219129 \nconverged\n\n\nThe model output can be tidied and neatly displayed using tidy() from the broom package (Robinson, Hayes, and Couch 2023) and kable() from the knitR package (Xie 2024).\n\ntidy(voter_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\nsporadic\n(Intercept)\n-1.046\n0.116\n-9.031\n0.000\n\n\nsporadic\nage\n0.041\n0.002\n18.834\n0.000\n\n\nsporadic\nparty_idNo affiliation\n-0.677\n0.081\n-8.389\n0.000\n\n\nsporadic\nparty_idRepublican\n-0.106\n0.095\n-1.118\n0.264\n\n\nalways\n(Intercept)\n-1.983\n0.131\n-15.157\n0.000\n\n\nalways\nage\n0.052\n0.002\n22.144\n0.000\n\n\nalways\nparty_idNo affiliation\n-0.872\n0.089\n-9.824\n0.000\n\n\nalways\nparty_idRepublican\n-0.091\n0.101\n-0.901\n0.368\n\n\n\n\n\n\nSuppress model fit messages\nThe coefficients are estimated using numerical approximation methods to find the values that maximize the likelihood function. The mulitnom function will produce a message summarizing the optimization iterations. This information is generally not necessary in practice, so include the code chunk option #| results:  hide in Quarto documents to suppress the message.\n\nPredictions from the multinomial model are computed using the predict() function. The argument type = \"probs\" produces a matrix of the predicted probabilities and type = \"class\" produces a vector of the predicted classes.\nWe add these to the original tibble for additional analysis.\n\n# compute predicted probabilities\nvoter_pred_prob &lt;- predict(voter_fit, type = \"probs\")\n\n# compute predicted class\nvoter_pred_class &lt;- predict(voter_fit, type = \"class\")\n\n# add predictions to original tibble\nvoters &lt;- voters |&gt; \n  bind_cols(voter_pred_prob) |&gt;\n  mutate(pred_class = voter_pred_class)\n\nThe syntax for the confusion matrix, ROC curves, and AUC are similar to logistic regression in Section 12.6.\nThe confusion matrix is produced using conf_mat() from the yardstick R package (Kuhn, Vaughan, and Hvitfeldt 2025). We use autoplot() to format the confusion matrix such that the cells are shaded by the number of observations.\n\nvoters |&gt; \n  conf_mat(voter_category, pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nThe multiclass ROC curves are produced using roc_curve() in the yardstick R package (Kuhn, Vaughan, and Hvitfeldt 2025). The argument truth = specifies the columns with the observed classes, and the next line of code indicates the columns that contain the predicted probabilities. The roc_curve() function produces the data underlying the curves, and autoplot() produces the curves.\n\nvoters |&gt; \n  roc_curve(\n    truth = voter_category, \n    `rarely/never`:always\n  ) |&gt; \n  autoplot()\n\n\n\n\n\n\n\nLastly, the AUC is computed using roc_auc() from the yardstick R package. The the weighted average AUC is computed using the argument estimator = \"macro_weighted\". The unweighted average AUC is computed by default.\n\n# compute unweighted average AUC\nvoters |&gt; \n  roc_auc(\n    truth = voter_category, \n    `rarely/never`:always\n  ) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc hand_till      0.656\n\n# compute weighted average AUC\nvoters |&gt; \n  roc_auc(\n    truth = voter_category, \n    `rarely/never`:always,\n    estimator = \"macro_weighted\"\n  ) \n\n# A tibble: 1 × 3\n  .metric .estimator     .estimate\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n1 roc_auc macro_weighted     0.627\n\n\n\n13.1.8 Generalized linear models",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#sec-random-intercepts",
    "href": "13-special-topics.html#sec-random-intercepts",
    "title": "13  Special topics",
    "section": "\n13.2 Random intercepts model",
    "text": "13.2 Random intercepts model\nOne of the key assumptions we’ve made throughout this book is that the observations in the sample data are independent of each other. Often in practice, however, there is an underlying grouping structure in the data, and the observations are not completely independent of one another. For example, the data may contain multiple measurements from each individual (call repeated measures) as in Figure 13.5.\n\n\n\n\n\nFigure 13.5: Repeated measures data structure\n\n\nAnother common grouping structure is having multiple observations from within a single group, such that we expect the observations within the group to be more similar than observations from different groups, as in Figure 13.6.\n\n\n\n\n\nFigure 13.6: Grouped data structure\n\n\nData that contain a grouping structure are called multilevel data. The individual observations are at the first level of the data, and the groups are at the second level. We use generalized linear mixed models (GLMM), also called multilevel models or hierarchical models, to account for the grouping structure when we fit the model and conduct statistical inference. There are many types of generalized linear mixed models; in fact, there are often entire undergraduate-level courses dedicated to these models. In this section, we will introduce one of the most widely used GLMMs in practice, the random intercepts model.\n\n13.2.1 Data: Lemurs\nIn Chapter 7, we introduced a data set containing the weight, age, and other characteristics of lemurs living at the Duke Lemur Center. The data used in that chapter contained observations from a single weigh-in for each lemur between 0 and 24 months old. The lemurs were, in fact, weighed regularly, so the original data from Community (2024)  includes multiple measures for each lemur. The data in this section includes all the measurements for each lemur when they were 1 to 24 months old.\nThe data are in lemurs-repeated-measures.csv. We will use the following variables:\n\n\nweight: Weight of the lemur (in grams)\n\ntaxon : Code made as a combination of the lemur’s genus and species. Note that the genus is a broader categorization that includes lemurs from multiple species.  This analysis focuses on the following taxon:\n\nERUF: Eulemur rufus, commonly known as Red-fronted brown lemur\nPCOQ: Propithecus coquereli, commonly known as Coquerel’s sifaka\nVRUB: Varecia rubra, commonly known as Red ruffed lemur\n\n\n\nsex : Sex of lemur (M: Male, F: Female)\n\nage: Age of lemur when weight was recorded (in months)\n\nlitter_size: Total number of infants in the litter the lemur was born into (this includes the observed lemur)\n\nSee Community (2024) for the full codebook. \nThe goal of the analysis is to understand a lemur’s growth rate (relationship between age and weight) after adjusting for other characteristics.\n\n\n\n\n\nFigure 13.7: Structure of lemur data\n\n\nThe lemurs data set is an example of repeated measures, where the data are grouped by lemur as shown in Figure 13.7. These data violate the independence assumption of linear regression models (Section 8.5) and logistic regression models (Section 11.6), because we expected observations from a single lemur to be correlated with one another. Some factors (e.g., taxon ) will remain constant for all observations from an individual lemur. Table 13.4 shows the data for an individual lemur.\n\n\n\nTable 13.4: All observations from an individual lemur\n\n\n\n\ndlc_id\nname\ntaxon\nsex\nlitter_size\nage\nweight\n\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n1.32\n590\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n8.22\n2560\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n10.06\n2610\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n12.79\n2800\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n14.63\n3160\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n16.47\n3020\n\n\n6492\nMIRFAK\nVRUB\nM\n2\n18.84\n3340\n\n\n\n\n\n\n\n\nIn this data, the lemur’s dlc_id, name, sex, taxon, and litter_size remain the same for its observations. These variables are at level two of the data, i.e., they differ between lemurs but are the same for all observations from an individual lemur. The age and weight are different across observations. These are variables at level one, because they change each time data are collected.\nThe data set contains seven observations for the lemur Mirfak in Table 13.4 , but there are not necessarily seven observations for every lemur in the data. The number of observations for each lemur range from as few as 1 to as many as 120. The GLMMs can account for the different number of observations from each individual. When we fit the model, lemurs with more observations will be weighted more heavily when estimating the coefficients.\n\n\n\n\n\n\n\nFigure 13.8: weight versus age for nine lemurs\n\n\n\n\nNow let’s take a look at the relationship between weight and age for the lemurs in the data. Figure 13.8 shows the relationship between these variables for a sample of nine lemurs. The figure illustrates how the number of observations differ for each lemur. We can also begin to see how the weight changes with age for the lemurs. The visualizations of these 9 lemurs give us an idea of the variation in the data, but we would like to visualize the relationship for all the lemurs in the data. To do so, we use a spaghetti plot, shown in Figure 13.9.\n\n\n\n\n\n\n\nFigure 13.9\n\n\n\n\nThe gray lines in Figure 13.9 are the relationships between age and weight for each of the 248 lemurs in the data set. The blue line is the average trajectory across all the lemurs. From this plot, we more easily see the difference in the growth rate for the lemurs by the different slopes of the relationship between age and weight. We can also see the general differences in weight across the lemurs at each age.\nThe linear regression (and logistic regression) models we’ve seen thus have a single intercept for all observations. In other words, if we fit a model using age to predict weight, there would be a single intercept for the expected weight for new born lemurs. From Figure 13.9, we see that the lemurs are generally different weights (even lemurs with the same age and taxon), so a single intercept may not best represent the trend in the data We will use a random intercepts model to account for this additional variability.3\n\n13.2.2 Model fit and interpretation\nGLMMs differ from the GLMs we’ve studied thus far, because they include both fixed and random effects. The fixed effects are the variables we are interested in studying or explicitly adjusting for in the model. These are the variables for which we want to obtain an estimated coefficient. In the lemurs analysis, the fixed effects are age, taxon, sex, and litter_size. The random effects are those we don’t want to estimate explicitly but whose variability we want to understand. These are generally defined by the grouping variable in the data. In this analysis, the random effect is the individual lemur, captured by the variable dlc_id.\nThe random intercepts model includes fixed and random effects, such that the random effects account for random variability in the intercept. The general form of the model is \nLet \\(\\text{weight}_{ij}\\) be the weight in the \\(j^{th}\\) observation for lemur \\(i\\). Then, the random intercepts model for the lemurs analysis is\n\\[\n\\begin{aligned}\n\\text{weight}_{ij} = \\beta_0 &+ \\beta_1 ~ \\text{taxonPCOQ}_{i} + \\beta_2 ~ \\text{taxonVRUB}_i \\\\\n& +\\beta_3 ~ \\text{sexM}_{i}  + \\beta_4 ~ \\text{litter\\_size}_{i} + \\beta_5 ~ \\text{age}_{ij}\\\\\n& + u_i + \\epsilon_{ij} \\\\\n&u_i \\sim N(0, \\sigma^2_{u}) \\hspace{5mm} \\epsilon_{ij} \\sim N(0, \\sigma^2_{\\epsilon})\n\\end{aligned}\n\\tag{13.4}\\]\nLet’s break down the components of Equation 13.4. The subscript indicate whether the variable is at the first or second level of the data. The variable age is the only predictor in the second level that changes for each observation, so the subscript indicates that we plug in the age for lemur \\(i\\) at the \\(j^{th}\\) observation. The other predictors are measured at the second level of the data, so the subscript \\(i\\) indicates that we plug in the same value for lemur \\(i\\) across all its observations. The term \\(\\beta_0\\) is the global intercept. It is the expected (mean) weight across all lemurs when all predictors are equal to 0.\nThe error term \\(\\epsilon_{ij}\\) is like the error term previously introduced for linear models. It is the difference between the observed and expected value of the weight for the \\(j^{th}\\) observation from lemur \\(i\\). As in linear regression, these error terms follow a normal distribution with mean of 0 and variance of \\(\\sigma^2_{\\epsilon}\\). This \\(\\sigma^2_{\\epsilon}\\) is a measure of the variability in the weights (response variable) within an individual lemur (group).\nThe term \\(u_{i}\\) is the random effect that makes this the “random intercepts model”. We don’t estimate \\(u_i\\) directly, but its distribution tells us about the variability in the average weights between lemurs. The \\(u_{i}\\) are normally distributed with a mean of 0 and variance of \\(\\sigma^2_{u}\\). This \\(\\sigma^2_{u}\\) is the measure of between lemur variability. From this model, the intercept of an individual lemur \\(i\\) is \\(\\beta_0 + u_i\\) where, \\(u_i\\) is drawn from \\(N(0,\\sigma^2_{u})\\).\nThe coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_5\\) and the variance components \\(\\sigma_u\\) and \\(\\sigma_{\\epsilon}\\) are estimated using maximum likelihood estimation. The output for the lemurs random intercepts model is in Table 13.5.\n\n\n\nTable 13.5: Random intercepts model for lemurs data\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\nfixed\n\n(Intercept)\n16.0\n79.07\n0.202\n\n\nfixed\n\ntaxonPCOQ\n449.9\n67.12\n6.703\n\n\nfixed\n\ntaxonVRUB\n1021.4\n94.48\n10.811\n\n\nfixed\n\nsexM\n-27.1\n45.22\n-0.599\n\n\nfixed\n\nlitter_size\n-66.8\n45.14\n-1.479\n\n\nfixed\n\nage\n147.7\n0.97\n152.338\n\n\nran_pars\ndlc_id\nsd__(Intercept)\n311.5\n\n\n\n\nran_pars\nResidual\nsd__Observation\n356.3\n\n\n\n\n\n\n\n\n\n\nThe equation of the fitted model is\n\\[\n\\begin{aligned}\n\\widehat{\\text{weight}}_{ij} = 15.969 &+  449.932 ~ \\text{taxonPOQ} +  1021.387 ~ \\text{taxonVRUB} \\\\\n&  -27.073 ~ \\text{sexM} - 66.771 ~ \\text{litter\\_size} + 147.719 ~ \\text{age} \\\\\n&\\hat{\\sigma}_u = 311.504 \\hspace{5mm} \\hat{\\sigma}_\\epsilon = 356.275\n\\end{aligned}\n\\]\n\nUse the model in Table 13.5.\n\nInterpret the coefficient of age in the context of the data.\nExplain what \\(\\hat{\\sigma}_u = 311.504\\) means in the context of the data.4\n\n\n\n13.2.3 Random intercepts model in R\nRandom intercepts models are fit using the lmer function in the lme4 R package (Bates et al. 2015). The syntax for the response and predictor variables is the same as in lm(). The grouping variable is specified using the syntax (1|grouping_var). This specifies that the random intercepts are based on grouping_var. The syntax for the model in Table 13.5 is below.\n\nlemurs_fit &lt;- lmer(weight ~ taxon + sex + litter_size + age + \n                     (1|dlc_id), \n                   data = lemurs)\n\nlemurs_fit\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: weight ~ taxon + sex + litter_size + age + (1 | dlc_id)\n   Data: lemurs\nREML criterion at convergence: 54636\nRandom effects:\n Groups   Name        Std.Dev.\n dlc_id   (Intercept) 312     \n Residual             356     \nNumber of obs: 3715, groups:  dlc_id, 248\nFixed Effects:\n(Intercept)    taxonPCOQ    taxonVRUB         sexM  litter_size          age  \n       16.0        449.9       1021.4        -27.1        -66.8        147.7  \n\n\nThe results can be displayed in a tidy format using tidy function in the broom.mixed R package (Bolker and Robinson 2024). The results below are neatly displayed using kable().\n\ntidy(lemurs_fit) |&gt;\n  kable()\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\nfixed\n\n(Intercept)\n16.0\n79.07\n0.202\n\n\nfixed\n\ntaxonPCOQ\n449.9\n67.12\n6.703\n\n\nfixed\n\ntaxonVRUB\n1021.4\n94.48\n10.811\n\n\nfixed\n\nsexM\n-27.1\n45.22\n-0.599\n\n\nfixed\n\nlitter_size\n-66.8\n45.14\n-1.479\n\n\nfixed\n\nage\n147.7\n0.97\n152.338\n\n\nran_pars\ndlc_id\nsd__(Intercept)\n311.5\n\n\n\n\nran_pars\nResidual\nsd__Observation\n356.3\n\n\n\n\n\n\n\n\n13.2.4 Further reading\nWe have just scratched the surface in this section, but there are many resources for readers interested in learning more about random intercepts models and other types of GLMMs. See Beyond Multiple Linear Regression (Roback and Legler 2021) and Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007) for an in-depth introduction to these topics.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#sec-decision-trees",
    "href": "13-special-topics.html#sec-decision-trees",
    "title": "13  Special topics",
    "section": "\n13.3 Decision trees",
    "text": "13.3 Decision trees\nThe models presented in this book thus far have been parametric methods, in which we specify a form of the model then estimate the parameters of that model (Section 1.1.1). An alternative to this approach are tree-based models, a set of non-parametric methods that split the data based on a series of decision points. These methods are particularly useful if a precise interpretation of the relationship between the response and predictor variables is not needed, but rather a general understanding how to use the predictors to estimate the response is sufficient.\nIn the book Introduction to Statistical Learning 2nd edition, James et al. (2021) [pp. 339] lists advantages of tree-based models compared to GLMs.\n\n\nThey are easier for others to understand;\nThey are believed to better mirror the human decision-making process;\nThey can more easily represented visually, making it easier for non-experts to understand them;\nThey can use categorical predictors without creating indicator variables.\n\n\nThis list could be summarized as “The key advantage to tree-based models is that they are easy for others, especially non-experts, to understand.” Here we introduce two tree-based models: regression trees for quantitative response variables and classification trees for categorical response variables.\n\n13.3.1 Regression trees\nRegression trees are decision trees to model data with quantitative response variables, so they are often an alternative to linear regression. The general idea is to use the predictor variables to split the data into groups of similar observations. For each observation, the predicted value of the response is computed as the mean response among the observations in its group. To illustrate this, let’s go back to the total expenditure on basketball programs for Division I NCAA institutions introduced in Chapter 9. We will use a regression tree to predict expenditure_m (total expenditure in millions of dollars) using type (private or public institution), region (North Central, Northeast, South, West), and enrollment_th (total student enrollment in thousands).\n\n\n\n\n\n\n\n\n\n(a) Original expenditure_m\n\n\n\n\n\n\n\n\n\n(b) Log-transformed expenditure_m\n\n\n\n\n\n\nFigure 13.10: Distribution of original and log-transformed expenditure_m\n\n\nThe distribution of expenditure_m is heavily right-skewed Figure 13.10 (a), so we will use the log-transformed variable log(expenditure_m) in the model Figure 13.10 (b).\n\nWhen the response variable is strongly skewed, it is good practice to use the transformed version in the regression tree. Though regression trees do not rely on the same assumptions as linear regression, their results can be strongly influenced by extreme observations when the distribution of the response variable is heavily skewed. Predictor variables do not need to be transformed for regression trees, because the tree does not depend on a linear relationship between the response and predictor variables.\n\nA regression tree is made up of a series of nodes (also called leaves) and branches. The nodes are the decision-making points at each split (e.g., enrollment_th &lt; 10?). The branches connect two nodes (e.g, enrollment_th &lt; 10 = TRUE and enrollment_th &lt; 10 = FALSE). The terminal nodes are the points at the end of the tree where the data are no longer split and the predictions are made. The prediction is the mean value of the response variable for the observations in that node.\nAt each step, the model searches to find the variable and associated cut point that minimizes the sum of squares residual (SSR) in Equation 13.5. This is the same SSR introduced in Section 4.7.2. Minimizing SSR maximizes the similarity of observations within a node. This process continues until making a new split no longer minimizes the SSR.\n\\[\nSSR = \\sum_{i = 1}^n(y_i - \\hat{y}_i)^2\n\\tag{13.5}\\]\nNow let’s fit the regression tree to predict expenditure_m using enrollment_th, region, and type. Because the primary objective for the tree is prediction, we split the data into training (80%) and testing (20%) sets (Section 10.4). This will give us an assessment on how well the model performs on new data.\n\n\n\n\n\n\n\nFigure 13.11: Regression tree predicting NCAA basketball expenditure\n\n\n\n\nFigure 13.11 is the output of the regression tree. In this output, we see the variables and associated cutpoint used to make the split at each step, along with the percentage of observations in each subsequent node. This output also shows the predicted expenditure_m in each node.\nFor example, the first split is whether enrollment_th &lt; 12. About 62% of the observations in the training data have enrollment_th &lt; 12, and the mean log(expenditure) for those observations is 1.5 million. In contrast, about 38% of the observations have enrollment_th &gt;= 12, and the mean log(expenditure) for those observations is about 2.4 million. If we were to stop there, then 1.5 million would be the predicted log(expenditure) for all observations with enrollment &lt; 12 and 2.4 million would be the predicted log(expenditure) for all other observations.\nLet’s use the tree to find the predicted expenditure for a private institution in the South region that has enrollment of 6,000 students. At the first step, we go down the “Yes” branch for enrollment_th &lt; 12. Next, we go down the “No” branch for type = Public. Next, we go down the “No” branch for enrollment_th &lt; 4.1. Next, we go down the branch, “No” for region = Northeast. Lastly, we go down the “Yes” branch for enrollment_th &lt; 6.3. This branch leads to the terminal node of \\(\\widehat{\\log(\\text{expenditure\\_m})} = 2.2\\), which is equal to a predicted expenditure of 9.025 (\\(e^{2.2}\\)) million dollars.\n\nWhat is the predicted expenditure for a public institution in the West region with enrollment of 20,000 students.5\n\nWe evaluate the model’s performance using the testing data. We compute the predicted expenditures for the observations in the testing data the root mean square error (RSME), introduced in Section 4.7, as a measure of the model performance.\nThe RMSE for the testing data is 0.506. This is the terms of the response variable, log(expenditure_m), so the error in terms of the original variable is 1.659. This means that the average difference between the observed expenditure and predicted is 1.659 million dollars.\n\n13.3.2 Classification trees\nClassification trees are decision trees for predicting categorical outcomes. We might use such a tree as an alternative to logistic regression Chapter 11 or multinomial logistic regression Section 13.1. They are very similar to regression trees with two primary differences. The first is that predictions in the terminal nodes are the predicted classes. The second is that the potential variables and associated cutoffs to define each split are evaluated using different criteria. The default in most software is the Gini Index shown in Equation 13.6\n\\[\n\\text{Gini index} = 1 - \\sum_{k = 1}^K p_{mk}^2\n\\tag{13.6}\\]\nwhere \\(p_{mk}\\) is the proportion of observations in the \\(m^{th}\\) node that belong to class \\(k\\). Similar to RSS, the model selects the variable and cutoff that minimizes the Gini index at each step. Low values of the Gini index indicate a large proportion of observations in a given node are from the same class.\nLet’s go back to the voter frequency data introduced in Section 13.1.1. We’ll use a classification tree to predict the voter_category (rarely/never, sporadic, always) using age, party_id, and an additional variable income_cat. We split the data into training (80%) and testing sets (20%), so we can evaluate the model’s performance on new data.\n\n\n\n\n\n\n\nFigure 13.12: Classification tree voter frequency\n\n\n\n\nFigure 13.12 is the output from the classification tree. Something that may stand out is that the predictor party_id is not used in the tree. This means there was never a point where using party_id was more useful in classifying observations compared to the other predictors.\nThe first split in Figure 13.12 is at age &lt; 34. About 21% of the observations have age &lt; 34, and this group leads to a terminal node. Among those with age &lt; 34, 34% of them are classified as rarely/never, 21% are classified as sporadic, and 30% are classified as always in the data. Therefore, if age &lt; 34, the predicted class is rarely/never based on the classification tree. There is a bit more work among those with age &gt;= 34. The next split is age &lt; 61. If age &lt; 61, then there is a terminal node with the predicted class sporadic. Otherwise, income_cat is taken into account, with a split at income_cat = Less than $40k. If yes, then the predicted class is sporadic; if no, the predicted class is always.\n\nBased on Figure 13.12, what is the predicted voting frequency for an a 40 year old individual whose party affiliation is Democrat and annual income is $70,000?6\n\n\n13.3.3 Decision trees in R\nWe fit decision trees using the rpart() function in the rpart R package (Therneau and Atkinson 2025). The argument method = is used to specify the type of decision tree.\nBelow is the code for the regression tree fit in Section 13.3.1. The argument method = \"anova\" specifies that we are fitting a regression tree. If we type the name of the tree, we see a summary of the nodes and branches\n\nncaa_tree &lt;- rpart(log(expenditure_m) ~ enrollment_th + \n                     type + region,\n                   data = ncaa_train,\n                   method = \"anova\")\n\nncaa_tree\n\nn= 284 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 284 165.00 1.840  \n   2) enrollment_th&lt; 11.7 176  65.20 1.510  \n     4) type=Public 95  15.90 1.240  \n       8) enrollment_th&lt; 4.35 22   2.32 0.821 *\n       9) enrollment_th&gt;=4.35 73   8.64 1.360 *\n     5) type=Private 81  34.30 1.820  \n      10) enrollment_th&lt; 4.06 40   5.41 1.500 *\n      11) enrollment_th&gt;=4.06 41  20.70 2.140  \n        22) region=Northeast 18   5.53 1.650 *\n        23) region=North Central,South,West 23   7.60 2.520  \n          46) enrollment_th&lt; 6.32 12   2.62 2.180 *\n          47) enrollment_th&gt;=6.32 11   2.13 2.880 *\n   3) enrollment_th&gt;=11.7 108  49.30 2.370  \n     6) enrollment_th&lt; 31.5 96  42.70 2.290  \n      12) enrollment_th&lt; 18.1 38  15.10 2.080  \n        24) region=North Central,Northeast,West 20   4.98 1.880 *\n        25) region=South 18   8.42 2.300 *\n      13) enrollment_th&gt;=18.1 58  24.70 2.440  \n        26) region=West 19   7.67 2.090 *\n        27) region=North Central,Northeast,South 39  13.60 2.610 *\n     7) enrollment_th&gt;=31.5 12   1.09 3.010 *\n\n\nThe code below is the classification tree from Section 13.3.2. We use the argument method = \"class\" to specify the classification tree. Similar to regression trees, we can type the name of the tree to see a summary of the nodes and branches.\n\nvoters_tree &lt;- rpart(voter_category ~ age + party_id +\n                       income_cat + educ, \n                   data = voters_train, \n                   method = \"class\")\n\nvoters_tree\n\nn= 4668 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 4668 2590 sporadic (0.2509 0.4460 0.3031)  \n   2) age&lt; 33.5 973  495 rarely/never (0.4913 0.2127 0.2960) *\n   3) age&gt;=33.5 3695 1820 sporadic (0.1876 0.5074 0.3050)  \n     6) age&lt; 60.5 2017  879 sporadic (0.2558 0.5642 0.1800) *\n     7) age&gt;=60.5 1678  914 always (0.1055 0.4392 0.4553)  \n      14) income_cat=Less than $40k 452  223 sporadic (0.1770 0.5066 0.3164) *\n      15) income_cat=$125k or more,$40-75k,$75-125k 1226  605 always (0.0791 0.4144 0.5065) *\n\n\nThe summaries produced by rpart() are challenging to read, so we use the rpart.plot() from the rpart.plot R package (Milborrow 2025) to create a visual representation of the tree. The code below produces the visualization of the regression tree for NCAA basketball expenditures.\n\nrpart.plot(ncaa_tree)\n\n\n\n\n\n\n\nPredictions are produced using predict(). This will produce a vector of predictions that can be added to the original data frame to evaluate model performance.\nThe code below produces the predictions for the testing data from the regression tree predicting NCAA basketball expenditures and saves the predictions as a column in the testing data. The same code is used to produce predictions from the classification tree.\n\nncaa_test &lt;- ncaa_test |&gt;\n  mutate(predict_expend = predict(ncaa_tree, ncaa_test))\n\nThe predictions are in the same units as the response variable, so in this case, the predicted values are the predicted log(expenditure_m).\n\n13.3.4 Further reading\nOverall, decision trees can be a nice alternative to linear and logistic regression. Their greatest advantage is that they can be easily represented visually and thus easier for non-experts to understand. The two greatest disadvantages to these models, however, are the inferior predictive accuracy compared to other modeling approaches and their sensitivity to small changes in the data (James et al. 2021, 340). In this section, we introduced decision trees, but there is extensive work in tuning (customizing) these models along with more advanced models that take into account multiple trees (e.g., random forest). We refer the reader to Chapter 8 James et al. (2021) for an in-depth discussion on the tree-based models.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#sec-causal",
    "href": "13-special-topics.html#sec-causal",
    "title": "13  Special topics",
    "section": "\n13.4 Causal inference",
    "text": "13.4 Causal inference\nThroughout this text, we have interpreted the coefficients of regression models to describe the association between the response and predictor variables and the “expected” change in the response variable based on the model. In other words, we have been bound by the common phrase in statistics “correlation does not equal causation.” There are many situations in practice, however, in which we want to make a stronger claim and conclude that “X causes Y.” For example, medical researchers developing a new treatment want to know if the treatment causes improved health outcomes. In social sciences, researchers may want to know if a particular social program improves outcomes for individuals living in poverty. Economists want to know the impact of changing economic measures, such as interest rates, on the economy overall. Each of these scenarios requires an analysis that can more thoroughly account for confounding variables, underlying variables that impact both the response and predictor variables, so that the impact of the medical treatment, social program, or economic measure can be quantified. We will refer to all of these as treatment or intervention throughout the rest of the section.\nThe ability to make causal claims start with the data. There are two types of data: experimental and observational data. Experimental data are data obtained from a randomized-control experiment  in which individuals were randomly assigned to the treatment group that receives the intervention or control group that does not receive the intervention. Because individuals are randomly assigned groups, we can assume the only difference between the treatment and control groups is the intervention itself. Therefore, as we interpret the model coefficients associated with the treatment, we can interpret it in terms of how a change in the treatment causes in a change in the response.\nThe ideal randomized experiment is often not feasible in practice due to a variety of factors. For example, it may be unethical to assign individuals to a control group and withhold beneficial treatment, or it is not feasible to restrict the behavior of individuals to implement experimental conditions. Therefore, researchers often rely on observational data, even when they want to draw conclusions stronger than a “correlation” or “association” between some treatment or intervention (predictor) and an outcome (response). To make such causal claims, we use statistical methods to make the observational data “look” more like data from a randomized experiment. These methods make up the branch of statistics called causal inference.\n\n13.4.1 Propensity score matching\nIn propensity score matching, we compute the probability (propensity) an observation is assigned to the treatment group based on a set of confounding variables that directly impact both the response and the likelihood an individual is in the treatment group. We then match individuals in the treatment and control groups who have the same or similar propensities to create a new data set in which the control and treatment groups have similar characteristics. The matched data set is used to estimate the effect of the treatment. \nLet’s apply propensity score matching to evaluate the effect of an educational program, Project ACE: Action for Equity. Project ACE was a “five year interdisciplinary program aimed to get more underrepresented high school students from disadvantaged backgrounds to get interested in college degrees in engineering as well as biomedical and behavioral sciences” (Texas at El Paso College of Liberal Arts n.d.). We will analyze data from Evans, Perez, and Morera (2025) who use propensity score matching to analyze outcomes of high school students. The data are available in project-ace-data.csv. We will follow their analysis closely with some modifications, so we refer readers to their article for the full analysis.\nWe will use the variables below. The definitions are adapted from Evans, Perez, and Morera (2025).\n\n\nGrade: Grade level (9, 10, 11, 12)\n\nGender: Gender (F, M)\n\nEthnicity: Ethnicity (Hispanic, Non-Hispanic)\n\nELL: Whether student is an English language learner (N, Y)\n\nSped: Whether student is in a special education program (N, Y)\n\nHomeless: Whether student is homeless (N, Y)\n\nTracking.Pathway: Whether student was in Project ACE (Treatment) or not (Control)\n\nCurrent.GPA: Grade Point Average (GPA) ranging 0 to 5.0\n\n\n\n\n\n\n\n\nFigure 13.13: Distribution of GPA for treatment and control groups\n\n\n\n\nFigure 13.13 shows the distribution of the response variable Current.GPA for the treatment and control groups. There is a lot of overlap in the distributions, but there does appear to be a higher proportion of students in the treatment group who have higher GPAs. The goal of this analysis is to measure if there is a difference in the GPAs between students in Project ACE versus those not in the program, and if so, the effect of the Project ACE on GPA.\nSome variables are associated with both the response and the chance to receiving the treatment. For example, research shows that demographic and socioeconomic variables are associated with academic performance (Evans, Perez, and Morera 2025). Because Project ACE is aimed at students who are “underrepresented” and from “disadvantaged backgrounds”, demographic and socioeconomic factors are also associated with the probability of being in the program. We see examples of this in Figure 13.14, where the distributions of three potential covariates differ between control and treatment groups.\n\n\n\n\n\n\n\n\n\n(a) Grade\n\n\n\n\n\n\n\n\n\n(b) Gender\n\n\n\n\n\n\n\n\n\n\n\n(c) ELL\n\n\n\n\n\n\nFigure 13.14: Distribution of Subset of confounding variables for treatment and control groups.\n\n\nIn a standard regression model, these variables and the other demographic and socioeconomic variables would be confounders that limit the strength of the claim we can make about the effect of the treatment, even if these variables are included as predictors in the model. Therefore, we will match the data in a way such that the distributions of these factors are similar between the treatment and control groups.\n\nChoosing the variables to include in the propensity score model can be challenging. We often choose these variables based on previous research and collaboration with individuals who are domain experts. Deciding up front which variables are important helps strengthen the analysis conclusions, as there is less potential there are confounding variables impacting the model of the treatment effect.\n\n\n\nThe propensity score model is a logistic regression model where the response variable is the binary indicator for Treatment vs. Control. As shown in Equation 13.7, we use this model to compute the probability that an individual is in the treatment group. The form of the propensity score model for this analysis is as follows:\n\\[\n\\begin{aligned}\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 &+ \\beta_1 ~ \\text{Grade10} + \\beta_2 ~ \\text{Grade11} + \\beta_3 ~ \\text{Grade12}  \\\\\n& + \\beta_4 ~ \\text{GenderM} + \\beta_5 ~ \\text{EthnicityNon-Hispanic} \\\\\n&+ \\beta_6 ~ \\text{ELL} + \\beta_7 ~ \\text{Sped} + \\beta_8 ~ \\text{Homeless}\n\\end{aligned}\n\\tag{13.7}\\] where \\(\\pi\\) is the propensity, the probability of being in the treatment group (participating in Project ACE). Table 13.6 is the output of the fitted propensity score model. \n\n\n\nTable 13.6\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n-1.670\n0.216\n-7.733\n0.000\n\n\nGrade10\n0.539\n0.264\n2.040\n0.041\n\n\nGrade11\n0.566\n0.263\n2.155\n0.031\n\n\nGrade12\n0.314\n0.273\n1.151\n0.250\n\n\nGenderM\n-1.390\n0.206\n-6.731\n0.000\n\n\nEthnicity.Hispanic.Y.NNon Hispanic\n-0.498\n0.775\n-0.643\n0.520\n\n\nELLY\n-0.792\n0.222\n-3.562\n0.000\n\n\nSpedY\n0.017\n0.277\n0.060\n0.952\n\n\nHomelessY\n1.126\n0.649\n1.734\n0.083\n\n\n\n\n\n\n\n\nIn general, we are not interested in interpreting the coefficients of the propensity score model but rather using it to compute the propensity for each observation. Before using the propensity scores to match, let’s take a look at the distribution of propensity scores for the treatment and control group in Figure 13.15.\n\n\n\n\n\n\n\nFigure 13.15: Distribution of propensity scores for treatment and control groups\n\n\n\n\nIn Figure 13.15, we see that the center of the distribution of propensity scores for individuals in the treatment group is higher than the center for the control group. This is what we might expect, because the individuals in the treatment group are more likely to have the characteristics of eligibility to participate in Project ACE.\nThe other important feature from this graph is the large overlap between the two distributions. This indicates common support, in which individuals in both the treatment and control groups have a nonzero probability of receiving the treatment. In the context of this analysis, this means every student had a non-zero chance of participating in Project ACE. Practically speaking, we use the propensity scores to match observations in the treatment group with an observation in the control group, so there needs to be overlap in the distributions.\nNow let’s use the propensity scores to match observations. The number of observations in the treatment group is often equal to or less than the number of observations in the control group. Therefore, the matching is done such that each observation in the treatment group is matched to an observation in the control group. Then, the observations in the control group that are not matched are discarded. In the Project ACE analysis, there are 148 observations in the treatment group and 1152 observations in the control group. Therefore, the final matched data set will contain 148 observations in the treatment group and 148 observations in the control group. The unmatched 1004 observations in the control group are discarded.\nOne of the most widely used matching approaches is nearest-neighbor matching, in which each observation in the treatment group is matched with an observation in the control group with the closest propensity score. Table 13.7 shows three treatment/control pairs in the matched data for the Project ACE analysis.\n\n\n\nTable 13.7: Three pairs of matched observations based on the model in Table 13.6. The column subclass identifies the matches.\n\n\n\n\nsubclass\nTracking.Pathway\npropensity_score\n\n\n\n14\nControl\n0.367\n\n\n14\nTreatment\n0.443\n\n\n50\nControl\n0.249\n\n\n50\nTreatment\n0.249\n\n\n118\nControl\n0.158\n\n\n118\nTreatment\n0.158\n\n\n\n\n\n\n\n\nSometimes there are exact matches, where an observation in the treatment group is matched with an observation in the control group with an equal propensity score. Matches 50 and 118 in the table are examples of this. This generally indicates that the two observations are the same in terms of the characteristics included in the propensity score model Equation 13.7. In other cases, such as Match 14, the scores are close but not exact.\n\n\n\nTable 13.8: Features for observations in Match 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTracking.Pathway\nGrade\nGender\nEthnicity.Hispanic.Y.N\nELL\nSped\nHomeless\npropensity_score\n\n\n\nTreatment\n12\nF\nHispanic\nN\nN\nY\n0.443\n\n\nControl\n9\nF\nHispanic\nN\nN\nY\n0.367\n\n\n\n\n\n\n\n\nFrom Table 13.8, we see that the two observations that make up the 14th match are the same in all characteristics except grade. The student in the treatment group is in grade 12, and the student in the control group is in grade 9.\n\nBefore fitting the model to estimate the effect of the treatment, let’s take a look at the distributions of the variables we examined in Figure 13.14, and see how the distributions compare between the treatment and control group for the 296 observations in the matched data.\n\n\n\n\n\n\n\n\n\n(a) Grade\n\n\n\n\n\n\n\n\n\n(b) Gender\n\n\n\n\n\n\n\n\n\n\n\n(c) ELL\n\n\n\n\n\n\nFigure 13.16: Distribution of subset of confounding variables for treatment versus control groups in the matched data.\n\n\nIn Figure 13.16, the distributions of these student characteristics are now the same for the treatment and control groups (compare to Figure 13.14). For brevity, we only show three variables here, but we could look at the distributions for all variables in the propensity score model Equation 13.7. Now the data look more like what we would expect in a randomized experiment. Therefore, we can be more confident that any differences in the GPA (response variable) are due to the treatment and not underlying confounding variables. \nOnce we have the matched data set, it’s time to evaluate the effect of the treatment. We do this by using a regression model with the treatment as a predictor.\n\\[\n\\text{GPA} = \\beta_0 + \\beta_1 ~\\text{Treatment} + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon})\n\\tag{13.8}\\] For the Project ACE analysis, the response variable GPA is quantitative, so we use a linear regression model of the form in Equation 13.8. The fitted model is in Table 13.9.\n\n\n\nTable 13.9: Model of treatment effect with 95% confidence intervals for coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n2.430\n0.069\n35\n0.000\n2.294\n2.57\n\n\nTracking.PathwayTreatment\n0.196\n0.098\n2\n0.046\n0.003\n0.39\n\n\n\n\n\n\n\n\nWe interpret the model just as we interpret other linear regression models (see Chapter 4 and Chapter 7). In Table 13.9, the estimated effect of the treatment, participating in Project ACE is 0.196 and the 95% confidence interval is 0.003 to 0.39. This estimate 0.196 is the average treatment effect (ATE), the difference in the expected response between the treatment and control group. Therefore, based on this analysis, we can make a causal relationship between participating in Project ACE and higher GPA, on average. The 95% confidence interval gives a plausible range for the effect in the population.\n\nInterpret the 95% confidence interval for the effect of participating in Project ACE in the context of the data.7\n\nAs we use these results to inform decision-making, one thing to keep in mind is that this is the average treatment effect in the matched population. Therefore, the results may not generalize to the entire population if the discarded individuals in the control group are significantly different from the individuals in the matched group. One way to mitigate this is to include more observations from the control group. Strategies for this are available in the resources at the end of this section.\n\n13.4.2 Causal inference in R\nMuch of the code for causal inference is the code for linear lm() and logistic glm() models that we’ve seen in previous chapters. The primarily new code is for creating the matched data set using the propensity scores.\nWe compute the propensity scores “manually” by fitting a logistic regression model using glm().\n\npropensity_score_model &lt;- glm(Tracking.Pathway ~ Grade + Gender + \n                                Ethnicity.Hispanic.Y.N + ELL + \n                                Sped + Homeless,\n                        data = ace, \n                        family = \"binomial\")\n\nThe assigned matches are done using the matchit() function from the MatchIt R package (Ho et al. 2011). The code below shows the propensity score matching for the Project Ace data.\nThe argument method = \"nearest\" species to conduct nearest-neighbor matching (the default method), and distance = \"logit\" indicates to compute the propensity scores using a logistic regression model. The matchit() function will refit the model with propensity scores.\n\nace_psm &lt;- matchit(Tracking.Pathway ~ Grade + Gender + Ethnicity.Hispanic.Y.N +\n                          Race + ELL + Sped + Homeless,\n                 data = ace,\n                 method = \"nearest\",   \n                 distance = \"logit\")\n\nace_psm\n\nA `matchit` object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 1300 (original), 296 (matched)\n - target estimand: ATT\n - covariates: Grade, Gender, Ethnicity.Hispanic.Y.N, Race, ELL, Sped, Homeless\n\n\nThis code creates a matchit object called ace_psm that contains information about the propensity score matching. We retrieve the matched data using the match.data() function as shown below.\n\nace_matched &lt;- match.data(ace_psm)\n\nWe use glimpse() to see what is in the new matched data set.\n\n\nRows: 296\nColumns: 17\n$ Student.ID                                        &lt;dbl&gt; 9478, 8268, 9846, 22…\n$ Grade                                             &lt;fct&gt; 12, 12, 12, 12, 11, …\n$ Gender                                            &lt;chr&gt; \"M\", \"M\", \"F\", \"F\", …\n$ Race                                              &lt;chr&gt; \"C\", \"C\", \"C\", \"C\", …\n$ Ethnicity.Hispanic.Y.N                            &lt;chr&gt; \"Hispanic\", \"Hispani…\n$ ELL                                               &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", …\n$ Sped                                              &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", …\n$ Homeless                                          &lt;chr&gt; \"N\", \"N\", \"Y\", \"N\", …\n$ Free.Lunch                                        &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", …\n$ Migrant                                           &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", …\n$ Current.GPA                                       &lt;dbl&gt; 1.45, 2.96, 2.96, 3.…\n$ Number.of.Classes.Enrolled.in.for.the.school.year &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7,…\n$ Tracking.Pathway                                  &lt;fct&gt; Control, Treatment, …\n$ propensity_score                                  &lt;dbl&gt; 0.0603, 0.0603, 0.44…\n$ distance                                          &lt;dbl&gt; 0.0590, 0.0590, 0.45…\n$ weights                                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1,…\n$ subclass                                          &lt;fct&gt; 1, 1, 42, 3, 2, 2, 4…\n\n\nThere are some new columns in addition to the columns in the original ace tibble. The column distance contains the propensity scores. The column sub.class identifies the matches. The column weights contains sample weights. All observations are equally weighted using the methods introduced in this section. See the resources in Section 13.4.3 to learn about propensity score matching that incorporates weighting.\nThe lm function is used to fit the linear regression model for the treatment effect.\n\ntreatment_effect &lt;- lm(Current.GPA ~ Tracking.Pathway, data = ace_matched)\n\n\n13.4.3 Further reading\nGiven the plethora of data available today, it is no surprise that causal inference is a rich and growing field in statistics and data science. We refer interested readers to The effect: An introduction to research design and causality (Huntington-Klein 2021) and Causal Inference: The Mixtape (Cunningham 2021) for an introduction to causal inference on observational data and Design and Analysis of Experiments (Montgomery 2017) for an introduction to experimental design.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#summary",
    "href": "13-special-topics.html#summary",
    "title": "13  Special topics",
    "section": "\n13.5 Summary",
    "text": "13.5 Summary\nIn this chapter, we introduced models that are extensions of linear and logistic regression. Specifically, we introduced the multinomial logistic regression model for categorical response variables with at least three levels, the random intercepts model for data with correlated observations, decision trees for prediction, and propensity scores models for causal inference. The goal of this book is to introduce readers to regression analysis, so this chapter is a springboard for readers interested in learning more about modeling. All the resources mentioned throughout the chapter are a nice followup to this text for readers interested in a deeper dive into the special topics introduced here and much more.\n\n\n\n\nBates, Douglas, Martin Maechler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using Lme4” 67. https://doi.org/10.18637/jss.v067.i01.\n\n\nBolker, Ben, and David Robinson. 2024. “Broom.mixed: Tidying Methods for Mixed Models.” https://doi.org/10.32614/CRAN.package.broom.mixed.\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale university press.\n\n\nEvans, Nicholas D, Perla C Perez, and Osvaldo F Morera. 2025. “Testing the Efficacy of Educational Interventions on Matched Student Samples: A Primer for Propensity Score Matching in r.” Journal of STEM Outreach 8 (1): 1–9.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference” 42. https://doi.org/10.18637/jss.v042.i08.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman; Hall/CRC.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2025. “Yardstick: Tidy Characterizations of Model Performance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\nMilborrow, Stephen. 2025. “Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’Plot.rpart’.” https://doi.org/10.32614/CRAN.package.rpart.plot.\n\n\nMontgomery, Douglas C. 2017. Design and Analysis of Experiments. John wiley & sons.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in r. Chapman; Hall/CRC.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. “Broom: Convert Statistical Objects into Tidy Tibbles.” https://CRAN.R-project.org/package=broom.\n\n\nTexas at El Paso College of Liberal Arts, The University of. n.d. “Project ACE: Action for Equity.” https://www.utep.edu/liberalarts/project-ace/.\n\n\nTherneau, Terry, and Beth Atkinson. 2025. “Rpart: Recursive Partitioning and Regression Trees.” https://doi.org/10.32614/CRAN.package.rpart.\n\n\nThomson-DeVeaux, Amelia, Jasmine Mithani, and Laura Bronner. 2020. “Why Many Americans Don’t Vote.” FiveThirtyEight. https://web.archive.org/web/20201102013301/https://projects.fivethirtyeight.com/non-voters-poll-2020-election/.\n\n\nVenables, W. N., and B. D. Ripley. 2002. “Modern Applied Statistics with s.” https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nXie, Yihui. 2024. “Knitr: A General-Purpose Package for Dynamic Report Generation in r.” https://yihui.org/knitr/.",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "13-special-topics.html#footnotes",
    "href": "13-special-topics.html#footnotes",
    "title": "13  Special topics",
    "section": "",
    "text": "Similar to how categorical predictors are interpreted as one level versus the baseline (see 7.4.2).↩︎\nThe accuracy is the 0.493 \\((563 + 2219 + 94)/5836)\\). The misclassification rate is 0.507 \\((1 - 0.407)\\).↩︎\nWe can also specify the GLMMs to account for the different slopes of age across the lemurs. That is beyond the scope of this text, but we provide resources at the end of the section for readers interested in learning more.↩︎\n\nage: As a lemur gets one month older, its weight is expected to increase by 147.719 grams, on average, after adjusting for taxon, sex, and litter size.\n\\(\\hat{\\sigma}_u = 311.504\\): The variability in the average weights between lemurs is 311.504. On average, the mean weight of an individual lemur is 311.504 grams away from the overall mean weight of all lemurs.↩︎\n\nThe predicted expenditure is 2.1 million. Note that the fact that it was a public institution was not used based on its path in the decision tree.↩︎\nThe predicted voting frequency is sporadic.↩︎\nWe are 95% confident that improvement in GPA from participating in Project ACE is between 0.003 and 0.39 points.↩︎",
    "crumbs": [
      "Part 4: Beyond linear regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html",
    "href": "appendix-linear.html",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "",
    "text": "A.1 Least-squares estimators for simple linear regression\nBelow are the mathematical details for deriving the least-squares estimators for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)) using calculus. We obtain the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by finding the values of \\(\\beta_0\\) and \\(\\beta_1\\), respectively, that minimize the sum of squared residuals (Equation A.1).\nSuppose we have a data set with \\(n\\) observations \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Then the sum of squared residuals is\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\beta_0 + \\beta_1 x_i)]^2 = [y_i - \\beta_0 - \\beta_1 x_i]^2\n\\tag{A.1}\\]\nTo find the value of \\(\\beta_0\\) that minimizes Equation A.1, we begin by taking the partial derivative of Equation A.1 with respect to \\(\\beta_0\\). Similarly, we take the partial derivative with respect to \\(\\beta_1\\) to find the value of \\(\\beta_1\\) that minimizes Equation A.1. The partial derivatives are\n\\[\n\\begin{aligned}\n&\\frac{\\partial \\text{SSR}}{\\partial \\beta_0} = -2 \\sum\\limits_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i) \\\\[10pt]\n&\\frac{\\partial \\text{SSR}}{\\partial \\beta_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\beta_0 - \\beta_1 x_i)\n\\end{aligned}\n\\tag{A.2}\\]\nTherefore, we want to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that satisfy the following:\n\\[\n\\begin{aligned}\n&-2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[10pt]\n&-2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{aligned}\n\\tag{A.3}\\]\nLet’s focus on \\(\\hat{\\beta}_0\\) for now and find \\(\\hat{\\beta}_0\\) that satisfies the first equality in ?eq-par-deriv-estimators. The mathematical steps are below in Equation A.4\n\\[\n\\begin{aligned}&-2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}(y_i - \\sum\\limits_{i=1}^{n} \\hat{\\beta}_0 - \\sum\\limits_{i=1}^{n} \\hat{\\beta}_1 x_i) = 0 \\\\[10pt]\n&\\Rightarrow  \\sum\\limits_{i=1}^{n}y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = n\\hat{\\beta}_0  \\\\[10pt]\n&\\Rightarrow \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big) = \\hat{\\beta}_0  \\\\[10pt]\n&\\Rightarrow \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\hat{\\beta}_0 \\\\\n\\end{aligned}\n\\tag{A.4}\\]\nFrom Equation A.4, we know the \\(\\hat{\\beta}_0\\) that satisfies the first equality in Equation A.3 is\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} \\tag{A.5}\\]\nThe formula for \\(\\hat{\\beta}_0\\) contains \\(\\hat{\\beta}_1\\), so now let’s find the value of \\(\\hat{\\beta}_1\\) that satisfies the second equality in Equation A.3. The mathematical steps are below in Equation A.6.\n\\[\n\\begin{aligned}\n&-2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i - \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\[10pt]\n\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i = (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 \\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i =  \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2\\\\[10pt]\n(\\text{Given }\\bar{x} = \\sum_{i=1}^n x_i)&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i  = n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2\\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} = \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2\\\\[10pt]\n&\\Rightarrow \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} = \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)\\\\[10pt]\n&\\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} = \\hat{\\beta}_1\n\\end{aligned}\n\\tag{A.6}\\]\nWe will use the following rules to write Equation A.6 in a form that is more recognizable:\n\\[\n\\sum_{i=1}^n x_iy_i - n\\bar{y}\\bar{x} = \\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{A.7}\\]\n\\[\n\\sum_{i=1}^n x_i^2 - n\\bar{x}^2 = \\sum_{i=1}^n(x_i - \\bar{x})^2 = (n-1)s_x^2\n\\tag{A.8}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nApplying Equation A.7 and Equation A.8 to Equation A.6, we have\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\[10pt]\n&= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})^2}\\\\[10pt]\n&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\[10pt]\n&= \\frac{\\text{Cov}(x,y)}{s_x^2}\n\\end{aligned}\n\\tag{A.9}\\]\nPlugging in the formula for \\(Cov(x,y)\\) into Equation A.9, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\]\nWe have found values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that satisfy Equation A.3. Now we need to confirm that we have found the minimum value (rather than a maximum value or saddle point). To do, we use the second partial derivative. If it is positive, then we know we have found a minimum.\nThe second partial derivatives are\n\\[\n\\begin{aligned}\n&\\frac{\\partial^2 \\text{SSR}}{\\partial \\beta_0^2} = \\frac{\\partial}{\\partial \\beta_0}\\Big(-2 \\sum\\limits_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i)\\Big) = 2n &gt; 0 \\\\[10pt]\n&\\frac{\\partial^2 \\text{SSR}}{\\partial \\beta_1^2} = \\frac{\\partial}{\\partial \\beta_1}\\Big(-2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\beta_0 - \\beta_1 x_i)\\Big) = 2\\sum_{i=1}^nx_i^2 &gt; 0\n\\end{aligned}\n\\tag{A.10}\\]\nBoth partial derivatives are greater than 0, so we have shown that the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), in fact, minimize SSR.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-least-sq-math",
    "href": "appendix-linear.html#sec-least-sq-math",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "",
    "text": "The last line of Equation A.4 is derived from the fact that \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\).\n\n\n\n\n\n\n\n\n\n\n\n\nThe correlation between \\(x\\) and \\(y\\) is \\[r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\]\nTherefore, \\[\\text{Cov}(x,y) = r s_xs_y\\]\nwhere \\(s_x\\) and \\(s_y\\) are the sample standard deviations of \\(x\\) and \\(y\\), respectively.\n\n\n\n\n\n\n\n\nThe least-squares estimators for the intercept and slope are\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\\hspace{20mm} \\hat{\\beta}_1 = r\\frac{s_y}{s_x}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-mlr-matrix",
    "href": "appendix-linear.html#sec-mlr-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.2 Matrix representation of linear regression",
    "text": "A.2 Matrix representation of linear regression\nThe matrix representation for linear regression introduced in this section will be used for the remainder of this appendix and in Appendix B. We will provide some linear algebra and matrix algebra details throughout, but we assume understanding of basic linear algebra concepts. Please see Chapter 1 An Introduction to Statistical Learning and online resources for an in-depth introduction to linear algebra.\nSuppose we have a data set with \\(n\\) observations. The \\(i^{th}\\) observation is represented as \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the predictor variables and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model of the form in Equation A.11 (see Chapter 8 for more detail).\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon \\hspace{8mm} \\epsilon \\sim N(0, \\sigma^2_{\\epsilon}) \\tag{A.11}\\]\nThe regression model in Equation A.11 can be represented using vectors and matrices.\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\hspace{8mm} \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I}) \\tag{A.12}\\]\nLet’s break down the components of Equation A.12.\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n=\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_{\\boldsymbol{\\epsilon}}\n\\tag{A.13}\\]\nFrom Equation A.12 and Equation A.13, we have the following components of the linear regression model in matrix form:\n\n\n\\(\\mathbf{y}\\) is an \\(n \\times 1\\) vector of the observed responses.\n\n\\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) matrix called the design matrix. The first column is always \\(\\mathbf{1}\\), a column vector of 1’s, that corresponds to the intercept. The remaining columns contain the observed values of the predictor variables.\n\n\\(\\boldsymbol{\\beta}\\) is a \\((p+1) \\times 1\\) vector of the model coefficients.\n\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n \\times 1\\) vector of the error terms.\n\nAs before the error terms are normally distributed, centered at \\(\\mathbf{0}\\), a column vector of 0’s, with a variance \\(\\sigma^2_{\\epsilon}\\mathbf{I}\\), where \\(\\mathbf{I}\\) is the identity matrix.\n\nThe variance of the error terms \\(\\boldsymbol{\\epsilon}\\) is \\[\\sigma^2_{\\epsilon}\\mathbf{I} = \\sigma^2_{\\epsilon} \\begin{bmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 &\\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\sigma^2_{\\epsilon} & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_{\\epsilon} &\\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2_{\\epsilon} \\\\\n\\end{bmatrix}\\]\nThis is the matrix notation showing that the error terms are independent and have the same variance \\(\\sigma^2_{\\epsilon}\\).\n\nBased on Equation A.12, the equation for the vector of estimated response values, \\(\\hat{\\mathbf{y}}\\), is\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-mlr-estimation-matrix",
    "href": "appendix-linear.html#sec-mlr-estimation-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.3 Estimating the Coefficients",
    "text": "A.3 Estimating the Coefficients\n\nA.3.1 Least-squares estimation\nIn matrix notation, the error terms can be written as \\[\n\\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\n\\tag{A.14}\\]\nAs with simple linear regression in Section A.1, the least-squares estimator of the vector of coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\), is the vector that minimizes the sum of squared residuals in Equation A.15. \\[\nSSR = \\sum\\limits_{i=1}^{n} \\epsilon_{i}^2 = \\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} = (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})\n\\tag{A.15}\\]\nwhere \\(\\boldsymbol{\\epsilon}^\\mathsf{T}\\), the transpose of the vector \\(\\boldsymbol{\\epsilon}\\).\nLet’s walk through the steps to minimize Equation A.15. We start by expanding the equation.\n\\[\n\\begin{aligned}\nSSR &= (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n& = (\\mathbf{Y}^\\mathsf{T}\\mathbf{Y} - \\mathbf{Y}^\\mathsf{T} \\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{Y} +\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta})\n\\end{aligned}\n\\tag{A.16}\\]\nNote that \\((\\mathbf{Y^\\mathsf{T}}\\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T} = \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{Y}\\). These are both constants (i.e. \\(1\\times 1\\) vectors), so we have\\(\\mathbf{Y^\\mathsf{T}}\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{Y^\\mathsf{T}}\\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}=  \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{Y}\\). Plugging this equality into Equation A.16, we have\n\\[\nSSR = \\mathbf{Y}^\\mathsf{T}\\mathbf{Y} - 2 \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{Y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}\n\\tag{A.17}\\]\nNext, we find the partial derivative of Equation A.17 with respect to \\(\\boldsymbol{\\beta}\\).\n\nLet \\(\\mathbf{x} = \\begin{bmatrix}x_1 & x_2 & \\dots & x_p\\end{bmatrix}^\\mathsf{T}\\)be a \\(k \\times 1\\) vector and \\(f(\\mathbf{x})\\) be a function of \\(\\mathbf{x}\\).\nThen \\(\\nabla_\\mathbf{x}f\\), the gradient of \\(f\\) with respect to \\(\\mathbf{x}\\) is\n\\[\\nabla_\\mathbf{x}f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\dots & \\frac{\\partial f}{\\partial x_p}\\end{bmatrix}^\\mathsf{T}\n\\]\n\nProperty 1\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{z}\\) be a \\(k \\times 1\\) vector, such that \\(\\mathbf{z}\\) is not a function of \\(\\mathbf{x}\\) . The gradient of \\(\\mathbf{x}^\\mathsf{T}\\mathbf{z}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^\\mathsf{T}\\mathbf{z} = \\mathbf{z}\\]\n\nProperty 2\nLet \\(\\mathbf{x}\\) be a \\(k \\times 1\\) vector and \\(\\mathbf{A}\\) be a \\(k \\times k\\) matrix, such that \\(\\mathbf{A}\\) is not a function of \\(\\mathbf{x}\\) . The gradient of \\(\\mathbf{x}^\\mathsf{T}\\mathbf{A}\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is\n\\[\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^\\mathsf{T}\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathsf{T} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^\\mathsf{T})\\mathbf{x}\n\\]If \\(\\mathbf{A}\\) is symmetric, then \\((\\mathbf{A} + \\mathbf{A}^\\mathsf{T})\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\\)\n\nUsing the matrix calculus, the partial derivative of Equation A.17 with respect to \\(\\boldsymbol{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\boldsymbol{\\beta}} &= \\frac{\\partial}{\\partial \\boldsymbol\\beta}(\\mathbf{Y}^\\mathsf{T}\\mathbf{Y} - 2\\boldsymbol{\\beta}^\\mathsf{T} \\mathbf{X}^\\mathsf{T}\\mathbf{Y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n& = -2\\mathbf{X}^\\mathsf{T}\\mathbf{Y} + 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\tag{A.18}\\]\nNote that \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is symmetric.\nThus, the least-squares estimator is the \\(\\hat{\\boldsymbol{\\beta}}\\) that satisfies\n\\[\n-2\\mathbf{X}^\\mathsf{T}\\mathbf{Y} + 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n\\tag{A.19}\\]\nThe steps to find this \\(\\hat{\\boldsymbol{\\beta}}\\) are below. \\[\n\\begin{aligned}\n&- 2 \\mathbf{X}^\\mathsf{T}\\mathbf{Y} + 2 \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\[10pt]\n& \\Rightarrow 2 \\mathbf{X}^\\mathsf{T}\\mathbf{Y} = 2 \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]& \\Rightarrow \\mathbf{X}^\\mathsf{T}\\mathbf{Y} = \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]& \\Rightarrow (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{Y} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]& \\Rightarrow (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}\\end{aligned}\n\\tag{A.20}\\]\nSimilar to Section A.1, we check the second derivative to confirm that we have found a minimum. In matrix representation, the second derivative is the Hessian matrix.\n\nThe Hessian matrix, \\(\\nabla_\\mathbf{x}^2f\\) is a \\(k \\times k\\) matrix of partial second derivatives\n\\[\n\\nabla_{\\mathbf{x}}^2f = \\begin{bmatrix} \\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_1\\partial x_p} \\\\\n\\frac{\\partial^2f}{\\partial\\ x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2f}{\\partial x_2 \\partial x_p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2f}{\\partial x_p\\partial x_1} & \\frac{\\partial^2f}{\\partial x_p\\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_p^2} \\end{bmatrix}\n\\]\nIf the Hessian matrix is…\n\npositive definite, then we found a minimum.\nnegative definitive, then we found a maximum.\nneither, then we found a saddle point.\n\n\nThus, the Hessian of Equation A.15 is\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 SSR}{\\partial \\boldsymbol{\\beta}^2} &= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(-2\\mathbf{X}^\\mathsf{T}\\mathbf{Y} + 2\\mathbf{X}^\\mathbf{T}\\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n& = 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\n\\end{aligned}\n\\tag{A.21}\\]\nEquation A.21 is proportional to \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\), which is a positive definite matrix. Therefore, we found a minimum.\n\nThe least-squares estimator in matrix notation is\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{Y}\n\\]\n\n\nA.3.2 Geometry of least-squares regression\nIn Section A.3.1, we used matrix calculus to find \\(\\hat{\\boldsymbol{\\beta}}\\), the least-squares estimators of the model coefficients. In this section, we present the geometry of least-squares regression to derive the estimator \\(\\hat{\\boldsymbol{\\beta}}\\). Figure A.1 is a visualization of the geometric representation of least-squares regression.\n\n\n\n\n\nFigure A.1: Geometry of least-squares regression\n\n\nLet \\(\\text{Col}(\\mathbf{X})\\) be the column space of the design matrix \\(\\mathbf{X}\\), the set all possible linear combinations of the columns of \\(\\mathbf{X}\\).1 We cannot derive the values of \\(\\mathbf{y}\\) using only linear combinations of \\(\\mathbf{X}\\) (recall the error term \\(\\boldsymbol{\\epsilon}\\) in Equation A.12). Therefore, \\(\\mathbf{y}\\) is not in \\(\\text{Col}(\\mathbf{X})\\). We want to find another vector \\(\\mathbf{Xb}\\) in \\(\\text{Col}(\\mathbf{X})\\) that minimizes the distance to \\(\\mathbf{y}\\). We call the vector \\(\\mathbf{Xb}\\) a projection of \\(\\mathbf{y}\\) onto \\(\\text{Col}(\\mathbf{X})\\).\nFor any vector \\(\\mathbf{Xb}\\) in \\(\\text{Col}(\\mathbf{X})\\), the vector \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\) is the difference between \\(\\mathbf{y}\\) and the projection \\(\\mathbf{Xb}\\). We know \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) from the data, so we need to find \\(\\mathbf{b}\\) that minimizes the length of \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\). It is minimized when \\(\\mathbf{e}\\) is orthogonal (also called normal) to \\(\\text{Col}(\\mathbf{X})\\). \n\nIf \\(\\mathbf{A}\\), an \\(n \\times k\\) matrix, is orthogonal to an \\(n \\times 1\\) vector \\(\\mathbf{c}\\), then \\(\\mathbf{A}^\\mathsf{T}\\mathbf{c} = \\mathbf{0}\\)\n\nBecause \\(\\mathbf{e}\\) is orthogonal to \\(\\text{Col}(\\mathbf{X})\\), then \\(\\mathbf{X}^\\mathsf{T}\\mathbf{e} = \\mathbf{0}\\). We plug in \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\) and solve for \\(\\mathbf{b}\\).\n\\[\n\\begin{aligned}\n&\\mathbf{X}^\\mathsf{T}(\\mathbf{y} - \\mathbf{Xb}) = \\mathbf{0}\\\\[10pt]\n\\Rightarrow &\\mathbf{X}^\\mathsf{T}\\mathbf{y} - \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\mathbf{b} = 0 \\\\[10pt]\n\\Rightarrow &\\mathbf{X}^\\mathsf{T}\\mathbf{y} = \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\mathbf{b}\\\\[10pt]\n\\Rightarrow &(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y} = \\mathbf{b}\n\\end{aligned}\n\\tag{A.22}\\]\nUsing the geometric interpretation of least-squares regression, we found that the vector \\(\\mathbf{b}\\) that minimizes \\(\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\\) is \\(\\hat{\\mathbf{b}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\). This is equal to the estimator found in Equation A.20.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-hat-matrix",
    "href": "appendix-linear.html#sec-hat-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.4 Hat matrix",
    "text": "A.4 Hat matrix\nThe fitted values of least-squares regression are \\(\\mathbf{y} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\). Plugging in \\(\\hat{\\boldsymbol{\\beta}}\\) from Equation A.20, we have\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}}_{\\mathbf{H}}\\mathbf{y}\n\\tag{A.23}\\]\nFrom Equation A.23, \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\) is called the hat matrix. The hat matrix is an \\(n \\times n\\) matrix that maps \\(\\mathbf{y}\\), the vector of observed responses, onto \\(\\hat{\\mathbf{y}}\\), the vector of fitted values (\\(\\hat{\\mathbf{y}} = \\mathbf{Hy}\\)). More precisely, \\(\\mathbf{H}\\) is a projection matrix that projects \\(\\mathbf{y}\\) onto the column space \\(\\text{Col}(\\mathbf{X})\\) (see Section A.3.2). Because \\(\\mathbf{H}\\) is a projection matrix, it has the following properties:\n\n\\(\\mathbf{H}\\) is symmetric (\\(\\mathbf{H}^\\mathsf{T} = \\mathbf{H}\\)).\n\\(\\mathbf{H}\\) is idempotent (\\(\\mathbf{H}^2 = \\mathbf{H}\\)).\nIf a vector \\(\\mathbf{v}\\) is in \\(\\text{Col}(\\mathbf{X})\\), then \\(\\mathbf{Hv}  = \\mathbf{v}\\).\nIf a vector \\(\\mathbf{v}\\) is orthogonal to \\(\\text{Col}(\\mathbf{X})\\), then \\(\\mathbf{Hv}= \\mathbf{0}\\).\n\nFrom Equation A.23, the hat matrix only depends on the design matrix \\(\\mathbf{X}\\), i.e., it only depends on the values of the predictors. It does not depend on the vector of responses. The diagonal elements \\(\\mathbf{H}\\) are the values of leverage. More specifically, \\(h_{ii}\\) is the leverage for the \\(i^{th}\\) observation. Recall from Section 6.4.2 that in simple linear regression, the leverage is a measure of how far an observation’s value of the predictor is from the average value of the predictor across all observations.\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\tag{A.24}\\]\nIn multiple linear regression, the leverage is a measure of how far the \\(i^{th}\\) observation is from the center of the \\(x\\) space.  It is computed as\n\\[\nh_{ii} = \\mathbf{x}_i^\\mathsf{T}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{x}_i\n\\tag{A.25}\\]\nwhere \\(\\mathbf{x}_i^\\mathsf{T}\\) is the \\(i^{th}\\) row of the design matrix \\(\\mathbf{X}\\).2\nThe sum of the leverages for all observation \\(p+1\\) where \\(p\\) is the number of predictors in the model. Thus, \\(\\text{rank}(\\mathbf{H}) = \\sum_{i=1}^n h_{ii} = p+1\\). The average value of leverage is \\(\\bar{h} = \\frac{p+1}{n}\\). Observations with leverage greater than \\(2\\bar{h}\\) are considered to have large leverage. \nUsing \\(\\mathbf{H}\\), we can write the residuals as\n\\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{Hy} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n\\tag{A.26}\\]\nEquation A.26 shows one feature of observations with large leverage. Observations with large leverage tend to have small residuals. In other words, the model tends to pull towards observations with large leverage.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-assumptions-matrix",
    "href": "appendix-linear.html#sec-assumptions-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.5 Assumptions of linear regression",
    "text": "A.5 Assumptions of linear regression\nIn Section 5.3, we introduced four assumptions of linear regression. Given the matrix form of the linear regression on model in Equation A.12, \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) such that \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I})\\) , these assumptions are the following:\n\nThe distribution of the response \\(\\mathbf{y}\\) given \\(\\mathbf{X}\\) is normal.\nThe expected value of \\(\\mathbf{y}\\) is \\(\\mathbf{X}\\boldsymbol{\\beta}\\). There is a linear relationship between the response and predictor variables.\nThe variance \\(\\mathbf{y}\\) given \\(\\mathbf{X}\\) is \\(\\sigma^2_{\\epsilon}\\mathbf{I}\\).\nThe error terms in \\(\\boldsymbol{\\epsilon}\\) are independent. This also means the observations are independent of one another.\n\nFrom these assumptions, we write the distribution of \\(\\mathbf{y}\\) given the regression model in Equation A.27.\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon}\\mathbf{I})\n\\tag{A.27}\\]\nIn Section A.2, we showed Assumption 4 from the distribution of the error terms. Here we will show Assumptions 1 - 3.\n\nSuppose \\(\\mathbf{z}\\) is a (multivariate) normal random variable such that \\(\\mathbf{z} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\). A linear transformation of \\(\\mathbf{z}\\) is also multivariate normal, such that\n\\[\n\\mathbf{A}\\mathbf{z} + \\mathbf{B} \\sim N(\\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{B}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^\\mathsf{T})\n\\]\n\nThe distribution of the error terms \\(\\boldsymbol{\\epsilon}\\) is normal, and the vector of responses \\(\\mathbf{y}\\) is linear combination of the error terms. More specifically, \\(\\mathbf{y}\\) is computed as the error terms, shifted by \\(\\mathbf{X}\\boldsymbol{\\beta}\\). Thus, from the math property above, we know that \\(\\mathbf{y}\\) is normally distributed.\n\nExpected value of a vector\nLet \\(\\mathbf{z} = \\begin{bmatrix}z_1 & \\dots & z_p\\end{bmatrix}^\\mathsf{T}\\) be a \\(p \\times 1\\) vector of random variables. Then \\(E(\\mathbf{z}) = E\\begin{bmatrix}z_1 & \\dots & z_p\\end{bmatrix}^\\mathsf{T} = \\begin{bmatrix}E(z_1) & \\dots & E(z_p)\\end{bmatrix}^\\mathsf{T}\\)\n\nLet \\(\\mathbf{A}\\) be an \\(n \\times p\\) matrix of constants, \\(\\mathbf{C}\\) a \\(n \\times 1\\) vector of random variables, and \\(\\mathbf{z}\\) a \\(p \\times 1\\) vector of random variables. Then\n\\[\nE(\\mathbf{Az} + \\mathbf{C}) = \\mathbf{A}E(\\mathbf{z}) +E(\\mathbf{C})\n\\]\n\nNext, let’s show Assumption 2 that the expected value of \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}\\) in linear regression. We can get the expected value of \\(\\mathbf{y}\\) from the mathematical properties above used for Assumption 1. Here, we show Assumption 2 directly from the properties of expected values.\n\\[\n\\begin{aligned}\nE(\\mathbf{y}) &= E(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\[10pt]\n& = E(\\mathbf{X}\\boldsymbol{\\beta}) + E(\\boldsymbol{\\epsilon})\\\\[10pt]\n& = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{0} \\\\[10pt]\n& = \\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\tag{A.28}\\]\n\nLet \\(\\mathbf{z}\\) be a \\(p \\times 1\\) vector of random variables. Then\n\\[\nVar(\\mathbf{z}) = E[(\\mathbf{z} - E(\\mathbf{z}))(\\mathbf{z} - E(\\mathbf{z}))^\\mathsf{T}]\n\\]\n\nLastly, we show Assumption 3 that \\(Var(\\mathbf{y}) = \\sigma^2_{\\epsilon}\\mathbf{I}\\) in linear regression. Similar to the expected value, we can get the variance from the mathematical property used to show Assumption 1. Here, we show Assumption 3 directly from the properties of variance. \\[\n\\begin{aligned}\nVar(\\mathbf{y}) &= E[(\\mathbf{y} - E(\\mathbf{y}))(\\mathbf{y} - E(\\mathbf{y}))^\\mathsf{T}] \\\\[10pt]\n& = E[(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} - \\mathbf{X}\\boldsymbol{\\beta})(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}] \\\\[10pt]\n& = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^\\mathsf{T}] \\\\[10pt]\n(\\text{given }E(\\boldsymbol{\\epsilon} = \\mathbf{0})) & = E[(\\boldsymbol{\\epsilon} - E(\\boldsymbol{\\epsilon}))(\\boldsymbol{\\epsilon} - E(\\boldsymbol{\\epsilon}))^\\mathsf{T}]\\\\[10pt]\n& = Var(\\boldsymbol{\\epsilon}) \\\\[10pt]\n& = \\sigma^2_{\\epsilon}\\mathbf{I}\n\\end{aligned}\n\\tag{A.29}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-beta-dist-matrix",
    "href": "appendix-linear.html#sec-beta-dist-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.6 Distribution of model coefficients",
    "text": "A.6 Distribution of model coefficients\nIn Section 8.4.1, we introduced the distribution of a model coefficient \\(\\hat{\\beta}_j \\sim N(\\beta_j, SE_{\\hat{\\beta}_j}^2)\\).  In matrix notation, the distribution of all the estimated coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) is\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N\\Big(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon}(\\mathbf{X}^\\mathsf{T},\\mathbf{X})^{-1}\\Big)\n\\tag{A.30}\\]\nSimilar to Section A.5, let’s derive each part of this distribution. We’ll start with \\(E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\\).\n\\[\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}) \\\\[10pt]\n& = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}E(\\mathbf{y})\\\\[10pt]\n& = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}E(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})\n\\\\[10pt]\n&=(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}E(\\mathbf{X}\\boldsymbol{\\beta}) + (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}E(\\boldsymbol{\\epsilon}) \\\\[10pt]\n& = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{0}\\\\[10pt]\n&= \\boldsymbol{\\beta}\n\\end{aligned}\n\\tag{A.31}\\]\nNext, we show that \\(Var(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2_{\\epsilon}\\mathbf{I}\\).\n\\[\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= E[(\\hat{\\boldsymbol{\\beta}} - E(\\hat{\\boldsymbol{\\beta}}))(\\hat{\\boldsymbol{\\beta}} - E(\\hat{\\boldsymbol{\\beta}}))^\\mathsf{T}] \\\\[10pt]\n& = E[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y} - \\boldsymbol{\\beta})((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y} - \\boldsymbol{\\beta})^\\mathsf{T}] \\\\[10pt]\n& = E[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) - \\boldsymbol{\\beta})((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) - \\boldsymbol{\\beta})^\\mathsf{T}] \\\\[10pt]\n&\\dots \\text{After distributing and simplifying}\\dots \\\\[10pt]\n& = E[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^\\mathsf{T}\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}] \\\\[10pt]\n& = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\underbrace{E(\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^\\mathsf{T})}_{\\sigma^2_{\\epsilon}\\mathbf{I}}\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\\\[10pt]\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\\\[10pt]\n& = \\sigma^2_{\\epsilon}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\n\\end{aligned}\n\\tag{A.32}\\]\nLastly, we show that the distribution of \\(\\boldsymbol{\\beta}\\) is normal. We’ll start by rewriting \\(\\hat{\\boldsymbol{\\beta}}\\) in terms of \\(\\boldsymbol{\\beta}\\).\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y} \\\\[10pt]\n& = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n& = \\boldsymbol{\\beta} + (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{\\epsilon}\n\\end{aligned}\n\\tag{A.33}\\]\nFrom Equation A.33, we see that \\(\\hat{\\boldsymbol{\\beta}}\\) is a linear combination of the error terms \\(\\boldsymbol{\\epsilon}\\). From the math property in Section A.5, because \\(\\boldsymbol{\\epsilon}\\) is normally distributed, the linear combination is also normally distributed. Thus, the distribution of the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) is normal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-multicollinearity-matrix",
    "href": "appendix-linear.html#sec-multicollinearity-matrix",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.7 Multicollinearity",
    "text": "A.7 Multicollinearity\nRecall the design matrix for a linear regression model with \\(p\\) predictors in Equation A.13\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 &x_{11} & \\dots & x_{1p}\\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n1 &  x_{n1} & \\dots &x_{np}\n\\end{bmatrix}\n\\tag{A.34}\\]\nThe design matrix \\(\\mathbf{X}\\) has full column rank \\(\\text{rank}(\\mathbf{X}) = (p + 1)\\) in linear regression. This means there are no linear dependencies among the columns, and thus no column is a perfect linear combination of the others.  The equation for the least-squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\) in Equation A.20 includes the term \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\). If \\(\\mathbf{X}\\) is not full rank, then \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is not full rank, and is therefore singular (not invertible). Thus, if there are linear dependencies in \\(\\mathbf{X}\\), we are unable to compute an the least-squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\).\n\nLet \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p\\) be a sequence of vectors. Then, \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p\\) are linearly dependent, if there exists scalars \\(a_1, a_2, \\ldots, a_p\\) such that\n\\[\na_1\\mathbf{x}_1 + a_2\\mathbf{x}_2 + \\dots + a_p\\mathbf{x}_p = \\mathbf{0}\n\\]\nwhere \\(a_1, a_2, \\ldots, a_p\\) are not all 0.\n\nIn practice, we rarely have perfect linear dependencies. In fact, this is mathematically why we only include \\(k-1\\) terms in the model for a categorical predictor with \\(k\\) levels. Ideally the columns of \\(\\mathbf{X}\\) would be orthogonal, indicating the predictors are completely independent on one another. In practice, we expect there to be some dependence between predictors (we see this from the non-zero off diagonals in \\(Var(\\hat{\\boldsymbol{\\beta}})\\)). If two or more variables are highly correlated, then there will be near linear dependence in the columns. This near-linear dependence is called multicollinearity.\nIn Section 8.6 we discussed the practical issues that come from the presence of multicollinearity. These primarily stem from the fact that when there is multicollinearity, \\(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is near-singular (almost a singular matrix) to a, thus making \\((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) very large and unstable. Consequently, \\(Var(\\hat{\\boldsymbol{\\beta}})\\) is then large and unstable, thus making hard to find stable values for the least-squares estimators.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-appendix-linear-mle",
    "href": "appendix-linear.html#sec-appendix-linear-mle",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.8 Maximum Likelihood Estimation",
    "text": "A.8 Maximum Likelihood Estimation\nIn Section 10.2.3, we introduced the likelihood function to understand the model performance statistics AIC and BIC. We also used a likelihood function to estimate the coefficients in logistic regression (more on this in Appendix B). The likelihood function is a measure of how likely it is we observe our data given particular value(s) of model parameters. When working with likelihood functions, we have fixed data (our observed sample data) and we can try out different values for the model parameters (\\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2_{\\epsilon}\\) in the context of regression).3\n\nLet \\(\\mathbf{z}\\) be a \\(p \\times 1\\) vector of random variables, such that \\(\\mathbf{z}\\) follows a multivariate normal distribution with mean \\(\\boldsymbol{\\mu}\\) and variance \\(\\boldsymbol{\\Sigma}\\). Then the probability density function of \\(\\mathbf{z}\\) is\n\\[f(\\mathbf{z}) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\Big\\{-\\frac{1}{2}(\\mathbf{z} - \\boldsymbol{\\mu})^\\mathsf{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}- \\boldsymbol{\\mu})\\Big\\}\\]\n\nIn Section A.5, we showed that the vector of responses \\(\\mathbf{y}\\) can be written as \\(\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon}\\mathbf{I})\\). Using this distribution to describe the relationship between the response and predictor variables, the likelihood function for the regression parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2_{\\epsilon}\\) is\n\\[\nL(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{(2\\pi)^{n/2}\\sigma^n_{\\epsilon}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\Big\\}\n\\tag{A.35}\\]\nwhere \\(n\\) is the number of observations, the mean is \\(\\mathbf{X}\\boldsymbol{\\beta}\\) and the variance is \\(\\sigma^2_{\\epsilon}\\mathbf{I}\\).\nThe data \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) are fixed (we use the same sample data in the analysis!), so we can plug in values for \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2_{\\epsilon}\\) to determine the likelihood of obtaining those values for the parameters given the observed data. In Section A.3.1, we used least-squares estimation (minimizing \\(SSR\\) ) find the estimated coefficients \\(\\hat{\\boldsymbol{\\beta}}\\). Another approach to estimate \\(\\boldsymbol{\\beta}\\) (and \\(\\sigma^2_{\\epsilon}\\)) is to find \\(\\hat{\\boldsymbol{\\beta}}\\) (and \\(\\hat{\\sigma}^2_{\\epsilon}\\)) that maximizes the likelihood function in Equation A.35. This is called maximum likelihood estimation.\nTo make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function is\n\\[\n\\begin{aligned}\\log L(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon &| \\mathbf{X}, \\mathbf{y}) \\\\ & = -\\frac{n}{2}\\log(2\\pi) - n \\log(\\sigma_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\end{aligned}\n\\tag{A.36}\\]\nGiven a fixed value of \\(\\sigma^2_{\\epsilon}\\), the log-likelihood in Equation A.36 is maximized when \\((\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\) is minimized. This is equivalent to minimizing the \\(SSR\\) in Equation A.15. Thus, the least-squares estimator of \\(\\boldsymbol{\\beta}\\) is also the maximum likelihood estimator ( \\(\\hat{\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\)) when the error terms are defined as in Equation A.12.\nWe previously found \\(\\hat{\\boldsymbol{\\beta}}\\) using least-squares estimation and the geometry of regression in Section A.3.1, so why does it matter that \\(\\hat{\\boldsymbol{\\beta}}\\) is also the maximum likelihood estimator? First, maximum likelihood estimation is used to find the coefficients for generalized linear models and many other statistical models that go beyond linear regression (we’ll see how its used for logistic regression in Appendix B). In fact, Casella and Berger (2024) stated maximum likelihood estimation is “by far, the most popular technique for deriving estimators.” [pp. 315].  Second, maximum likelihood estimators have nice statistical properties. These properties are beyond the scope of this text, but we refer interested readers to Chapter 7 of Casella and Berger (2024) for an in-depth discussion of maximum likelihood estimators and their properties.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#sec-var-transformations-math",
    "href": "appendix-linear.html#sec-var-transformations-math",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "\nA.9 Variable transformations",
    "text": "A.9 Variable transformations\nIn Chapter 9, we introduced regression models with transformations on the response and/or predictor variables. Here we will share some of the mathematical details behind the interpretation of the coefficients in these models.\n\nA.9.1 Transformation on the response variable\nIn Section 9.2, we introduced a linear regression model with a log transformation on the response variable\n\\[\n\\log(Y) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p + \\epsilon\n\\tag{A.37}\\]\n\nIn this text, \\(\\log(a)\\) is the natural log of \\(a\\) (also denoted as \\(\\ln(a)\\)).\n\n\\(\\log(a) + \\log(b) = \\log(ab)\\).\n\\(\\log(a) - \\log(b) = \\log(\\frac{a}{b})\\)\n\\(e^{\\log(a)} = a\\)\n\\(e^{b\\log(a)} = a^b\\)\n\n\nFrom Chapter 7, we have that the change in \\(\\log(Y)\\) when \\(X_j\\) increase by 1 unit is \\(\\beta_j\\). Thus \\(\\beta_j = \\log(Y|{X_j+1}) - \\log(Y|X_j)\\), where \\(Y|X_{j}+1\\) is the value of \\(Y\\) when we plug in \\(X_j+1\\) and \\(Y|{X_j}\\) is the value of \\(Y\\) when we plug in \\(X_j\\). In practice, we interpret the model coefficients \\(\\beta_j\\) in terms of the original variable \\(Y\\), so we can use the rules of logarithms and exponents to rewrite this in terms of the original units of the response variable.\n\\[\n\\begin{aligned}\n\\beta_j &= \\log(Y|X_j+1) - \\log(Y|X_j) \\\\[10pt]\n& = \\log\\Big(\\frac{Y|X_j+1}{Y|X_j}\\Big) \\\\[10pt]\n\\Rightarrow \\beta_j \\times Y|X_j &= Y|X_j+1\n\\end{aligned}\n\\]\nThus given the model in Equation A.37, when \\(X_j\\) increases by 1 unit, we expect \\(Y\\) to multiply by \\(\\beta_j\\), assuming all other predictors are held constant.\n\nA.9.2 Transformation on predictor variable(s)\nNext, we consider the models introduced in Section 9.3 that have a log transformation on a predictor variable.\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_j\\log(X_j) + \\dots + \\beta_pX_p + \\epsilon\n\\tag{A.38}\\]\nNow, we because the predictor \\(X_j\\) has been transformed to the logarithmic scale, we write interpretations in terms of a multiplicative change in \\(X_j\\). More specifically, given a constant \\(C\\), \\(\\log(CX_j) = \\log(C) + \\log(X_j)\\). Let \\(Y|CX_j\\) be the expected value of \\(Y\\) when we plug in \\(CX_j\\) and \\(Y|X_j\\) be the expected value of \\(Y\\) when we plug in \\(X_j\\). Assuming all other predictors held constant, we have\n\\[\n\\begin{aligned}\nY|CX_j - Y|CX &= \\beta_j\\log(CX_j) - \\beta_j\\log(X_j) \\\\\n& = \\beta_j[\\log(CX_j) - \\log(X_j) \\\\\n& = \\beta_j[\\log(C) + \\log(X_j) - \\log(X_j)] \\\\\n&= \\beta_j\\log(C) \\\\[10pt]\n\\Rightarrow Y|CX_j &=Y|X_j + \\beta_j\\log(C)\n\\end{aligned}\n\\]\nThus, given the model in Equation A.38, when predictor \\(X_j\\) is multiplied by a constant \\(C\\), we expect \\(Y\\) to change (increase or decrease) by \\(\\beta_j\\log(C)\\), holding all other predictors constant.\n\nA.9.3 Transformation on response and predictor variables\nLastly, we show the mathematics behind the interpretation of a model coefficient \\(\\beta_j\\) for the linear regression models introduced in Section 9.4 with a log transformation on the response variable and a predictor variable. As in Section A.9.1, we want to write the interpretation in terms of the original response variable \\(Y\\).\n\\[\n\\log(Y) = \\beta_0  + \\beta_1X_1 + \\dots + \\beta_j\\log(X_j) + \\dots + \\beta_pX_p\n\\tag{A.39}\\]\nCombining the results from Section A.9.1 and Section A.9.2 and holding all other predictors constant, we have\n\\[\n\\begin{aligned}\n\\log(Y|CX_j) -\\log(Y|X_j) &= \\beta_j\\log(C)\\\\[10pt]\n\\log\\Big(\\frac{Y|CX_j}{Y|X_j}\\Big)  &= \\beta_j\\log(C) \\\\[10pt]\n\\frac{Y|CX_j}{Y|X_j}  &= C^{\\beta_j} \\\\[10pt]\n\\Rightarrow Y|CX_j &= Y|X_jC^{\\beta_j}\n\\end{aligned}\n\\tag{A.40}\\]\nTherefore, given the model in Equation A.39 with a transformed response variable and transformed predictor, when \\(X_j\\) is multiplied by a constant \\(C\\), why is expected to multiply by \\(C^{\\beta_j}\\), holding all other predictors constant.\n\n\n\n\nCasella, George, and Roger Berger. 2024. Statistical Inference. 2nd ed. CRC Press.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-linear.html#footnotes",
    "href": "appendix-linear.html#footnotes",
    "title": "Appendix A — Mathematics of linear regression",
    "section": "",
    "text": "This is the span of \\(\\mathbf{X}\\).↩︎\nNote that when there is a single predictor, Equation A.24 and Equation A.25 produce the same result.↩︎\nNote that the likelihood function is not the same as a probability function. In a probability function, we fix the model parameters and plug in different values for the data.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics of linear regression</span>"
    ]
  },
  {
    "objectID": "appendix-logistic.html",
    "href": "appendix-logistic.html",
    "title": "Appendix B — Mathematics of logistic regression",
    "section": "",
    "text": "B.1 Matrix representation of logistic regression\nIn Chapter 11, we introduced logistic regression models for binary response variables. Here we will show some of the mathematics underlying these models, making using of the matrix notation for regression introduced in Appendix A.\nGiven a binary response variable \\(Y\\) and predictors \\(X_1, X_2, \\ldots, X_p\\), the logistic regression model is\n\\[\n\\text{logit}(\\pi) = \\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\tag{B.1}\\]\nwhere \\(\\pi = Pr(Y = 1)\\).\nSimilar to linear regression, we can write a matrix representation of Equation B.1.\n\\[\n\\text{logit}(\\boldsymbol{\\pi}) = \\log\\Big(\\frac{\\boldsymbol{\\pi}}{1 - \\boldsymbol{\\pi}}\\Big) = \\mathbf{X}\\boldsymbol{\\beta}\n\\tag{B.2}\\]\nWe have the following components in Equation B.2:\nThough not directly in Equation B.1 or Equation B.2, the underlying data also includes \\(\\mathbf{y}\\), an \\(n\\times 1\\) vector of the binary response variables.\nWe are often interested in the probabilities computed from the logistic regression model. The probabilities computed from Equation B.2 are\n\\[\\boldsymbol{\\pi} = \\frac{e^{\\mathbf{X}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{X}\\boldsymbol{\\beta}}} \\tag{B.3}\\] See Section 11.2 for more detail about the relationship between the logit, odds, and probability.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Mathematics of logistic regression</span>"
    ]
  },
  {
    "objectID": "appendix-logistic.html#matrix-representation-of-logistic-regression",
    "href": "appendix-logistic.html#matrix-representation-of-logistic-regression",
    "title": "Appendix B — Mathematics of logistic regression",
    "section": "",
    "text": "\\(\\boldsymbol{\\pi}\\) is the \\(n \\times 1\\) vector of probabilities, such that \\(\\boldsymbol{\\pi}_i = Pr(y_i = 1)\\)\n\n\n\\(\\mathbf{X}\\) is the \\(n \\times (p + 1)\\) design matrix. Similar to linear regression, the first column is \\(\\mathbf{1}\\), a column of 1’s corresponding to the intercept.\n\n\\(\\boldsymbol{\\beta}\\) is a \\((p+1) \\times 1\\) vector of model coefficients.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Mathematics of logistic regression</span>"
    ]
  },
  {
    "objectID": "appendix-logistic.html#sec-logistic-estimation-matrix",
    "href": "appendix-logistic.html#sec-logistic-estimation-matrix",
    "title": "Appendix B — Mathematics of logistic regression",
    "section": "\nB.2 Estimation",
    "text": "B.2 Estimation\nWe want to find estimates \\(\\hat{\\boldsymbol{\\beta}}\\) that are the best fit for the data based on the model in Equation B.2. In Section 11.4, we outlined how we use maximum likelihood estimation to find \\(\\hat{\\boldsymbol{\\beta}}\\). Here we will show more of the mathematical details behind the model estimation.\n\nLet \\(Z\\) be a random variable that takes values 0 or 1. Then \\(Z\\) follows a Bernoulli distribution such that\n\\[\nPr(Z = z) = p^{z}(1 - p)^{1-z}\n\\]\nwhere \\(p = Pr(z = 1)\\).\n\n\\(E(Z) = p\\) and \\(Var(Z) = p(1-p)\\).\n\nThe response variable follows a Bernoulli distribution, such that \\(P(Y = y_i) = \\pi_{i}^{y_i}(1 - \\pi_i)^{1-\\pi_i}\\). Let \\(\\mathbf{x}_i^\\mathsf{T}\\) be the \\(i^{th}\\) row of the design matrix \\(\\mathbf{X}\\). Then, using Equation B.3, we have\n\\[\nP(Y=y_i) = \\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{y_i}\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{1 - y_i}\n\\tag{B.4}\\] that the likelihood function is a measure of how likely we observe the data given particular values of the model parameters \\(\\hat{\\boldsymbol{\\beta}}\\).\n\nLet \\(Z_1, Z_2, \\ldots, Z_n\\) be independent Bernoulli random variables. The joint distribution of \\(Z_1, Z_2, \\ldots, Z_n\\) (the probability of observing these values) is\n\\[\nf(Z_1, Z_2, \\ldots, Z_n) = \\prod_{i=1}^n p_i^{z_i}(1-p_i)^{1-z_i}\n\\]\nwhere \\(p_i = Pr(Z_i = 1)\\) .\n\nUsing Equation B.4, the likelihood function for logistic regression is\n\\[\nL(\\boldsymbol{\\beta}|\\mathbf{X},\\mathbf{y}) = \\prod_{i=1}^n\\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{y_i}\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)^{1 - y_i}\n\\tag{B.5}\\]\nTo make the math more manageable, we will maximize the log likelihood shown in Equation B.6. Maximizing Equation B.6 is equivalent to maximizing Equation B.5.\n\\[\n\\begin{aligned}\n\\log L(\\boldsymbol{\\beta}|\\mathbf{X},\\mathbf{y}) &= \\sum_{i=1}^n y_i \\log\\Big(\\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big) + \\sum_{i=1}^n(1-y_i)\\log\\Big(1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Big)\\\\[10pt]\n& \\Bigg[\\text{Given } 1 - \\frac{e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}} = \\frac{1}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\\Bigg] \\\\[10pt]\n& =  \\sum_{i=1}^n y_i \\log(e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) - \\sum_{i=1}^ny_i \\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) + \\sum_{i=1}^ny_i \\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}})  \\\\[10pt]\n& = \\sum_{i=1}^ny_i\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta} - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}})\n\\end{aligned}\n\\tag{B.6}\\]\nWe take the first derivative of Equation B.6 with respect to \\(\\boldsymbol{\\beta}\\). An outline of the steps is shown below. The maximum likelihood estimator is the vector of coefficients such that is the solution to \\(\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}} = 0\\) .\n\\[\n\\begin{aligned}\n\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}} &= \\sum_{i=1}^ny_ix_i^\\mathsf{T}\\boldsymbol{\\beta} - \\sum_{i=1}^n\\log(1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}) \\\\[10pt]\n&= \\sum_{i=1}^ny_i\\mathbf{x}_i^\\mathsf{T} - \\frac{e^{\\mathbf{x}_i^\\mathbf{T}\\boldsymbol{\\beta}} \\mathbf{x}_i^\\mathsf{T}}{1 + e^{\\mathbf{x}_i^\\mathsf{T}\\boldsymbol{\\beta}}}\n\\end{aligned}\n\\tag{B.7}\\]\nThere is no closed-from solution for this, i.e., there is no neat formula for \\(\\hat{\\boldsymbol{\\beta}}\\) as we found in Section A.3.1 for linear regression. Therefore, numerical approximation methods such are used to find the maximum likelihood estimators \\(\\hat{\\boldsymbol{\\beta}}\\). One popular method is Newton-Raphson, a “root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function” (Wikipedia contributors 2025). Numerical approximation methods such as Newton Raphson systematically search the space of of possible values of \\(\\hat{\\boldsymbol{\\beta}}\\) until it converges on the solution (the “root”) to Equation B.7.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Mathematics of logistic regression</span>"
    ]
  },
  {
    "objectID": "appendix-logistic.html#sec-logistic-inference-matrix",
    "href": "appendix-logistic.html#sec-logistic-inference-matrix",
    "title": "Appendix B — Mathematics of logistic regression",
    "section": "\nB.3 Inference for logistic regression",
    "text": "B.3 Inference for logistic regression\nIn Section 11.5, we introduced inference for a single coefficient \\(\\beta_j\\) in the logistic regression model. Because there is no closed-form solution for the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\beta}}\\) found in Section B.2, there is no closed form solution for the mean and variance for the distribution of \\(\\hat{\\boldsymbol{\\beta}}\\). We rely on theoretical results to know the distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) as \\(n\\) gets large (called asymptotic results).\nGiven \\(n\\) is large,\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, (\\mathbf{X}^\\mathsf{T}\\mathbf{V}\\mathbf{X})^{-1})\n\\tag{B.8}\\]\nwhere \\(\\mathbf{V}\\) is an \\(n \\times n\\) diagonal matrix, such that \\(V_{ii}\\) is the estimated variance for the \\(i^{th}\\) observation.1\nThe standard error used for hypothesis testing and confidence intervals for a single coefficient \\(\\beta_j\\) , is computed as the \\(j^{th}\\) diagonal element of \\(Var(\\hat{\\boldsymbol{\\beta}})^{1/2} = (\\mathbf{X}^\\mathsf{T}\\mathbf{V}\\mathbf{X})^{-1/2}\\). This is why the hypothesis tests and confidence intervals in Section 11.5.2 are only reliable for large \\(n\\), because they depend on asymptotic approximations in Equation B.8. We can use simulation-based methods if the data has a small sample size.\n\n\n\n\nWikipedia contributors. 2025. “Newton’s Method.” https://en.wikipedia.org/wiki/Newton%27s_method.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Mathematics of logistic regression</span>"
    ]
  },
  {
    "objectID": "appendix-logistic.html#footnotes",
    "href": "appendix-logistic.html#footnotes",
    "title": "Appendix B — Mathematics of logistic regression",
    "section": "",
    "text": "Recall that the variance of the Bernoulli distribution depends on \\(\\pi\\), so each observation has a different variance. This is in contrast to linear regression, where all observations have the same variance \\(\\sigma^2_{\\epsilon}\\).↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Mathematics of logistic regression</span>"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Appendix C — Data sets",
    "section": "",
    "text": "The data sets used in this book are listed below in alphabetical order.\n\ncoffee-ratings.csv: Aroma and flavor grades for 1338 coffees rated in 2018. These data are originally from the Coffee Quality Database and was featured as part of the TidyTuesday data visualization challenge in July 2020.\n\nThis data set is analyzed in Chapter 6.\n\nfivethirtyeight-voters-data.csv: Voting frequency, political party affiliation, and demographic information for 5836 adults in the United States. The data were collected through an online poll conducted by Ipsos and was analyzed in the FiveThirtyEight article “Why Many Americans Don’t Vote” (Thomson-DeVeaux, Mithani, and Bronner 2020). The final sample only includes adults who were eligible to vote for at least four election cycles at time the data were collected in 2020.\n\nThis data set is analyzed Section 13.1 and Section 13.3.\n\ngss24-ai.csv: Demographic information, political leanings, and comfort with driverless vehicles for 1521 adults in the United States. The data were collected in the 2024 General Social Survey administered by National Science Foundation and is administered by National Opinion Research Center (NORC) at the University of Chicago. The data were collected through a combination of online surveys and in-person interviews.\n\nThis data set is analyzed in Chapter 11 and Chapter 12.\n\nlemurs-repeated-measures.csv: Weight, age, and other characteristics for 248 lemurs living at the Duke Lemur Center at the time the data were collected. The data were originally analyzed in Zehr et al. (n.d.) and featured as part of the TidyTuesday data visualization challenge in August 2021. The data set includes the lemurs’ measurements from ages 1 to 24, so there can be multiple measurements for an individual lemur. There are 3715 total observations in the data.\n\nThis data set is analyzed in Section 13.2.\n\nlemurs-sample-young.csv: Weight, age, and other characteristics for 252 lemurs age 24 months or younger living at the Duke Lemur Center at the time the data were collected. The data were originally analyzed in Zehr et al. (n.d.) and featured as part of the TidyTuesday data visualization challenge in August 2021. There is one observation for each lemur.\n\nThis data set is analyzed in Chapter 7 and Chapter 8.\n\nlife-expectancy-data.csv: Information about life expectancy, healthcare, and other societal factors for 140 countries. The data set was obtained from Zarulli et al. (2021) and includes data from the Human Development Database and the World Health Organization.\n\nThis data set is analyzed in Chapter 1.\n\nmovie-scores.csv: Critics and audience scores on Rotten Tomatoes for 146 movies released 2014 - 2015. This data set is adapted from the fandango data frame in the fivethirtyeight R package (Kim, Ismay, and Chunn 2018).\n\nThis data set is analyzed in Chapter 4.\n\nncaa-basketball-DI-2023-2024.csv: Total expenditure on basketball programs and other features of 355 Division I NCAA colleges and universities in the 2023 - 2024 academic year. The data were collected from the Equity in Athletics Data Analysis (EADA) tool from the Office of Postsecondary Education in the United States Department of Education (ope.ed.gov/athletics).\n\nThis data set is analyzed in Chapter 9 and Section 13.3.\n\nparks.csv: Total expenditure per resident and number of playgrounds per 10,000 residents in 97 of the most populated cities in the United States in 2020. These data were originally analyzed in the 2021 report Parks and an Equitable Recovery (The Trust for Public Land 2021) from the Trust for Public Land. It was featured as part of the TidyTuesday data visualization challenge in June 2021.\n\nThis data set is analyzed in Chapter 5.\n\npenguins: Measurements and other features of 344 penguins at Palmer Station in Antarctica. The data were collected by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program. It is available in the palmerpenguins R package (Horst, Hill, and Gorman 2020).\n\nThis data set is analyzed in Chapter 2.\n\nproject-ace-data.csv: Demographic information, Project ACE (Action for Equity) participation, and educational outcomes for 1300 high school students in the United States. The data were obtained from Evans, Perez, and Morera (2025).\n\nThis data set is analyzed in Section 13.4.\n\nrecipes.csv: Author, cook time, serving size, and other features of 2218 recipes published on Allrecipes.com between 2009 and 2025. The data is modified from the cuisines data frame in the tastyR R package (Mubia 2025). The data were originally scraped from Allrecipes.com by Brian Mubia.\n\nThis data set is analyzed in Chapter 3.\n\nspotify-songs-sample.csv: Music features of 3000 songs on Spotify, a music streaming platform. The data are a subset of data originally analyzed in Pavlik (2019). It was featured as part of the TidyTuesday data visualization challenge (Community 2024) in January 2020.\n\nThis data set is analyzed in Chapter 10.\n\n\n\n\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly Social Data Project.” https://tidytues.day.\n\n\nEvans, Nicholas D, Perla C Perez, and Osvaldo F Morera. 2025. “Testing the Efficacy of Educational Interventions on Matched Student Samples: A Primer for Propensity Score Matching in r.” Journal of STEM Outreach 8 (1): 1–9.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.” https://doi.org/10.5281/zenodo.3960218.\n\n\nKim, Albert Y., Chester Ismay, and Jennifer Chunn. 2018. “The Fivethirtyeight r Package: ’Tame Data’ Principles for Introductory Statistics and Data Science Courses” 11. https://escholarship.org/uc/item/0rx1231m.\n\n\nMubia, Brian. 2025. “tastyR: Recipe Data from ’Allrecipes.com’.” https://doi.org/10.32614/CRAN.package.tastyR.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using Spotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nThe Trust for Public Land. 2021. “Parks and an Equitable Recovery.” Online.\n\n\nThomson-DeVeaux, Amelia, Jasmine Mithani, and Laura Bronner. 2020. “Why Many Americans Don’t Vote.” FiveThirtyEight. https://web.archive.org/web/20201102013301/https://projects.fivethirtyeight.com/non-voters-poll-2020-election/.\n\n\nZarulli, Virginia, Elizaveta Sopina, Veronica Toffolutti, and Adam Lenart. 2021. “Health Care System Efficiency and Life Expectancy: A 140-Country Study.” PLoS One 16 (7): e0253450.\n\n\nZehr, SM, RG Roach, D Haring, J Taylor, FH Cameron, and AD Yoder. n.d. “Life History Profiles for 27 Strepsirrhine Primate Taxa Generated Using Captive Data from the Duke Lemur Center. Sci Data. 2014; 1: 140019.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Akaike, H. 1974. “A New Look at the Statistical Model\nIdentification.” IEEE Transactions on Automatic Control\n19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nAlby, Tom. 2023. Data Science in Practice. Chapman; Hall/CRC.\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications\nin r. Chapman; Hall/CRC.\n\n\nAllrecipes. 2025. “About Us.” https://www.allrecipes.com/about-us-6648102.\n\n\nBates, Douglas, Martin Maechler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using\nLme4” 67. https://doi.org/10.18637/jss.v067.i01.\n\n\nBaumer, Benjamin S, Daniel T Kaplan, and Nicholas J Horton. 2017.\nModern Data Science with r. Chapman; Hall/CRC.\n\n\nBolker, Ben, and David Robinson. 2024. “Broom.mixed: Tidying\nMethods for Mixed Models.” https://doi.org/10.32614/CRAN.package.broom.mixed.\n\n\nBox, George EP, and David R Cox. 1964. “An Analysis of\nTransformations.” Journal of the Royal Statistical Society\nSeries B: Statistical Methodology 26 (2): 211–43.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with\nComments and a Rejoinder by the Author).” Statistical\nScience 16 (3): 199–231.\n\n\nBryan, Jenny. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nBurnham, Kenneth P, and David R Anderson. 2002. Model Selection and\nMultimodel Inference: A Practical Information-Theoretic Approach.\nSpringer.\n\n\nCampbell, Elizabeth, Elizaveta Berezina, and Gill Hew. 2020. “The\nEffects of Music Induction on Mood and Affect in an Asian\nContext.” Psychology of Music 49 (5): 1132–44. https://doi.org/10.1177/0305735620928578.\n\n\nCannon, Ann, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock,\nThomas Moore, Allan Rossman, and Jeffrey Witmer. 2019. “Stat2Data:\nDatasets for Stat2.” https://doi.org/10.32614/CRAN.package.Stat2Data.\n\n\nCasella, George, and Roger Berger. 2024. Statistical Inference.\n2nd ed. CRC Press.\n\n\nÇetinkaya-Rundel, Mine, and Johanna Hardin. 2024. Introduction to\nModern Statistics. 2nd ed. OpenIntro.\n\n\nÇetinkaya-Rundel, Mine, Johanna Hardin, Benjamin S Baumer, Amelia\nMcNamara, Nicholas J Horton, and Colin Rundel. 2021. “An\nEducator’s Perspective of the Tidyverse.” arXiv Preprint\narXiv:2108.03510.\n\n\nChacon, Scott, and Ben Straub. 2014. Pro Git. Apress.\n\n\nCommunity, Data Science Learning. 2024. “Tidy Tuesday: A Weekly\nSocial Data Project.” https://tidytues.day.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021a.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\n———. 2021b. “Infer: An\nr Package for Tidyverse-Friendly Statistical\nInference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,\nand Dan Tenenbaum. 2024. “Remotes: R Package Installation from\nRemote Repositories, Including ’GitHub’.” https://doi.org/10.32614/CRAN.package.remotes.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale\nuniversity press.\n\n\nDavern, Michael, Rene Bautista, Jeremy Freese, Pamela Herd, and Stephen\nL. Morgan. 2025. “General Social Survey 1972–2024.” NORC at\nthe University of Chicago; [Machine-readable data file]. https://gss.norc.org/content/dam/gss/get-documentation/pdf/codebook/GSS%202024%20Codebook.pdf.\n\n\nDuke Lemur Center. n.d. “Meet the Lemurs.” https://lemur.duke.edu/discover/meet-the-lemurs/.\n\n\nEvans, Nicholas D, Perla C Perez, and Osvaldo F Morera. 2025.\n“Testing the Efficacy of Educational Interventions on Matched\nStudent Samples: A Primer for Propensity Score Matching in r.”\nJournal of STEM Outreach 8 (1): 1–9.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2025. “Rsample: General Resampling\nInfrastructure.” https://doi.org/10.32614/CRAN.package.rsample.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2024.\nviridis(Lite) - Colorblind-Friendly\nColor Maps for r. https://doi.org/10.5281/zenodo.4679423.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge university\npress.\n\n\nHarrell Jr, Frank E. 2025. “Hmisc: Harrell Miscellaneous.”\nhttps://doi.org/10.32614/CRAN.package.Hmisc.\n\n\nHealy, Kieran. 2023. “Gssr: General Social Survey Data for Use in\nr.” http://kjhealy.github.io/gssr.\n\n\nHickey, Walt. 2015. “Be Suspicious of Online Movie Ratings,\nEspecially Fandango’s.” FiveThirtyEight, Available at:\nHttp://Fivethirtyeight. Com/Features/Fandango-Movies-Ratings.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing\nfor Parametric Causal Inference” 42. https://doi.org/10.18637/jss.v042.i08.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman.\n2020a. “Palmerpenguins: Palmer Archipelago (Antarctica) Penguin\nData.” https://doi.org/10.5281/zenodo.3960218.\n\n\n———. 2020b. “Palmerpenguins: Palmer Archipelago (Antarctica)\nPenguin Data.” https://doi.org/10.5281/zenodo.3960218.\n\n\nHowell, Joel, Dianne Singer, Erica Solway, Nicholas Box, Scott Roberts,\nLauren Hutchens, Emily Smith, and Jeffrey Kullgren. 2024. “The\nSound of Music.” University of Michigan National\nPoll on Healthy Aging. https://doi.org/10.7302/22174.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. Chapman; Hall/CRC.\n\n\nIsmay, Chester, and Albert Y Kim. 2019. Statistical Inference via\nData Science: A Moderndive into r and the Tidyverse.\n\n\nJabkowski, Piotr, and Aneta Piekut. 2024. “Not Random and Not\nIgnorable. An Examination of Nonresponse to Income Question in the\nEuropean Social Survey, 2008–2018.” Field Methods 36\n(3): 213–28.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr. 2nd ed. Springer.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. “Ggmosaic:\nMosaic Plots in the ’Ggplot2’ Framework.” https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nKass, Robert E, and Adrian E Raftery. 1995. “Bayes\nFactors.” Journal of the American Statistical\nAssociation 90 (430): 773–95.\n\n\nKim, Albert Y., Chester Ismay, and Jennifer Chunn. 2018a. “The\nFivethirtyeight r Package: ’Tame Data’ Principles for Introductory\nStatistics and Data Science Courses” 11. https://escholarship.org/uc/item/0rx1231m.\n\n\nKim, Albert Y, Chester Ismay, and Jennifer Chunn. 2018b. “The\nFivethirtyeight r Package:‘tame Data’principles for Introductory\nStatistics and Data Science Courses.” Technology Innovations\nin Statistics Education 11 (1).\n\n\nKubicek, Lorrie. 2022. “Can Music Improve Our Health and Quality\nof Life?” July 25, 2022. https://www.health.harvard.edu/blog/can-music-improve-our-health-and-quality-of-life-202207252786.\n\n\nKuhn, Max. 2025. “Tune: Tidy Tuning Tools.” https://doi.org/10.32614/CRAN.package.tune.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection: A Practical Approach for Predictive Models. Chapman;\nHall/CRC.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \"\nO’Reilly Media, Inc.\".\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2025a. “Yardstick:\nTidy Characterizations of Model Performance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\n———. 2025b. “Yardstick: Tidy Characterizations of Model\nPerformance.” https://doi.org/10.32614/CRAN.package.yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2020. “Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples.” https://www.tidymodels.org.\n\n\nLawlor, Jake. 2020. “PNWColors: Color Palettes Inspired by Nature\nin the US Pacific Northwest.” https://doi.org/10.32614/CRAN.package.PNWColors.\n\n\nLiedtke, Michael. 2025. “Amazon’s Zoox Launches Its Robotaxi\nService in Las Vegas.” AP News. https://apnews.com/article/amazon-zoox-robotaxis-las-vegas-bd5cb24602fb16243efcba05c7fe518f.\n\n\nMacGregor, Emily. 2025. “Music Can Lift Mood, Foster Community and\nEven Rewire Brains – but Does It Need to Have a Purpose?” April\n14, 2025. https://www.theguardian.com/music/2025/apr/14/mood-music-radio-3-unwind-emily-macgregor.\n\n\nMaxmen, Amy. 2018. “Self-Driving Car Dilemmas Reveal That Moral\nChoices Are Not Universal.” Nature 562: 469–70. https://doi.org/10.1038/d41586-018-07135-0.\n\n\nMilborrow, Stephen. 2025. “Rpart.plot: Plot ’Rpart’ Models: An\nEnhanced Version of ’Plot.rpart’.” https://doi.org/10.32614/CRAN.package.rpart.plot.\n\n\nMiller, Thomas Lumley based on Fortran code by Alan. 2024. “Leaps:\nRegression Subset Selection.” https://doi.org/10.32614/CRAN.package.leaps.\n\n\nMontgomery, Douglas C. 2017. Design and Analysis of\nExperiments. John wiley & sons.\n\n\nMubia, Brian. 2025a. “tastyR: Recipe Data from\n’Allrecipes.com’.” https://doi.org/10.32614/CRAN.package.tastyR.\n\n\n———. 2025b. “I Scraped 14K Recipes, so You Won’t Have To.”\nAugust 1, 2025. https://www.brians.works/i-scraped-14k-recipes-so-you-wont-have-to/.\n\n\nNational Centers for Environmental Information (NCEI). 2025.\n“Meteorological Versus Astronomical Seasons.” https://www.ncei.noaa.gov/news/meteorological-versus-astronomical-seasons.\n\n\nNCAA. 2013. “Divisional Differences and the History of\nMultidivision Classification.” National Collegiate Athletic\nAssociation. https://www.ncaa.org/sports/2013/11/20/divisional-differences-and-the-history-of-multidivision-classification.aspx.\n\n\nNowosad, Jakub. 2019. Check Color Palettes for Problems with Color\nVision Deficiency. https://jakubnowosad.com/colorblindcheck/.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using\nSpotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nPBC, Posit. 2025. “Quarto: An Open-Source Scientific and Technical\nPublishing System.” https://quarto.org/.\n\n\nPosit Software, PBC. 2025. “Download RStudio – Posit.” https://posit.co/downloads/.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” https://www.R-project.org/.\n\n\nR Core Team. 2025. “The r Base Package — Index (Version\n4.6.0).” Seminar für Statistik, ETH Zürich; https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear\nRegression: Applied Generalized Linear Models and Multilevel Models in\nr. Chapman; Hall/CRC.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023a. “Broom:\nConvert Statistical Objects into Tidy Tibbles.” https://CRAN.R-project.org/package=broom.\n\n\n———. 2023b. “Broom: Convert Statistical Objects into Tidy\nTibbles.” https://CRAN.R-project.org/package=broom.\n\n\n———. 2025. “Broom: Convert Statistical Objects into Tidy\nTibbles.” https://doi.org/10.32614/CRAN.package.broom.\n\n\nRudis, Bob, and Dave Gandy. 2023. “Waffle: Create Waffle Chart\nVisualizations.” https://github.com/hrbrmstr/waffle.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a\nModel.” The Annals of Statistics, 461–64.\n\n\nSpotify for Developers. n.d. “Get Audio Features (Spotify Web API\nReference).” Online Documentation. https://developer.spotify.com/documentation/web-api/reference/get-audio-features.\n\n\nTaruffi, Liila, Corinna Pehrs, Stavros Skouras, and Stefan Koelsch.\n2017. “Effects of Sad and Happy Music on Mind-Wandering and the\nDefault Mode Network.” Scientific Reports 7 (1): 14396.\nhttps://doi.org/10.1038/s41598-017-14849-0.\n\n\nTexas at El Paso College of Liberal Arts, The University of. n.d.\n“Project ACE: Action for Equity.” https://www.utep.edu/liberalarts/project-ace/.\n\n\nThe Trust for Public Land. 2021. “Parks and an Equitable\nRecovery.” Online.\n\n\nTherneau, Terry, and Beth Atkinson. 2025. “Rpart: Recursive\nPartitioning and Regression Trees.” https://doi.org/10.32614/CRAN.package.rpart.\n\n\nThomson-DeVeaux, Amelia, Jasmine Mithani, and Laura Bronner. 2020.\n“Why Many Americans Don’t Vote.” FiveThirtyEight.\nhttps://web.archive.org/web/20201102013301/https://projects.fivethirtyeight.com/non-voters-poll-2020-election/.\n\n\ntidyverse Development Team. 2025. “Tidyverse: R Packages for Data\nScience.” tidyverse; https://tidyverse.org/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data\nScience: A First Introduction. Chapman; Hall/CRC.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” In\nBreakthroughs in Statistics: Methodology and Distribution,\n408–52. Springer.\n\n\nTukey, John Wilder et al. 1977. Exploratory Data Analysis. Vol.\n2. Springer.\n\n\nU.S. Department of Education, Office of Postsecondary Education. 2025.\n“Equity in Athletics Data Analysis Cutting Tool.” U.S.\nDepartment of Education. https://ope.ed.gov/athletics/.\n\n\nUshey, Kevin, and Hadley Wickham. 2025. “Renv: Project\nEnvironments.” https://doi.org/10.32614/CRAN.package.renv.\n\n\nVaughan, Davis, and Simon Couch. 2025. “Workflows: Modeling\nWorkflows.” https://doi.org/10.32614/CRAN.package.workflows.\n\n\nVenables, W. N., and B. D. Ripley. 2002. “Modern Applied\nStatistics with s.” https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning\nSeveral Parameters When the Number of Observations Is Large.”\nTransactions of the American Mathematical Society 54 (3):\n426–82.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. “Skimr: Compact and Flexible\nSummaries of Data.” https://CRAN.R-project.org/package=skimr.\n\n\nWhite, Edward. 2025. “Top Sensor Maker Hesai Warns World Not Ready\nfor Fully Driverless Cars.” Financial Times, September.\nhttps://www.ft.com/content/1cea9526-17a8-4554-a660-1c1e6d69676b.\n\n\nWickham, Hadley. 2014a. “Tidy Data.” Journal of\nStatistical Software 59: 1–23.\n\n\n———. 2014b. “Data Science: How Is It Different to\nStatistics?” https://imstat.org/2014/09/04/data-science-how-is-it-different-to-statistics/.\n\n\n———. 2016. “Ggplot2: Elegant Graphics for Data Analysis.”\nhttps://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019a.\n“Welcome to the Tidyverse” 4:\n1686. https://doi.org/10.21105/joss.01686.\n\n\n———, et al. 2019b. “Welcome to the\nTidyverse” 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. 2nd ed. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023a. “Dplyr: A Grammar of Data Manipulation.” https://doi.org/10.32614/CRAN.package.dplyr.\n\n\n———. 2023b. “Dplyr: A Grammar of Data Manipulation.” https://doi.org/10.32614/CRAN.package.dplyr.\n\n\nWikipedia contributors. 2025a. “Data Science.” https://en.wikipedia.org/wiki/Data_science.\n\n\n———. 2025b. “Newton’s Method.” https://en.wikipedia.org/wiki/Newton%27s_method.\n\n\nWilke, Claus O. 2025. “Ggridges: Ridgeline Plots in\n’Ggplot2’.” https://doi.org/10.32614/CRAN.package.ggridges.\n\n\nXie, Yihui. 2024. “Knitr: A General-Purpose Package for Dynamic\nReport Generation in r.” https://yihui.org/knitr/.\n\n\nZarulli, Virginia, Elizaveta Sopina, Veronica Toffolutti, and Adam\nLenart. 2021. “Health Care System Efficiency and Life Expectancy:\nA 140-Country Study.” PLoS One 16 (7): e0253450.\n\n\nZehr, SM, RG Roach, D Haring, J Taylor, FH Cameron, and AD Yoder. n.d.\n“Life History Profiles for 27 Strepsirrhine Primate Taxa Generated\nUsing Captive Data from the Duke Lemur Center. Sci Data. 2014; 1:\n140019.”",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]