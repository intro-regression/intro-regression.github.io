# Exploratory data analysis {#sec-ch-eda}

```{r}
#| include: false
source("_common.R")
```

<!--# do i rename this to cleaning and exploration to be consistent with the workflow in chapter 1? -->

<!--# do i talk about the goals of EDA enough here?-->

<!--# do i need more here about notable features in the data-->

<!--# figure out how i talk about outliers and missing data here vs referencing other chapters-->

<!--# reconcile country and country_simp.-->

## Learning goals {.unnumbered}

-   Describe the distribution of an individual variable using visualizations and summary statistics

-   Describe the relationship between two or more variables using visualizations and summary statistics

-   Identify outliers and other interesting features in the data

-   Explain how outliers and other unusual features may impact analysis results <!--# I'd like another word besides unusual or interesting-->

<!--# learning goal?  Conduct exploratory data analysis in R ?-->

```{=html}
<!--# 
## Software and packages {.unnumbered}

-   `tidyverse` [@tidyverse]

-   `tastyR` [@tastyR]

-   `knitr` [@knitr]

-   `skimr` [@skimr]

-   `ggridges` [@ggridges]
-->
```

```{r}
#| label: make-recipes-data frame
#| eval: false

library(tidyverse)

recipes <- tastyR::cuisines |>
  # extract the year, month, and date
  mutate(year = year(date_published), 
         month = factor(month(date_published)),
         days_on_site = time_length(interval(date_published, dmy("01/08/2025")), "day"),
         chef_john = factor(if_else(author == "John Mitzewich", 1, 0)), #most common author on the site
         season = case_when(
           month %in% c("3", "4", "5") ~  "Spring", 
           month %in% c("6", "7", "8") ~ "Summer", 
           month %in% c("9", "10", "11") ~ "Fall", 
           month %in% c("12", "1", "2") ~ "Winter"
         ), 
         season = factor(season, levels = c("Winter", "Spring", "Summer", "Fall")), 
         # if i want to use country? 
         country_simp = case_when(
    country == "Italian" ~ "Italian", 
    country == "French" ~ "French", 
    country == "Brazilian" ~ "Brazilian", 
    country == "Filipino" ~ "Filipino", 
    TRUE ~ "Other"
  ), 
  country = country_simp) |>
  select(-country_simp)


recipes |> write_csv('data/recipes.csv')
```

```{r}
#| label: load-packages-data


library(tidyverse)
library(tastyR)
library(knitr)
library(skimr)
library(ggridges)
library(waffle)
library(ggmosaic)
library(PNWColors)

recipes <- read_csv("data/recipes.csv")

# use sunset2 color palettes from PNW palettes
sunset2 <- pnw_palette("Sunset2",2)
sunset4 <- pnw_palette("Sunset2",4)
sunset12 <- pnw_palette("Sunset2", 12)
```

## Introduction: Recipes {#sec-intro-recipes}

Allrecipes <!--# do I add a trademark? --> ([allrecipes.com](https://allrecipes.com)) is a popular cooking website visited by over 60 million users each month [@allrecipes_aboutus]. The site is most known for providing recipes and tips making it easy to cook a variety of dishes from around in the world at home. The recipes are primarily submitted by non-professional home cooks, and users can rate and review the recipes. Each week, over 200 new recipes are added to the site and over 2000 recipes receive new ratings [@allrecipes_aboutus].

The analysis in this chapter will focus on `r nrow(recipes)` recipes that were posted on Allrecipes between February 2009 and July 2025. In this data, a recipe is "posted" if it is newly published or updated. The data were adapted from the `cuisines` data frame in the **tastyR** R package [@tastyR]. The data in `cuisines` were originally scraped from Allrecipes by data scientist Brian Mubia [@briansworks_14krecipes_2025]. The **tastyR** package was featured as part of TidyTuesday [@tidytuesday], the weekly data visualization challenge, in September 2025.

The data are in `recipes.csv`. We will use the following variables: <!--# remove all the variables I don't end up using-->

-   `country`: The country / region the cuisine is from (`Brazilian`, `Filipino`, `French`, `Italian`, `Other`). This is a simplified version of the `country` variable in the `cuisines` data frame.

-   `author`: User who posted the recipe

-   `chef_john`: Indicator of whether recipe was posted by American Chef John Mitzewich (known as Chef John), derived from the variable `author`. (`1`: Posted by Chef John, `0`: Not posted by Chef John)

-   `month`: Month in which the recipe posted, derived from the variable `date_published` in the `cuisines` data frame. (`1`: January, `2`: February, `3`: March, . . ., `12`: December).

-   `season`: Season in which the recipe was posted, derived from variable `date_published` and based on the definition of seasons in @ncei.)

    -   `Spring`: Months 3, 4, 5

    -   `Summer`: Months 6, 7, 8

    -   `Fall`: Months 9, 10, 11

    -   `Winter`: Months 12, 1, 2

-   `calories`: Number of calories per serving

-   `protein`: Amount of protein per serving (in grams)

-   `avg_rating`: Average rating (`1`: worst to `5`: best)

-   `total_ratings`: Number of ratings

-   `reviews`: Number of reviews

-   `prep_time`: Preparation time (in minutes)

-   `cook_time`: Cooking time (in minutes)

-   `total_time`: Total time to make dish (in minutes).

    -   Note this is often `prep_time` + `cook_time` but not always. This could include other factors such as marination time or additional waiting periods.

-   `servings`: Number of servings

See the **tastyR** documentation [@tastyR] for the full codebook.

::: {.objective latex=""}
Our goal is to use exploratory data analysis to understand the distributions of key variables, the relationships between variables, and other interesting features in the data.
:::

## Purpose of exploratory data analysis

This book is about regression analysis, using statistical models to describe the relationship between a response variable and one or more predictor variables, as well as use predict values of the response variable. This chapter focuses exploratory data analysis, an important step of the workflow that happens before modeling as show @fig-data-science-workflow. **Exploratory data analysis** **(EDA)** is the process of examining the data to better understand the observations, distributions of variables, relationships between variables, and other interesting features of the data. It helps us identify observations that are far from the majority of the data, instances of missing data, and potential errors in the data <!--# such as errors in how the data were recorded or how the data set was loaded into the statistical software-->. Lastly, it is where we gain initial insights about potential relationships between variables that help inform the decisions we'll make as we do the regression analysis.

In his 1977 book *Exploratory Data Analysis* , Statistician John Tukey described EDA as "looking at data to see what it seems to say" [@tukey1977exploratory, p. v]. This short definition emphasizes an important point about EDA. It is only used to glean initial insights from the data ("what it *seems* to say") [**not**]{.underline} to draw conclusions. We use statistical inference (@sec-ch-slr-inf and @sec-ch-mlr-inf) to draw conclusions from the data. Though we can't draw conclusions from EDA, it is still a critical step in th workflow. The insights gleaned from EDA help us more fully understand the implications of our results as we apply them in practice. It also helps us as data scientists better communicate the nuances in the data to collaborators and stakeholders. We typically don't publish all the results of EDA in a final report or presentation, but the knowledge gained from EDA is infused throughout any report or presentation as we describe the data, interpret results, and share conclusions. Simply put,

> "Exploratory and confirmatory \[statistical inference\] can â€“ and should - proceed side by side." [@tukey1977exploratory, p. vii]

<!--# is this quote correct, did he not say "exploratoy and confirmatory statistics"? -->

<!--# make sure confirmatory is referring only to inference-->

<!--# is this the correct citation format?-->

![Exploratory data analysis workflow](images/03-eda-workflow.png){#fig-eda-workflow fig-align="center"}

@fig-eda-workflow shows the workflow for exploratory data analysis. We typically start by getting a general sense of the data, then explore more detailed relationships in the data. <!--# maybe make this a diagram? Maybe something like a pyramid and then add outliers and such alongside it-->

-   Clean data
-   Explore individual variables
-   Explore relationships between two variables
-   Explore relationships between three or more variables

In each step of the workflow, we will also make note of **outliers**, observations that are far from the majority of the data. The remainder of this chapter follows the progression outlined above, but the workflow may not be as linear in practice. In fact, some data cleaning may occur based on what is learned at each subsequent step. The goal is to *explore*, and thus use judgment and creativity to move around the data and guide the exploration.

## Initial data cleaning {#sec-initial-check}

After loading the data into software, the first step of the EDA workflow ([@fig-eda-workflow]) is an initial check to get an overall view of the data and some initial data cleaning. The purpose of this check is to see whether the data loaded into the software as expected, identify what the rows and columns represent, and understand the variable types and their formats. This step is important, because it allows us to begin data cleaning and make necessary changes to the data and variable formats before moving too far into the analysis.

Statistical software typically has functionality to view a summary of the data. For example, the `glimpse()` function in the **tidyverse** R package [@tidyverse-2] provides a summary (or "glimpse") of the data, like the one shown in @tbl-glimpse-recipe. We can often view data in its original format (e.g., `.xlsx` or `.csv`) before loading it into the software; however, we still need to check the data after loading it into the software in case of information loss or other problems.

```{r}
#| label: tbl-glimpse-recipe
#| tbl-cap: "Summary of `recipes` data"

glimpse(recipes)
```

@tbl-glimpse-recipe shows a summary of the `recipes` data. The rows contain the observations (also known as *cases*) in the data and the columns contain the variables that describe each observation. There are `r nrow(recipes)` rows (observations) and `r ncol(recipes)` columns (variables) in the data. Each row represents a recipe posted on Allrecipes and each column is a feature or characteristic of the recipe. At this point, we can ask ourselves if these are the expected number of rows and columns based on the original data source. If not, we need to figure out the discrepancy in the number of rows and/or columns and address it before moving forward.

Once we understand the structure of the data, we look at the individual variables more closely. At this point, the primary focus is on the variable type, but we may also notice if there are any missing values (typically coded as `NA`) and anything else notable about the variable that we want to explore further.

There are three types of variables: quantitative variables, categorical variables, and identifier variables. We typically assign the variable type based on the description and how the observations appear in the data. For example, we typically think of a variable such as *age*, an individual's age in years, as quantitative. This is the case if the values are numeric values such as `18, 24, 35,...`. However, *age* is a categorical variable if is defined as age ranges, such as `18-25, 26-30, 31-35,...`. It is important to know the variable type, because that informs how the variable is handled in the regression analysis.

::: {.yourturn latex=""}
-   What type of variable is `month`?

-   What is the more reliable identifier variable - `name` or `url`?[^03-eda-1]
:::

[^03-eda-1]: Month is a categorical variable, because each number represents a specific month. The more reliable identifier variable is `url`. There is one recipe per page, and multiple pages cannot have the same URL. Therefore, the URL uniquely identifies the recipe. Though unlikely, it is possible for multiple recipes to have the same name. This actually occurs for two names in the data "Cajun Chicken and Sausage Gumbo" and "Chicharrones de Pollo".

In addition to knowing the variable type, we also need to know how it is stored in the software. When the data set is loaded into the software, the software stores each column (the variables) as a specific format. Typically, this column format aligns with the variable type (this is what we want!), but occasionally the format does not align with the variable type. Therefore, the next step of the initial data check is to make sure the column formats are as we expect and address any mismatches. <!--# figure out when I want to use format versus type-->

In general, quantitative variables are stored in columns with *double / floating point number* formats, which are formats for columns that contain real numbers with decimals. In the output above, this is coded by the software as `dbl`, but you may also see this denoted by `float64` or similar codes. The software treats columns with this format as numeric data. Examples of these columns in the `recipes` data set are `avg_rating` and `prep_time`. These are both quantitative variables and are being stored in the *double/floating point number* format in the software. Therefore, the column format aligns with the variable type and will be treated as we expect by the software as we do the analysis.

Columns that contain categorical data are typically stored as *character* or *factor* formats. The software treats both of these formats as text rather than numeric data. The primary difference between character and factor types is that the data stored as factors also have an ordering applied. Let's take a look at some categorical variables in the `recipe` data . The variables `country` and and `author` are two categorical variables that are correctly stored in the software as character (`<chr>`) data formats.

<!--# I might not need this? The variable season  is also a categorical stored as a character data format. By default, the software will arrange values of this variable in alphabetical order in graphs and analysis output. The seasons do have a specific calendar order that the software is unaware of currently. We can convert this variable to a factor data type and tell the software the calendar order.-->

```{r}
#| label: make-season-factor
#| echo: false

recipes <- recipes |>
  mutate(season = factor(season, levels = c("Winter", "Spring", "Summer", "Fall"))
  )
```

<!--# this variables type vs column type is feeling a bit convoluted-->

<!--# Make a table for this mapping between variable type and column-->

One of the most common mismatches between the variable's type and how it is stored occurs when categorical variables are stored in the software under numeric formats. From @tbl-glimpse-recipe, we see this has occurred with the variables `month` and `chef_john`. This often happens when the categories are represented by numbers (e.g., "1" instead of "January"). Though we know they are categorical from @sec-intro-recipes, they are currently stored as *double* column types by the software. This means the the software thinks we can do calculations, such as find the average or standard deviation; however, these types of operations would not make sense in the context of `month`, for example. This mismatch can become particularly troublesome in the regression models. As we'll see in @sec-ch-mlr, quantitative and categorical variables are handled differently in regression models.

We address these mismatches before moving forward with the EDA by changing the format in which such variables are stored. Once we have made changes to the data, we examine the summary of the data again to check the changes we've made and address any remaining issues. In @tbl-glimpse-recipes-2, we see that `chef_john` and `month` are now correctly stored as factors, a format suitable for categorical variables. We are now ready to move on to the next step and explore the distributions of individual variables. <!--# [Add reference here to R for data science? Telling stories with data? Modern data science for more detail on data cleaning / prep?] --> <!--# may not need this detail? We specifically make month a factor variable type, because the months have a natural ordering.-->

```{r}
#| label: fix-month-chefjohn
#| echo: false


recipes <- recipes |>
  mutate(month = factor(month), 
         chef_john = factor(chef_john))
```

```{r}
#| label: tbl-glimpse-recipes-2
#| tbl-cap: "Summary of `recipes` data with correct formats for `chef_john` and `month`"

glimpse(recipes)
```

## Distribution of a single variable {#sec-univar-eda}

**Univariate exploratory data analysis** is the exploration of individual variables. The goal is to better understand the variables in the data before exploring at relationships between variables. This is also where we begin to see how the observations in our sample data compare to the population of interest by identifying ways in which the sample data are representative of the population and ways it differs. This helps us understand the scope of inferential conclusions we can draw and potential limitations of the analysis. Univariate EDA is also where we can more clearly see outliers and missing data, and continue cleaning data, as needed.

### Categorical variable {#sec-univar-eda-cat}

The univariate distribution for a categorical variable includes the levels (or categories) of the variable and the number of observations or proportion of observations at each level. We examine distributions of categorical variables using visualizations and frequency tables.

Let's examine the distribution of `month`, the month a recipe was posted, and get some initial insight into the times of year people generally post recipes to the website.

#### Visualize the distribution {.unnumbered}

A **bar chart** <!--# do we want to call this bar plot or bar graph?--> is a commonly used visualization for univariate categorical distributions. A bar chart has one bar for each level, such that the height of the bar represents the **frequency**, the number of observations at that level. @fig-univar-month is the bar chart for `month`. There is a bar for each month, and the height of the bar represents the number of recipes posted that month.

<!--# do i want to adjust the labels? or leave as is?-->

```{r}
#| label: fig-univar-month
#| fig-cap: "Bar chart for the distribution of `month`"
ggplot(data = recipes, aes(x = month)) + 
  geom_bar() + 
  labs(x = "Month") + 
  theme_bw()
```

As we look at @fig-univar-month, one thing that immediately stands out is the large number of recipes in November. We are unable to determine the reason for this large spike from the plot alone, but we may have some hypotheses about why this is the case based on our understanding the site's users and our collaborators' subject-matter expertise. One hypothesis is that there is a large uptick in recipes during the holiday season in the United States, Thanksgiving in particular, because it is a United States holiday in November that traditionally includes a large meal.

The bar chart in @fig-univar-month displays the frequencies, but perhaps we are more interested in visualizing the proportions, also called **relative frequencies**. In that case we can (1) remake the bar plot such that that height of the bars represent the proportions rather than frequencies ([@fig-univar-eda-month-prop-1]), (2) make a pie chart, or (3) make a waffle chart.

```{r}
#| label: fig-univar-eda-month-prop
#| fig-cap: "Visualizations of distribution of `month` with proportions"
#| fig-subcap: 
#|   - "Bar chart"
#|   - "Pie chart"
#|   - "Waffle chart"
#| layout-ncol: 2

recipes_prop <- recipes |> 
  count(month) |>
  mutate(prop = n / sum(n))

# bar chart
ggplot(data = recipes_prop, aes(x = month, y = prop)) +
  geom_col(color = "black") + 
  labs(y = "Proportion",
       x = "Month") +
  theme_bw()

# pie chart 
ggplot(data = recipes_prop, aes(x = "", y = n, fill = month)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar(theta = "y") +
  scale_fill_manual(values = sunset12) +
  labs(fill = "Month") +
 # labs(title = "Pie Chart Example") +
  theme_void()  # removes background, axes, etc.

# waffle chart 
ggplot(data = recipes_prop, aes(fill = month, values = n)) +
  geom_waffle(
    color = "black", flip = TRUE, make_proportional = TRUE, na.rm = TRUE
  ) +
    theme_enhance_waffle() +
  labs(fill = "Month", title = NULL) +
#  scale_fill_brewer("Set2") +
  coord_equal() +
  theme(
   # legend.position = "bottom",
    legend.text = element_text(size = 12)
  ) +
  scale_fill_manual(values = sunset12)
```

In @fig-univar-eda-month-prop-2, the distribution of `month` is visualized using a **pie chart**. In a pie chart, each slice represents the proportion of observations at a given level. As we observed from the bar chart, the slice for Month 11 (November) is the largest, indicating the large proportion of recipes in the data were published that month.

In @fig-univar-eda-month-prop-3, the distribution of `month` is visualized using a **waffle chart**. In a waffle chart, the number of squares for each level represents the proportion of observations at that level. It is similar to a pie chart in that is shows how the parts (levels) make up the whole (variable). In fact, it is sometimes referred to as a "square pie chart". <!--# is that true widely or on one site? --> A nice feature of the waffle chart is that we can count the squares to approximate the proportion of observations at each level.

In general, we can use any of these charts to visualize a univariate distribution of a categorical variable. As we see in @fig-univar-eda-month-prop, the pie chart and waffle chart can be challenging to read when there are many levels. It can be hard to compare the size of the slices in the pie chart, and distinguish between the many colors on both charts. Therefore, it is preferable to reserve these for categorical variables with fewer levels.

#### Summarize the distribution {.unnumbered}

In addition to visualizations, we can make a frequency table, a table of the number and proportion of observations at each level.

```{r}
#| label: tbl-univar-eda-month
#| tbl-cap: "Frequency table for `month`"

recipes_prop |> 
  kable(digits = 3)
```

From @tbl-univar-eda-month, we not only see that Month 11 (November) is the most popular month for posting recipes, but we see more specifically that about 17.8% of the recipes in the data were posted that month. We can also more easily identify the least common month to post recipes, Month 10 (October). Only about 3.7% of the recipes in the data were posted that month.

#### Example: Distribution of `season` {.unnumbered}

Let's use the visualizations and summaries introduced in this section to describe the distribution of the season in which a recipe was posted.

```{r}
#| label: fig-univar-season 
#| fig-cap: "Distribution of `season`"
#| fig-subcap: 
#|   - "Bar chart"
#|   - "Pie chart"
#|   - "Waffle chart"
#| layout-ncol: 2

season_prop <- recipes |> 
  count(season) |>
  mutate(prop = n / sum(n))

# bar plot 
ggplot(data = recipes, aes(x = season)) + 
  geom_bar() + 
  theme_bw()

# pie chart 
ggplot(data = season_prop, aes(x = "", y = n, fill = season)) +
  geom_bar(stat = "identity", width = 1, color = "black") +
  coord_polar(theta = "y") +
 # labs(title = "Pie Chart Example") +
  theme_void() +  # removes background, axes, etc.
  scale_fill_manual(values = sunset4)

# waffle chart 
ggplot(data = season_prop, aes(fill = season, values = n)) +
  geom_waffle(
    color = "black", flip = TRUE, make_proportional = TRUE, na.rm = TRUE
  ) +
    theme_enhance_waffle() +
  labs(fill = NULL, title = "Season") +
#  scale_fill_brewer("Set2") +
  coord_equal() +
  theme(
   # legend.position = "bottom",
    legend.text = element_text(size = 12)
  ) +
  scale_fill_manual(values = sunset4)
```

```{r}
#| label: tbl-univar-eda-season
#| tbl-cap: "Frequency table of `season`"
#| echo: false

# frequency table
season_prop |> 
  kable(digits = 3)

```

::: {.yourturn latex=""}
Use @fig-univar-season and @tbl-univar-eda-season to describe the distribution of `season`.[^03-eda-2]
:::

[^03-eda-2]: In general, there number of recipes posted is approximately equal across the winter, summer, and fall seasons. The smallest proportion of recipes are posted during the spring season.

Both `month` and `season` provide information about when individuals post recipes to the site, so we wouldn't necessarily need both variables in later stages of the analysis. An advantage to `season` is that it has fewer levels, so visualizations are easier to interpret, particularly the pie chart and waffle chart, compared to `month` ([@fig-univar-eda-month-prop]). It will also be easier to interpret as we look at relationships between variables. An advantage to `month` is that we get the more specific detail, such as the large number of recipes posted in November. In practice, we could refer back to the analysis objective to determine which variable to use in later steps.

### Quantitative variable {#sec-univar-eda-quant}

As with categorical variables, we examine the univariate distributions of quantitative variables using visualizations and summary statistics. The description of the distribution of a quantitative variable includes the following components: shape, center, spread, and the presence of notable features such as outliers. Let's examine the distribution of `avg_rating`, a recipe's average user rating. The ratings range from 1 to 5, with 1 being the worst and 5 being the best.

#### Visualize the distribution {.unnumbered}

A **histogram** is commonly used to visualize the distribution of a quantitative variable. In a histogram, the values of the variable are divided into ranges of equal width (called *bins*), and there is one bar on the graph for each bin. The height of the bar represents the number of observations that have values within the bin. It is similar to a bar chart, but the bars represent a range of values instead of individual levels.

```{r}
#| label: fig-univar-eda-rating
#| fig-cap: "Histogram of `avg_rating`"

ggplot(data = recipes, aes(x = avg_rating)) + 
  geom_histogram(fill = "steelblue", color = 'black', binwidth = 0.1) +
  labs(x = "Average rating",
       y = "Count") +
  theme_bw()
```

@fig-univar-eda-rating is the histogram of `avg_rating`. We now clearly see that a vast majority of the recipes have average ratings between 4 and 5. There are very few recipes with ratings less that 3, and at least one outlier with a an average rating around 1.

::: {.analysis_in_practice latex=""}
The width of the bins on a histogram can make it easier or harder to see the features of the distribution. In general, the default bin width set by the statistical software is a good choice. In some cases, however, we may wish to change the width to illuminate more detail in the distribution (decrease bin width) or smooth out the distribution (increase bin width).

```{r}
#| label: fig-histogram-binwidth
#| fig-cap: "Different histogram bin widths"
#| fig-subcap: 
#|   - Default
#|   - Small
#|   - Large
#| layout-ncol: 3

set.seed(12345)
n <- 1000

symmetric <- rnorm(n, mean = 5, sd = 2)

df <- tibble(x = symmetric)

# default
ggplot(data = df, aes(x = x)) + 
  geom_histogram(fill = "steelblue", color = 'black') +
  theme_bw() +
  labs(x = "")

# small
ggplot(data = df, aes(x =x)) + 
  geom_histogram(fill = "steelblue", color = 'black', binwidth = 0.08) +
  theme_bw() +
  labs(x = "")

# large
ggplot(data = df, aes(x = x)) + 
  geom_histogram(fill = "steelblue", color = 'black', binwidth = 3) +
  theme_bw() + 
  labs(x = "")
```

@fig-histogram-binwidth shows examples of the default bin width, very small bin widths, and very large bin widths. Small bin widths adds more detail the histogram. This can make the histogram more challenging to read without adding much useful information for the analysis. In contrast, large bin widths can aggregate the data so much that we lose key information about the distribution.

In general, it is good practice to start with the default bins set by software and make small adjustments, if needed.
:::

@fig-univar-eda-rating-alt shows two commonly used alternatives to histograms, density plots and boxplots. A **density plot** ([@fig-univar-eda-rating-alt-1]) is often thought of as a "smoothed out histogram". Similar to a histogram, portions of the distribution with larger heights indicate the parts of the distribution where more observations occur. In general, we are not concerned with the exact values from the density plot, but rather we use the plot for a summary view of the distribution.

A **boxplot** ([@fig-univar-eda-rating-alt-2]) is a visualization that highlights the quartiles and outliers of a distribution. The left line of the box is the first quartile, $Q_1$, which marks the $25^{th}$ percentile. The middle line in the box is the **median**, $Q_2$, which marks the $50^{th}$ percentile. The right line of the box is the third quartile, $Q_3$, which marks the $75^{th}$ percentile. The points on the graph represent outliers.

These visualizations are most useful for describing the shape of a distribution and for identifying outliers. Some features are more apparent on one type of visualization compared to others, so we use the goals of the EDA to help choose which visualization(s) to use.

```{r}
#| label: fig-univar-eda-rating-alt
#| fig-cap: "Density plot and boxplot of `avg_rating`"
#| fig-subcap:
#|   - "Density plot"
#|   - "Boxplot"
#| layout-ncol: 2

ggplot(data = recipes, aes(x = avg_rating)) + 
  geom_density(fill = "steelblue", color = 'black') +
  theme_bw()

ggplot(data = recipes, aes(x = avg_rating)) + 
  geom_boxplot(fill = "steelblue", color = 'black') +
  theme_bw()
```

#### Describe the distribution: Shape {.unnumbered}

The description of **shape** includes the skewness and number of modes.

```{r}
#| label: fig-shape-skew
#| fig-cap: Skewness of distribution
#| fig-subcap: 
#|   - "Left-skewed"
#|   - "Right-skewed"
#|   - "Symmetric"
#| layout-ncol: 3

# simulation-code generated by Chatgpt
set.seed(12345)
n <- 1000

right_skew <- rexp(n, rate = 1/2)

left_skew <- -rexp(n, rate = 1/2) + 10

symmetric <- rnorm(n, mean = 5, sd = 2)

# Combine into one data frame
df <- tibble(left_skew = left_skew, 
             right_skew = right_skew, 
             symmetric = symmetric)

#left-skew histogram 
ggplot(data = df, aes(x = left_skew)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()

#right-skew histogram 
ggplot(data = df, aes(x = right_skew)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()

#symmetric histogram 
ggplot(data = df, aes(x = symmetric)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()
```

The skewness is described as left-skewed, right-skewed, or symmetric. @fig-shape-skew shows examples of a distribution with each type of skewness. If a distribution has a **tail**, a portion of the distribution that covers a wide range of values with very few observations, then the direction of the skewness is determined by the direction of the tail. @fig-shape-skew-1 shows a **left-skewed** distribution (also called *negatively-skewed*). The majority of the observations take values at the higher end of the range, and the tail extends out to the lower values (the left side) of the distribution. An example of a left-skewed distribution is a typical distribution of online ratings for a product. Most online ratings tend to be very high, with few people giving low ratings, as we have observed with `avg_rating`.

@fig-shape-skew-2 is an example of a **right-skewed** distribution (also called *positively-skewed*). The majority of the observations take values at the lower end of the range, and the tail extends out to the higher values (the right side) of the distribution. An example of a right-skewed distribution is a distribution of annual income among adults in the United States. The income for most adults lie within a particular range, and a few adults have annual incomes far greater than the majority of the population (e.g., professional athletes, celebrities, etc.)

@fig-shape-skew-3 is an example of a **symmetric** distribution. If we divide the distribution down the middle, both sides of the distribution are approximately the same. Another way to think about it is if we fold the distribution along the center, the left side would fit almost perfectly on the right side. An example of a symmetric distribution is the distribution of shoe size among adults. A majority of individuals have a shoe size around the center of the distribution, and a small number of individuals have very small or very large shoe sizes.

```{r}
#| label: fig-shape-mode
#| fig-cap: Modality of distribution
#| fig-subcap: 
#|   - "Unimodal"
#|   - "Bimodal"
#|   - "Multimodal"
#| layout-ncol: 3

# simulation code generated by ChatGPT

set.seed(12345)
n <- 1000

# (1) Unimodal: a single Normal distribution
unimodal <- rnorm(n, mean = 0, sd = 1)


# helper: create mixture ensuring integer counts sum to n for bimodal and multimodal
make_mixture <- function(means, sds, weights = NULL, n = 1000) {
  k <- length(means)
  if (is.null(weights)) weights <- rep(1/k, k)
  if (length(weights) != k || length(sds) != k) stop("means, sds, and weights must have same length")
  # convert weights to integer sizes that sum to n
  raw_sizes <- weights * n
  sizes <- floor(raw_sizes)
  # allocate remainder to largest fractional parts to sum to n
  remainder <- n - sum(sizes)
  if (remainder > 0) {
    frac <- raw_sizes - floor(raw_sizes)
    order_idx <- order(frac, decreasing = TRUE)
    sizes[order_idx[1:remainder]] <- sizes[order_idx[1:remainder]] + 1
  }
  # generate components
  components <- mapply(function(m, s, nn) rnorm(nn, mean = m, sd = s),
                       means, sds, sizes, SIMPLIFY = FALSE)
  do.call(c, components)
}

# (2): Bimodal: a single Normal distribution
means_bi <- c(-1.5, 1.5)   # far apart
sds_bi   <- c(0.8, 0.8) # tight peaks
weights_bi <- c(0.5, 0.5)

bimodal <- make_mixture(means_bi, sds_bi, weights_bi, n = n)

# (3) Multimodal

means_multi <- c(-3, 0, 3)
sds_multi   <- c(0.8, 0.8, 0.8)
weights_multi <- c(0.33, 0.34, 0.33)  # sums to 1

multimodal <- make_mixture(means_multi, sds_multi, weights_multi, n = n)

# Combine into one data frame
df <- tibble(unimodal = unimodal, 
             bimodal = bimodal, 
             multimodal = multimodal)

#unimodal histogram 
ggplot(data = df, aes(x = unimodal)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()

#bimodal histogram 
ggplot(data = df, aes(x = bimodal)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()

#multimodal histogram 
ggplot(data = df, aes(x = multimodal)) + 
  geom_histogram(fill = "darkcyan", color = "black") + 
  theme_bw()
```

In addition to the skewness, the shape of the distribution includes the **modality**, number of peaks. @fig-shape-mode shows examples of distributions with three different modalities: unimodal, bimodal, and multimodal. @fig-shape-mode-1 is an example of a **unimodal** distribution, a distribution with a single peak. The distribution of annual incomes for adults in the United States is an example of a unimodal distribution.

@fig-shape-mode-2 is an example of a **bimodal** distribution, a distribution with two peaks. Bimodal distributions generally indicate there are at least two subgroups represented in the data that we may want to take into account in the analysis. An example of a bimodal distribution is a city's average daily temperatures in a city across a year. If the city has four distinct seasons, there will typically be a peak around the mean temperature in the fall and winter (low temperatures) and a peak around the mean temperature for spring and summer (high temperatures).

@fig-shape-mode-3 is an example of a **multimodal** distribution, a distribution with three or more peaks. Similar to bimodal distributions, multimodal distributions generally indicate there are multiple subgroups in the data we want to take into account i the analysis. An example of a multimodal distribution is the distribution of heights for school-aged children across a large age range, about 5 - 18 years old. There will be a peak around the mean height for elementary school children (around ages 5 - 9), a peak around the mean height middle school children (around ages 10 - 14), and a peak around the mean height for high school children (around ages 14 - 18).

::: {.analysis_in_practice latex=""}
To determine the modality of a plot, think about using a pencil or finger to trace along the top of the distribution. The modality, then, is the number of peaks in the traced outline.

As we trace the distribution, we don't need to hit every small dip in the distribution, because these small dips are often noise in the data. Rather, we are concerned about large peaks and valleys in the distribution.
:::

::: {.yourturn latex=""}
Use @fig-univar-eda-rating and @fig-univar-eda-rating-alt to describe the shape of the distribution of `avg_rating`.[^03-eda-3]
:::

[^03-eda-3]: The shape of the distribution of `avg_rating` is skewed left and unimodal.

#### Summarize the distribution {.unnumbered}

In addition to visualizations, we compute summary statistics to more precisely describe aspects of the distribution. The summary statistics for `avg_rating` are in @tbl-univar-eda-rating.

```{r}
#| label: tbl-univar-eda-rating
#| tbl-cap: "Summary statistics for `avg_rating`"

recipes |> 
  skim(avg_rating) |>
  select(complete_rate:numeric.p100) |>
  kable(digits = 3, col.names = c("Prop Complete", "Mean", "SD", "Min", "Q1", "Median", "Q3", "Max"))
```

We will primarily use these summary statistics to describe the center and spread of the quantitative distribution. We have also computed the statistic `Prop Complete` that is the proportion of observations that have a reported value for the variable. If the proportion is less than 1, then there are observations with missing values for the variable. We need to address the missingness before moving forward with the analysis. Strategies for dealing with missing data are in @sec-ch-special-topics.

Let's use the summary statistics to describe the center and spread of the distribution.

#### Describe the distribution: Center {.unnumbered}

We use the **mean** (also called *average*) or **median** (also called $Q_2$, the $2^{nd}$ quartile, $50^{th}$ percentile) to describe the **center** of a distribution. To determine which measure is the better representation of the center, consider the shape of the distribution and whether there are outliers. If the distribution is approximately symmetric with no (or few) outliers, then the mean is the preferred measure of center. The mean is impacted by skewness and outliers, so the median is preferred if skewness or outliers are present.

::: {.analysis_in_practice latex=""}
We prefer the mean over the median to describe the center of the distribution when the criteria mentioned above are met. This is because the mean is computed using all the observations in the data set. In contrast, the median is determined based on the one or two observations in the middle to the distribution. In general, we prefer statistics that use all the data if they will be representative of the distribution and not skewed by a few extreme observations.
:::

From @tbl-univar-eda-rating, the mean value of `avg_rating` in the data is about `r round(mean(recipes$avg_rating, na.rm = TRUE), 3)` and the median is `r round(median(recipes$avg_rating, na.rm = TRUE), 3)`. The distribution of `avg_rating` is left-skewed and has outliers (see @fig-univar-eda-rating), so the median is the more representative measure of the center of the distribution.

#### Describe the distribution: Spread {.unnumbered}

The standard deviation, interquartile range (IQR), and range are measures used to describe the **spread** of a distribution. If shape of the distribution is approximately symmetric with no (or few) outliers, the **standard deviation**, a measure of the average distance between each observation and the mean, is the preferred measure of the spread. Because the mean is used to to compute the standard deviation, it is impacted by skewness and outliers. When the distribution is skewed or there are outliers, the **interquartile range (IQR)** is the more representative measure of spread. The IQR is the difference between the $75^{th}$ and $25^{th}$ percentiles, $Q_3 - Q_1$.

The **range** $(\text{max} - \text{min})$ should be used with caution and not reported as the only measure of spread. Because it only takes into account the minimum and maximum values, it only gives describes the spread of the extreme ends of the distribution, which is not typically the spread of the majority of the data. Additionally, it is impacted by skewness and outliers, and thus can make the spread of a distribution appear larger than what is true for the vast majority of the data.

::: {.analysis_in_practice latex=""}
To describe the center and spread of a distribution, use

-   the mean and standard deviation if there is no extreme skewness or outliers, or

-   the median and IQR if skewness or outliers are present
:::

From @tbl-univar-eda-rating, the standard deviation of `avg_rating` is `r round(sd(recipes$avg_rating, na.rm = TRUE), 3)`, so the average rating for each recipe is about `r round(sd(recipes$avg_rating, na.rm = TRUE), 3)` points from the mean rating of `r round(mean(recipes$avg_rating, na.rm = TRUE), 3)`, on average. The IQR is `r IQR(recipes$avg_rating,na.rm = TRUE)`. This means the spread of the middle 50% of the distribution, $Q_3 - Q_1$ is about `r IQR(recipes$avg_rating, na.rm = TRUE)` points. Lastly, the range, the distance between the minimum and maximum values, is `r max(recipes$avg_rating, na.rm = TRUE) - min(recipes$avg_rating, na.rm = TRUE)` .

Because the distribution of `avg_rating` is skewed with outliers, the IQR `r IQR(recipes$avg_rating, na.rm = TRUE)` is the most representative measure of spread.

<!--# does it make sense to put something about missing values here?-->

#### Describe the distribution: Outliers {.unnumbered}

The last part of describing the distribution of a quantitative variable is the presence of outliers. Outliers can be valid observations that are very different from the rest of the data (e.g,. a professional athlete's annual salary in the distribution of annual salaries for 1000 randomly sampled adults in the United States). Sometimes, however, they indicate errors in the data (e.g., a person's age is recorded as 150 years old).

Outliers are most visible in histograms ([@fig-univar-eda-rating]) and boxplots ([@fig-univar-eda-rating-alt-2]). It is more challenging to differentiate between outliers and skewness in a density plot. From the histogram, we visually assess outliers by looking for observations that are set apart on the graph, typically very high or very low. For example, in @fig-univar-eda-rating, there appears to be at least a few outliers that have low average ratings around 1 and 2.

On a boxplot, the outliers are marked as points on the plot. An observation is considered an outlier if it is less than $Q_1 - 1.5 \times IQR$ or greater than $Q_3 + 1.5 \times IQR$. Based on the boxplot in @fig-univar-eda-rating-alt, observations with average ratings about 3.5 or less have been identified as outliers. Based on this plot, there are no outliers on the high end of the distribution of average rating.

If there are outliers due to data entry errors, we need to input the correct values, if possible, or remove the observations from the analysis data. If there are a lot of data entry errors for an individual variable, we may consider removing that variable from the analysis and thus retaining more observations for the analysis. If outliers are unusual yet valid observations, there are options on how to handle them based on the analysis goals and their impact on the regression analysis results. We discuss these options further in @sec-handle-outliers.

<!--# add this about notable features here? Unusual patterns are those that may not follow what we would expect, such as a mode at an unexpected value. This commonly happens in practice with modes at values such as -1 or 0, which are intended to represent missing data rather than actual observed values.-->

::: {.analysis_in_practice latex=""}
The description of the distribution of a quantitative variable includes the following:

-   **Shape**: Skewness and modality of distribution

-   **Center**: Middle of the distribution (measured by mean or median)

-   **Spread**: How far apart the observations are (measured by standard deviation or IQR)

-   **Outliers**: Observations that are far from the rest of the data
:::

#### Example: Distribution of `cook_time` {.unnumbered}

Let's use the visualizations and summary statistics introduced in the previous section to describe the distribution of `cook_time`, the amount of time (in minutes) it takes to cook a dish.

```{r}
#| label: fig-univar-eda-cook-time
#| fig-cap: "Distribution of `cook_time`"

ggplot(data = recipes, aes(x = cook_time)) + 
  geom_histogram(fill = "steelblue", color = 'black') +
  theme_bw()
```

```{r}
#| label: tbl-univar-eda-cook-time
#| tbl-cap: "Summary statistics for `cook_time`"

recipes |> 
  skim(cook_time) |>
  select(complete_rate:numeric.p100) |>
  kable(digits = 3, col.names = c("Prop Complete", "Mean", "SD", "Min", "Q1", "Median", "Q3", "Max"))
```

::: {.yourturn latex=""}
Use the visualizations in @fig-univar-eda-cook-time and summary statistics in @tbl-univar-eda-cook-time to describe the distribution of `cook_time`. Include the shape, center, spread, and potential outliers in the description.[^03-eda-4]
:::

[^03-eda-4]: The distribution of `cook_time` is unimodal and right-skewed. The center is best represented by the median, of 25 minutes. The spread is best represented by the IQR, of 35 minutes (45 - 10). There are some clear outliers with cook times of about 350 minutes and greater (the max is 600).

When a distribution has extreme outliers, such as the distribution of `cook_time`, the outliers can compress the majority of the observations on the graph, making it difficult to get a detailed view of the majority of the distribution. In this case, we can also make a visualization without the outliers to get a better view.

About 95% of the observations have cook times of 150 minutes or less, so we create a new histogram that only includes those observations.

```{r}
#| label: fig-univar-eda-cook-time-zoom
#| fig-cap: "Original versus zoomed-in distribution of `cook_time`" 
#| fig-subcap: 
#|   - "All observations"
#|   - "Cook time 150 minutes or less"
#| layout-ncol: 2


ggplot(data = recipes, aes(x = cook_time)) + 
  geom_histogram(fill = "steelblue", color = 'black') +
  theme_bw()

recipes |> 
  filter(cook_time <= 150) |>
  ggplot(aes(x = cook_time)) + 
  geom_histogram(binwidth = 5, fill = "steelblue", color = 'black') +
  theme_bw()
```

In @fig-univar-eda-cook-time-zoom, we compare a visualization of the full distribution with one that only includes recipes with cook time 150 minutes or less. For example, in @fig-univar-eda-cook-time-zoom-2, we get a lot of detail about the distribution of recipes with cook time 50 minutes or less. In contrast, all these recipes are contained in the first three bars of the histogram in @fig-univar-eda-cook-time-zoom-1.

## Relationship between two variables {#sec-bivar-eda}

After we have explored the distributions of individual variables in the univariate EDA, we explore the relationships between variables. We begin with **bivariate EDA**, the exploration of the relationship between two variables. In @sec-multivar-eda, we extend this to the relationship between three or more variables. There are three types of bivariate relationships: relationship between two quantitative variables, relationship between one quantitative and one categorical variable, and the relationship between two categorical variables.

### Two quantitative variables {#sec-bivar-quant-eda}

Similar to univariate EDA, we use visualizations and summary statistics to explore the relationship between two variables. The components to describe the relationship between two quantitative variables include the shape, direction, and potential outliers. We use a scatterplot to visualize the relationship and the correlation to quantify it.

#### Visualize the relationship {.unnumbered}

A **scatterplot** is a plot for two quantitative variables, such that one variable is on the x-axis (horizontal axis), one variable is on the y-axis (vertical axis), and each observation is represented by a point. Let's take a look at the relationship between the number of calories per serving (`calories`) and the total grams of protein per serving (`protein`). <!--# do i need a motivating sentence? Add something here about current obsession with protein and how one might wonder how the protein content relates to the total calories ( can thsi be a measure of how filling a meal is? ) in the meals.?-->

```{r}
#| label: fig-bivar-eda-protein-calories
#| fig-cap: "Relationship between `calories` and `protein`"

ggplot(data = recipes, aes(x = calories, y = protein)) + 
  geom_point(alpha = 0.5) + 
  labs(x = "Calories per serving", 
       y = "Protein in grams") +
#  geom_smooth(method = "lm") +
  theme_bw()
```

@fig-bivar-eda-protein-calories is the scatterplot with `calories` on the x-axis and `protein` on the y-axis. From the visualization, we can see the shape and direction of the relationship, along with outliers. We also get an indication of the strength of the relationship that we will quantify using summary statistics.

<!--# do i need to mention this? We may need to check some of these protein and calorie estimates - did some people misinterpret how to post these values. This is a good reason to do the EDA and fix before the analysis.-->

#### Describe the relationship: Shape and direction {.unnumbered}

In bivariate EDA, the **shape** is a description of the overall trend of the points in the scatterplot. Common shapes we are linear, quadratic, and logarithmic. There may also be no clear trend. These are shown in @fig-bivariate-shape. @fig-bivariate-shape shows examples of the commonly observed shapes. When we do linear regression, we generally assume that the relationship between the response variable and the predictor variable(s) is linear as in @fig-bivariate-shape-1. Additionally, there are methods to account for quadratic ([@fig-bivariate-shape-2]) and logarithmic ([@fig-bivariate-shape-3]) relationships in the model. A scatterplot with no clear trend as in @fig-bivariate-shape-4 indicates there is little to no relationship between the two variables.

```{r}
#| label: fig-bivariate-shape
#| fig-cap: Shapes of bivariate relationships
#| fig-subcap: 
#|   - "Linear"
#|   - "Quadratic"
#|   - "Logarithmic"
#|   - "No clear shape"
#| layout-ncol: 2

set.seed(1234)

#simulation code generated by ChatGPT 5

n <- 150  # points per relationship

# Linear
x_lin <- runif(n, 0, 10)
y_lin <- 2.5 * x_lin + 1.5 + rnorm(n, mean = 0, sd = 3.0)
df_linear <- tibble(x = x_lin, y = y_lin, relation = "linear")

# plot linear 
ggplot(data = df_linear, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

# Quadratic
x_quad <- runif(n, -3, 6)
y_quad <- -1.2 * x_quad^2 + 3.0 * x_quad + 10 + rnorm(n, mean = 0, sd = 5.0)
df_quad <- tibble(x = x_quad, y = y_quad, relation = "quadratic")

# plot quadratic 
ggplot(data = df_quad, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()


# Logarithic
# keep x > 0.1 to avoid -Inf
x_log <- runif(n, 0.1, 10)
y_log <- 6.0 * log(x_log) + 2.0 + rnorm(n, mean = 0, sd = 1.8)
df_log <- tibble(x = x_log, y = y_log, relation = "logarithmic")

#plot log
ggplot(data = df_log, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

# No clear relationship
x_none <- rnorm(n, mean = 5, sd = 2.5)
y_none <- rnorm(n, mean = 20, sd = 5.0)
df_none <- tibble(x = x_none, y = y_none, relation = "none")

#plot none
ggplot(data = df_none, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()
```

If the shape of the relationship is **monotonic**, always increasing or always decreasing, then we describe the direction of the relationship in addition to the shape. Linear ([@fig-bivariate-shape-1]) and logarithmic ([@fig-bivariate-shape-3]) trends are examples of monotonic relationships. There are three potential directions: positive, negative, and no direction. These are illustrated in @fig-bivariate-direction.

```{r}
#| label: fig-bivariate-direction 
#| fig-cap: Direction of bivariate relationships
#| fig-subcap: 
#|   - "Positive"
#|   - "Negative"
#|   - "No direction"
#| layout-ncol: 3


#simulation code generated by Chat GPT 5

set.seed(12345)
n <- 150  # points per relationship

# Positive
x_pos <- runif(n, 0, 10)
y_pos <- 2.5 * x_pos + 1.5 + rnorm(n, mean = 0, sd = 4)
df_pos <- tibble(x = x_pos, y = y_pos, relation = "linear")

ggplot(data = df_pos, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

# negative
x_neg <- runif(n, 0, 10)
# True relation y = -2.5*x + 30 + noise
y_neg <- -2.5 * x_neg + 30 + rnorm(n, mean = 0, sd = 4)
df_neg <- tibble(x = x_neg, y = y_neg, relation = "negative_linear")

ggplot(data = df_neg, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

# flat
x_flat <- runif(n, 0, 10)
y_flat <- 15 + 0.1 * x_flat + rnorm(n, mean = 0, sd = 3)
df_flat <- tibble(x = x_flat, y = y_flat, relation = "flat_linear")

ggplot(data = df_flat, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

```

The direction is **positive** [@fig-bivariate-direction-1] if one variable tends to increase as the other increases. The direction is **negative** [@fig-bivariate-direction-2] if one variable tends to decrease as the other increases. Lastly, there **no direction** [@fig-bivariate-direction-3] if there is no clear pattern in how one variable changes as the other changes.

From @fig-bivar-eda-protein-calories, relationship between `calories` and `protein` is positive and linear. In general, recipes with more calories per serving tend to also have more grams of protein per serving.

#### Describe the relationship: Strength {.unnumbered}

The **strength** of the relationship between two variables is a measure how closely the observations follow the overall pattern or shape. Points that are tightly clustered together indicate a stronger relationship than points that are more dispersed. @fig-bivariate-strength shows examples of linear relationships with different strengths.

```{r}
#| label: fig-bivariate-strength
#| fig-cap: Strength of bivariate relationships
#| fig-subcap: 
#|   - "Strong"
#|   - "Moderate"
#|   - "Weak"
#| layout-ncol: 3


set.seed(12345)
n <- 150  # points per relationship

# Strong
x_strong <- runif(n, 0, 10)
y_strong <- 2.5 * x_strong + 1.5 + rnorm(n, mean = 0, sd = 3)
df_strong <- tibble(x = x_strong, y = y_strong, relation = "linear")
cor_strong <- round(cor(x_strong, y_strong),3)

ggplot(data = df_strong, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw() + 
  annotate(geom = "label", x = 1, y =30,
    label = paste0("r = ", cor_strong),
    hjust = "left", color = "black", 
    size = 7
  )
  
# Moderate
x_mod<- runif(n, 0, 10)
y_mod <- 2.5 * x_mod + 1.5 + rnorm(n, mean = 0, sd = 8.25)
df_mod <- tibble(x = x_mod, y = y_mod, relation = "linear")
cor_mod <- round(cor(x_mod, y_mod),3)


ggplot(data = df_mod, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw() +
  annotate(geom = "label", x = 1, y =35,
    label = paste0("r = ", cor_mod),
    hjust = "left", color = "black", 
    size = 7
  )

# Weak 
x_weak <- runif(n, 0, 10)
y_weak <- 2.5 * x_weak + 1.5 + rnorm(n, mean = 0, sd = 14)
df_weak <- tibble(x = x_weak, y = y_weak, relation = "linear")
cor_weak <- round(cor(x_weak,y_weak), 3)


ggplot(data = df_weak, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw() +
  annotate(geom = "label", x = 1, y =45,
    label = paste0("r = ", cor_weak),
    hjust = "left", color = "black", 
    size = 7
  )
```

When the shape between two variables is linear, we use the correlation to quantify the strength of the relationship. The **correlation**, denoted $r$, is a measure of the strength and direction of the [linear relationship]{.underline} between two variables. The correlation ranges from -1 to 1, with $r \approx -1$ indicating a nearly perfect negative linear relationship, $r \approx 1$ indicating a nearly perfect positive relationship, and $r \approx 0$ indicating a very weak to no linear relationship.

The direction of the linear relationship between two variable is indicated by the sign of the correlation. The strength is indicated by the magnitude of the correlation, $|r|$.

```{r}
#| label: cor-calories-protein

r_calories_protein <- recipes |>
  filter(!is.na(calories), !is.na(protein)) |>
  summarise(cor(calories, protein)) |>
  pull() 
```

Let's use the correlation to describe the strength of the linear relationship between `calories` and `protein`. The correlation between the two variables is `r round(r_calories_protein, 3)`. Based on the scatterplot in @fig-bivar-eda-protein-calories and the correlation, there is a strong, positive linear relationship between the two variables.

```{r}
#| label: make-correlation-sim-df

set.seed(123)
n <- 200

# simulation code generated by ChaptGPT 5

# no clear relationship
x1 <- rnorm(n)
y1 <- 0.15 * x1 + rnorm(n, sd = sqrt(1 - 0.15^2))
cor1 <- round(cor(x1, y1),2)
df1 <- tibble(x = x1, y = y1)

# parabolic relationsip
x2 <- runif(n, -3, 3)
y2 <- x2^2 + rnorm(n, 0, 2)  # parabolic shape with noise
cor2 <- round(cor(x2, y2),2)
df2 <- tibble(x = x2, y = y2)

```

::: {.analysis_in_practice latex=""}
Correlation measures the strength of the *linear* relationship, so values of correlation close to 0 do not necessarily mean there is no relationship between the two variables. In the graphs below, both relationships have correlation with magnitudes around $|r| = 0.12$ (`r cor1` and `r cor2` to be exact). From the scatterplots, however, we see a clear relationship between the two variables. The relationship is quadratic and not well described by a line, so the correlation is low.

It is important to use visualizations alongside summary statistics to provide additional context and reveal features that may be hidden by the summary statistic alone.

```{r}
#| label: fig-low-corr-examples
#| fig-cap: Relationships with low correlation 
#| fig-subcap: 
#| - "|r| = 0.12"
#| - "|r| = -0.12"
#| layout-ncol: 2

# no clear relationship 

ggplot(data = df1, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()

# quadratic relationship

ggplot(data = df2, aes(x = x, y = y)) + 
  geom_point() + 
  theme_bw()
```
:::

#### Describe the relationship: Outliers and notable features {.unnumbered}

When looking at the relationship between two variables, outliers are observations fall outside the general trend of the data. Outliers may have out-sized influence when building a regression model. Therefore, it is important to identify outliers in the EDA, so we can take them into account when evaluating the results from the regression analysis. We discuss how the impact of outliers on regression modeling in @sec-outliers-impact.

There are outliers in the relationship between `calories` and `protein` in @fig-bivar-eda-protein-calories. In particular, there are two observations with over 1500 calories per serving and protein close to 0 grams. These are far outside the general trend of the data, as the amount of protein is lower than expected based on the other observations with high calories per serving.

In addition to identifying outliers, we want to make note of other interesting features in the relationship. These features are most often observed from visualizations. For example, the variability (spread) in the values of one variable may increase as the values of the other increases. We see this in the relationship between `protein` and `calories` in @fig-bivar-eda-protein-calories, as the variability in protein (as seen by the vertical spread of the points) increases as the calories increase. Patterns like this are important to identify, because they directly relate to the assumptions we make when doing linear regression ([@sec-slr-foundation]). Recognizing these potential issues in the EDA can help us make decisions to address potential issues in the analysis.

::: {.analysis_in_practice latex=""}
The description of the relationship between two quantitative variables includes the following:

-   **Shape**: Overall pattern or trend in the data (e.g., linear, quadratic, etc.)

-   **Direction:** For a monotonic relationship, description of whether variables move together (positive), move opposite of one another (negative), or have no clear direction (none)

-   **Strength**: How clustered or spread out the observations are

-   **Outliers**: Observations that do not follow general trend of the data

-   **Interesting features**: Other interesting features, such as increased variability in one variable as the other increases
:::

#### Example: Relationship between `prep_time` and `avg_rating` {.unnumbered}

Is the amount of time it takes to prepare a dish associated with users' opinions about the recipe? To answer this, let's look at the relationship between `prep_time` and `avg_rating`.

```{r}
#| label: fig-bivar-eda-prep-rating
#| fig-cap: "Relationship between `prep_time` and `avg_rating`"

recipes |> 
 # filter(prep_time < 45) |>
  ggplot(aes(x = prep_time, y = cook_time)) + 
  geom_jitter(alpha = 0.5) + 
  theme_bw() + 
  labs(x = "Prep time in minutes", 
       y = "Average rating")
```

In @fig-bivar-eda-prep-rating, we see there are a few recipes with very long preparation times. Based on the descriptions in the data, these outlier recipes are those that require foods to marinate before cooking. They are making it difficult to see whether there is an association between the two variables for the vast majority of the data. The $95^{th}$ percentile for `prep_time` is 45 minutes, so we will narrow the scope of data and evaluate the relationship for those recipes that have preparation time of 45 minutes or less.

```{r}
#| label: fig-bivar-eda-prep-rating-45min
#| fig-cap: "Relationship between `prep_time` and `avg_rating` for recipes with `prep_time <= 45`"

recipes |> 
  filter(prep_time <= 45) |>
  ggplot(aes(x = prep_time, y = cook_time)) + 
  geom_jitter(alpha = 0.5) + 
  theme_bw() +
  labs(x = "Preparation time in minutes", 
       y = "Average rating")

prep_rating_cor <- recipes |> 
  filter(prep_time <= 45, !is.na(prep_time), !is.na(avg_rating)) |>
  summarise(cor = cor(prep_time, avg_rating)) |>
  pull(cor)
```

::: {.yourturn latex=""}
@fig-bivar-eda-prep-rating-45min is the updated scatterplot for recipes with preparation time 45 minutes or less. Based on @fig-bivar-eda-prep-rating-45min, does there appear to be a relationship between `prep_time` and `avg_rating`? Explain.[^03-eda-5]
:::

[^03-eda-5]: There does not appear to be a relationship between the average rating and the preparation time. There is not a clear trend on the plot.

Something that may stand out on this plot is that the data appear to be organized in columns. This is not an error in the data, but rather reveals something about the way `prep_time` appears on the website. The preparation times are recorded in 5-minute increments for recipes with preparation time of five minutes or greater. The exact time is recorded for the few recipes with preparation time less than five minutes.

The next thing that stands out on @fig-bivar-eda-prep-rating-45min is the recipes with preparation times equal to 0. This seems unusual, as we would expect some preparation for any recipe. So what is happening here? The recipes with 0 minutes for the preparation times are those that show no preparation time on the website. In other words, these are examples of missing data. We don't know why exactly these values are missing but some possible explanations are (1) the recipe author included the preparation time in the value for `cook_time`, (2) the recipe author just did not include preparation time in the materials posted on the site. In practice, we need to deal with these types of observations before moving too far into the analysis and modeling. <!--# We provide some strategies to do so in @sec-ch-special-topics.-->

### Quantitative and categorical variables {#sec-bivar-eda-quant-cat}

Next, we examine the relationship between a quantitative variable and a categorical variable. In general, we are interested in how the distribution of the quantitative variable differs based on levels of the categorical variable. Here we discuss three plots commonly used to visualize the relationship between a quantitative and categorical variable: side-by-side boxplots, ridgeline plots, and violin plots. These are all extensions of the plots for univariate quantitative distributions, so we can rely on the observations from @sec-univar-eda-quant as we examine these plots.

```{r}
#| label: top-authors

top_authors <- recipes |>
  count(author) |>
  arrange(desc(n)) |>
  slice(1:3)
```

John Mitzewich, more commonly known as Chef John, is an American chef who posts online content about cooking and has posted a lot of recipes on allrecipes.com. He is the author of `r top_authors |> slice(1) |> pull(n)` recipes on the site; the second most common author (besides anonymous authors) has `r top_authors |> slice(3) |> pull(n)` recipes. The variable `chef_john` is an indicator of whether a recipe is posted by Chef John. Let's look at the relationship between `chef_john` and `avg_rating` to see how users' ratings of Chef John's recipes compare to the ratings for other authors. @fig-bivar-eda-chef-rating shows three visualizations to explore the relationship between these two variables.

```{r}
#| label: fig-bivar-eda-chef-rating
#| fig-cap: "Relationship between `chef_john` and `avg_rating`" 
#| fig-subcap: 
#|   - Boxplot
#|   - Ridgeline plot
#|   - Violin plot
#| layout-ncol: 2

# boxplot
ggplot(data = recipes, aes(x = chef_john , y = avg_rating, fill = chef_john)) + 
  geom_boxplot(color = "black") +
  labs(x ="Chef John", 
       y = "Average rating", 
       fill = "Chef John") + 
  theme_bw() +
  scale_fill_manual(values = sunset2)

# ridgeline plot
ggplot(data = recipes, aes(x = avg_rating,  y = chef_john, fill = chef_john)) +
  geom_density_ridges() +
  scale_y_discrete() +
  scale_x_continuous() +
  labs(y ="Chef John", 
       x = "Average rating", 
       fill = "Chef John") + 
  theme_bw() +
  scale_fill_manual(values = sunset2)

# violin plot
ggplot(data = recipes, aes(x = chef_john, y = avg_rating, fill = chef_john)) + 
  geom_violin(color = "black", 
              draw_quantiles = c(0.5)) +
  theme_bw() +
  labs(x ="Chef John", 
       y = "Average rating", 
       fill = "Chef John") + 
  scale_fill_manual(values = sunset2)
```

The **side-by-side boxplot** in @fig-bivar-eda-chef-rating-1 shows a boxplot of the distribution of `avg_rating` for each level of `chef_john`. When evaluating whether there appears to be a relationship between the two variables, we want to see if the boxplots are relatively similar for the recipes posted by Chef John (`chef_john = 1`) and those posted by all other authors (`chef_john = 0`). In this plot, the median for Chef John's recipes is slightly higher than the median for all other posters, though there is overlap in the middle 50% of both distributions, as shown by the overlapping boxes. There is less variability in the average ratings among Chef John's recipes compared to the variability in the average ratings for all others. We also note that there are more extreme low outliers in the distribution of recipes posted by all other authors.

The **ridgeline plot** in @fig-bivar-eda-chef-rating-2 shows the density plot of the distribution of `avg_rating` for each level of `chef_john`. These plots are useful for comparing the center and shape of the distribution at each level. They also show how the spreads of the distributions compare. As with the side-by-side boxplots, we see there is more spread (variability) in the average ratings for recipes posted by others compared to those posted by Chef John. The number of outliers is less apparent, however, compared to side-by-side boxplot.

Lastly, the **violin plot** in @fig-bivar-eda-chef-rating-3 is like a combination of the boxplot and ridgeline plot. The plots show the density of `avg_rating` for each level of `chef_john` with the median marked by a horizontal line. This makes it easier to compare the center of the distributions. We can also easily compare the spreads of the distributions. As with the ridgeline plots, it is less clear which observations are outliers.

One thing that is not clear from the visualizations is the number of observations in each subgroup. Knowing the number of observations in each subgroup provides useful context to better understand the differences in the distributions. Therefore, in addition to visualizations, we compute the number of observations and other summary statistics introduced in @sec-univar-eda-quant to describe the distribution of the quantitative variable at each level of the categorical variable. The summary statistics for `avg_rating` for each level of `chef_john` is shown in @tbl-bivar-eda-chef-rating.

```{r}
#| label: tbl-bivar-eda-chef-rating
#| tbl-cap: "Summary statistics of `avg_rating` for each level of `chef_john`"
skim_results <- recipes |> 
  group_by(chef_john) |>
  skim(avg_rating) |>
  select(complete_rate:numeric.p100)

recipes |> 
  count(chef_john) |>
  bind_cols(skim_results) |>
  kable(digits = 3, col.names = c("chef_john", "n", "Prop Complete", "Mean", "SD", "Min", "Q1", "Median", "Q3", "Max"))
```

When evaluating a potential relationship, we are primarily concerned with how the centers of the distribution for the quantitative variable compare for each level of the categorical variable. If there are large differences between the centers, then there is indication of a potential relationship between the two variables. Otherwise, if the centers are equal or relatively close, there is little to no indication of a relationship between the variables. It is worth noting again that we are only making observations about the data, not drawing conclusions at this point.

#### Example: Relationship between `country` and `avg_rating` {.unnumbered}

How do users rate recipes from different countries? @fig-bivar-eda-country-rating shows the relationship between `country` and `avg_rating`.

```{r}
#| label: fig-bivar-eda-country-rating
#| fig-cap: "Relationship between `country` and `avg_rating`"

ggplot(data = recipes, aes(x = country, y = avg_rating, fill = country)) + 
  geom_violin(color = "black", 
              draw_quantiles = c(0.5)) +
  theme_bw() +
  scale_fill_manual(values = sunset12) + 
  labs(x = "Country", 
       y = "Average rating") +
  theme(legend.position = "none")
```

::: {.yourturn latex=""}
Write two observations from @fig-bivar-eda-country-rating. Does there appear to be a relationship between `country` and `avg_rating`?[^03-eda-6]
:::

[^03-eda-6]: Example observations: (1) There is the least variability in the `avg_rating` for French recipes. (2) The median `avg_rating` seems to be similar for all countries.\
    There does not appear to be a relationship between `country` and `avg_rating`. The median `avg_rating`is similar across countries, and there is a lot of overlap in the distributions of `avg_rating`

### Two categorical variables {#sec-bivar-eda-cat-cat}

The last type of bivariate relationship is the relationship between two categorical variables. We will be particularly interested in these relationships in @sec-ch-logistic, as we fit models with categorical response variables. We use visualizations along with tables of frequencies and relative frequencies to examine these relationships. There are three commonly used visualizations for the relationship between two categorical variables: grouped bar plot, segmented bar plot, and mosaic plot.

When looking at the relationship between a quantitative and categorical variable, we naturally looked at the distribution of the quantitative variable by levels of the categorical variable. When we have two categorical variables, the software does not have a natural way in which to arrange the data. Therefore, we rely on the analysis objective to determine how to most effectively visualize and summarize the relationship.

In @sec-intro-recipes we introduced Chef John, the most prolific poster on allrecipes.com based on the observations in our data. We'd like to explore whether he tends to post recipes at similar times of the year compared to other authors. Therefore, we will explore the relationship between `season` and `chef_john`, by looking at the distribution of `season` for Chef John compared to the distribution for all other authors. <!--# do i need this part? This will help us more easily compare Chef John's posting patterns compared to all others. It may be more challenging to make such comparisons if we looked at the distribution of chef_john for each season. -->

@fig-bivar-eda-chef-season shows three different visualizations of the relationship between `chef_john` and `season`.

<!--# figure out why mosaic plot is not working or alterantives-->

```{r}
#| label: fig-bivar-eda-chef-season 
#| fig-cap: "Relationship between `chef_john` and `season`"
#| fig-subcap: 
#|   - "Grouped bar chart"
#|   - "Segmented bar plot"
#|   - "Mosaic plot"
#| layout-ncol: 3


# grouped bar plot

ggplot(data = recipes, aes(x = chef_john, fill = season)) + 
  geom_bar(position = "dodge", color = "black") +
  theme_bw() +
  labs(x = "Chef John", 
       y = "Count", 
       fill = "Season") +
  scale_fill_manual(values = sunset4)

# segmented barplot 

ggplot(data = recipes, aes(x = chef_john, fill = season)) + 
  geom_bar(position = "fill", color = "black") + 
  theme_bw() + 
  labs(x = "Chef John", 
       y = "Proportion", 
       fill = "Season") +
  scale_fill_manual(values = sunset4)

# mosaic plot 
ggplot(data = recipes) +
  geom_mosaic(aes(x = product(season, chef_john), fill = season), color = "black") +
  labs(title = "",
       x = "Chef John", 
       y = "Season", 
       fill = "Season") + 
  scale_fill_manual(values = sunset4)
```

<!--# maybe put barplot by itself then mosaic and stacked barplot on a row-->

The **grouped bar plot** in @fig-bivar-eda-chef-season-1 shows a bar chart of `season` based on the levels of `chef_john`. The height of the bars represent the number of observations that take a given level of `season` and given level of `chef_john`. As we evaluate whether there is a relationship between the two variables, we look to see whether the relative bar heights across seasons is the same for recipes posted by Chef John compared to the relative bar heights for other authors. The data give some indication of a relationship between the variables if the distribution of `season` differs based on values of `chef_john`.

@fig-bivar-eda-chef-season-1 shows a limitation of the grouped bar chart. Because the heights of the bars represent the number of observations, these plots can be difficult to interpret when the data have a large imbalance. That is the case here, as there are `r recipes |> count(chef_john) |> filter(chef_john == 1) |> pull(n)` recipes were posted by Chef John and `r recipes |> count(chef_john) |> filter(chef_john == 0) |> pull(n)` were posted by all other authors.

In a **segmented bar chart** (also called *stacked bar chart*), the bars for one variable are filled in based on the distribution of another variable. In the segmented bar chart in @fig-bivar-eda-chef-season-2, there is a bar for each level of `chef_john`, and each bar is filled in based on the distribution of `season`. Because we are now working with proportions, we can more easily make comparisons across groups even when the data are imbalanced. If the distributions are approximately the same within each bar, then it is a indication of no relationship between the two variables. Otherwise, differences in the distributions within the bars indicate a potential relationship between the variables.

A **mosaic plot** <!--# is that what its called--> has some of the advantages of both grouped bar plots and segmented bar plots. In a mosaic plots, the bars for grouping variable are filled in based on the distribution of the other variable, and the width of the bars are based on the number of observations at each level of the grouping variable. @fig-bivar-eda-chef-season-2 shows the mosaic plot of `season` versus `chef_john`. Now we can not only see the distribution of season for recipes posted by Chef John and for recipes posted by all other authors, but we also see there are far fewer observations that take values `chef_john = 1` compared to `chef_john = 0`.

From the plots in @fig-bivar-eda-chef-season, we see that a larger proportion of recipes were posted by Chef John in the fall and winter seasons, compared to all other authors. The other authors posted a higher proportion of recipes in the summer compared to Chef John. These figures indicate a potential relationship between whether the recipe was posted by Chef John and the season in which it was posted.

<!--# do I want to make a point about the mosaic plot is a one-stop shop but it could be harder for others to interpret?-->

#### Example: Relationship between `country` and `season` {.unnumbered}

Is there a relationship between the country a recipe is from and the time of year it is posted? @fig-bivar-eda-country-season is a segmented bar chart showing the relationship between `country` and `season`.

```{r}
#| label: fig-bivar-eda-country-season
#| fig-cap: "Segmented bar chart of `country` versus `season`" 

ggplot(data = recipes, aes(x = country, fill = season)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "Country", 
       y = "Proportion",
       fill = "Season") + 
  scale_fill_manual(values = sunset4) +
  theme_bw()
```

::: {.yourturn latex=""}
Write two observations from @fig-bivar-eda-country-season. Does there appear to be a relationship between `country` and `season`?[^03-eda-7]
:::

[^03-eda-7]: Example observations: (1) A larger proportion of Filipino and French recipes are posted in the fall compared to recipes from other countries. (2) A much smaller proportion of Filipino recipes are posted in the spring compared to other countries.\
    There does appear to be a relationship between `season` and `country` . There are differences in the distribution of season across countries.

## Relationship between three or more variables {#sec-multivar-eda}

The next step is **multivariate EDA**, exploring the relationship between three or more variables. We typically limit this to the exploration of three variables, because it can become challenging to interpret and glean meaningful insights from the relationship between a large number of variables. We often use multivariate EDA when we want to explore potential interaction terms for the regression model ([@sec-mlr-interaction]). We generally rely on visualizations for multivariate EDA. We can start with one of the bivariate visualizations introduced in @sec-bivar-eda and further explore them based on subgroups of a third variable.

In @sec-bivar-eda-quant-cat, we looked at the relationship between the average rating and whether a recipe was posted by Chef John or another author. Now let's expand on that and consider whether this relationship differs between seasons. @fig-multivar-eda shows side-by-side boxplots of `avg_rating` versus `chef_john` faceted (split up) by `season`.

```{r}
#| label: fig-multivar-eda
#| fig-cap: "Multivariate EDA of `avg_rating` versus `chef_john` faceted by `season`" 

recipes |>
ggplot(aes(x = chef_john, y = avg_rating, fill = chef_john)) +
  geom_boxplot(color = "black") +
  facet_wrap(vars(season)) + 
  labs(x = "Chef John", 
       y = "Average rating", 
       fill = "Chef John") +
  theme_bw() +
  scale_fill_manual(values = sunset2)
```

As we look at the visualization, the primary question to ask is whether the relationship between `avg_rating` and `chef_john` looks similar or different across the levels of `season`. In other words, are the boxplots the same relative to one another for each level of `season`. Note that we are not necessarily interested in the absolute position of the boxplots (it's OK if some seasons generally have higher or lower ratings compared to others) but rather we are interested in the relative positioning of the boxplots within a season.

From @fig-multivar-eda, the relationship between `avg_rating` and `chef_john` is approximately the same in each season. Within each season, the median `avg_rating` for recipes posted by Chef John is higher compared to those posted by other authors. Additionally, there is generally a lot of overlap in the boxes within each season. The exception is the summer, but we keep in mind the very small variability (and small sample size) of recipes posted by Chef John. Based on this EDA, the relationship between `avg_rating` and `chef_john` does not appear to differ by `season`.

<!--# add another example-->

## Exploratory data analysis in R

Much of the exploratory data analysis in this chapter is done using functions from the **dplyr** and **ggplot2** packages. We refer the reader to @sec-ch-computing for a more detailed introduction to those packages. Here we will focus on the new functions that were used in this chapter: `skim()` in @sec-univar-eda-quant, `waffle()` in @sec-univar-eda-cat, `geom_ridges()` in @sec-bivar-eda-quant-cat, and `geom_mosaic()` in @sec-bivar-eda-cat-cat.

### Data summaries

The `skim()` function in the **skimr** R package [@skimr] provides a summary of all the columns in a data frame or tibble. This function is particularly useful for initially checking the data [@sec-initial-check] and univariate data analysis ([@sec-univar-eda]). The code below produces a summary for each column `recipes`. Different types of summary output is produced based on the column's data format.

```{r}
#| label: tbl-skim-recipes 
#| tbl-cap: "`skim()` output for `recipes`"
#| echo: true

recipes |> 
  skim()
```

If we are only interested in the summary for particular variables, we specify them in the `skim()` function. The code below produces the summaries for `avg_rating` and `season`.

```{r}
#| label: tbl-skim-recipes-subset
#| tbl-cap: "`skim()` output for `avg_rating` and `season`"
#| echo: true


recipes |>
  skim(avg_rating, season)
```

The output from `skim()` is a tibble, so we can apply the usual **dplyr** functions to the results. For example, we may not be interested in all the summary output, so we can use `select()` to choose certain columns. The code below shows the mean and standard deviation for `avg_rating`. The column names from the `skim()` output differ slightly from what is displayed, so use `glimpse()` or `names()` to see the underlying column names.

```{r}
#| label: skim-recipes-some-cols
#| echo: true

recipes |> 
  skim(avg_rating) |>
  select(numeric.mean, numeric.sd)
```

### Waffle charts

We introduced waffle charts in @sec-univar-eda-cat to explore the univariate distribution of a categorical variable. Waffle charts are produced using `geom_waffle()` in the **waffle** R package [@waffle]. The functions in this package are developed to work within the **ggplot2** framework, so we can use all the **ggplot** functions from @sec-customize-plots to customize the charts.

The code to create a waffle chart for the `season` is below. We begin by using `count()` to compute the number of observations at each level. Inside `geom_waffle()`, the argument `flip = TRUE` arranges the level of season by rows, rather than the default arrangement by columns. The argument `make_proportional = TRUE` produces a chart such that the number of squares is approximately equal to the proportion of observations at each level, rather than the raw number of observations. Lastly, `theme_enhance_waffle()` removes unnecessary axis labels.

```{r}
#| label: waffle-chart
#| echo: true

recipes |>
  count(season) |>
  ggplot(aes(fill = season, values = n)) +
  geom_waffle(flip = TRUE, make_proportional = TRUE) +
  theme_enhance_waffle() +
  scale_fill_manual(values = sunset4)
```

### Ridgeline plots

In @sec-bivar-eda-quant-cat, we introduced ridgeline plots to explore the relationship between a quantitative and categorical variable. We use `geom_density_ridges()` in the **ggridges** R package [@ggridges] to create the plots. Similar to **ggwaffle**, the functions in this package are developed to work within the **ggplot2** framework, so we can utilize the functions from @sec-customize-plots to customize the plots.

The code for the ridgeline plot of the relationship between `avg_rating` and `chef_john` is below.

```{r}
#| label: ggridges
#| echo: true

ggplot(data = recipes, aes(x = avg_rating, y = chef_john)) + 
  geom_density_ridges() 

```

We customize the ridgeline plot by filling in the color of the densities based on `chef_john`, updating the labels, and applying a new theme to the visualization.

```{r}
#| echo: true

ggplot(data = recipes, aes(x = avg_rating, y = chef_john, fill = chef_john)) + 
  geom_density_ridges() +
  labs(x = "Average rating", 
       y = "Chef John", 
       fill = "Chef John") + 
  theme_bw() +
  scale_fill_manual(values = sunset2)
```

### Mosaic plots

We introduced the mosaic plot for examining the relationship between two categorical variables in @sec-bivar-eda-cat-cat. Mosaic plots are created using `geom_mosaic()` in the **ggmosaic** R package [@ggmosaic].

Below is a mosaic plot of `chef_john` versus `season`. The `aes()` function to define the aesthetics must be an argument of `geom_mosaic()`. Additionally, the `x` aesthetic is defined as the product of the two categorical variables of interest.

```{r}
#| label: mosaic-plot 
#| echo: true

ggplot(data = recipes) +
  geom_mosaic(aes(x = product(season, chef_john), fill = season))
```

Similar to the packages for waffle plots and ridgeline plots, we can customize mosaic plots using the functions in @sec-customize-plots. In the code below, we update the colors using a palette from the **viridis** R package [@viridis].

```{r}
#| label: mosaic-plot-customized
#| echo: true

library(viridis)

ggplot(data = recipes) +
  geom_mosaic(aes(x = product(season, chef_john), fill = season)) +
  scale_fill_viridis_d()
```

## Summary

::: {.analysis_in_practice latex=""}
**Exploratory data analysis workflow:**

-   Clean data
-   Explore individual variables
-   Explore relationships between two variables
-   Explore relationships between three or more variables
:::

In this chapter we introduced the exploratory data analysis, the step in the data science workflow when we begin to clean and explore the data. This is an important step in a regression analysis, because having a clear understanding of the data helps inform the decisions we make throughout the analysis process. It is also the point at which we may identify errors or missing values in the data. We conduct EDA by using visualizations and summary statistics to get an overview of the data, explore the distributions of individual variables, explore the relationships between two variables, and explore the relationships between three or more variables.

The remainder of the text focuses on regression analysis, methods for modeling the relationship between a response variable and one or more predictor variables. Given the importance of EDA in every analysis, each chapter will begin with a short exploration of key variables and relationships in the analysis. The EDA in these sections are is meant to provide context about the data, equipping us to more fully interpret and draw conclusions from the regression analysis results. Because the focus of each chapter is the regression analysis method, the EDA will be brief and focused, similar to what might be presented in a final presentation or report. In practice, the EDA is more in-depth, particularly when working with data sets that have a large number of variables.

We begin with simple linear regression in @sec-ch-slr, using regression analysis to model the relationship between the response variable and one predictor variable. This will provide the foundation for the more complex models introduced in @sec-ch-mlr.
